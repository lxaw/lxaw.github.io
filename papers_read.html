
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Searchable paper list</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <h1>Here's where I keep a list of papers I have read.</h1>
    <p>
        This list was curated by <a href="index.html">Lexington Whalen</a>, beginning from his first year of PhD to end. As he is me, I hope he keeps going!
    </p>
    <p>
        I typically use this to organize papers I found interesting. Please feel free to do whatever you want with it. Note that this is not every single paper I have ever read, just a collection of ones that I remember to put down.
    </p>
    <p id="paperCount">
        So far, we have read 265 papers. Let's keep it up!
    </p> 
    <small id="searchCount">
        Your search returned 265 papers. Nice! 
    </small>
    
    <div class="search-inputs">
        <input type="text" id="titleSearch" placeholder="Search title...">
        <input type="text" id="authorSearch" placeholder="Search author...">
        <input type="text" id="yearSearch" placeholder="Search year...">
        <input type="text" id="topicSearch" placeholder="Search topic...">
        <input type="text" id="venueSearch" placeholder="Search venue...">
        <input type="text" id="descriptionSearch" placeholder="Search description...">
        <button id="clearSearch">Clear Search</button>
    </div>
    
    <table id="paperTable">
        <thead>
            <tr>
                <th data-sort="title">Title</th>
                <th data-sort="author">Author</th>
                <th data-sort="year">Year</th>
                <th data-sort="topic">Topic</th>
                <th data-sort="venue">Venue</th>
                <th data-sort="description">Description</th>
                <th>Link</th>
            </tr>
        </thead>
        <tbody>
        
            <tr>
                <td>d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning</td>
                <td>Siyan Zhao et al</td>
                <td>2025</td>
                <td>rl, diffusion, llm</td>
                <td>Arxiv</td>
                <td>Zhao et al. introduce d1, a novel framework that enhances reasoning capabilities in diffusion Large Language Models (dLLMs) through a two-stage post-training approach combining supervised fine-tuning (SFT) and reinforcement learning. While traditional autoregressive models like Claude or GPT generate text left-to-right sequentially, diffusion models like LLaDA generate through iterative denoising from masked tokens in a non-autoregressive manner. The authors&#x27; key innovation is diffu-GRPO, a policy gradient algorithm specifically adapted for masked dLLMs that efficiently estimates log-probabilities using a one-step random prompt masking technique. This approach serves as a regularization mechanism, allowing more gradient updates per batch and reducing computational demands. Applied to LLaDA-8B-Instruct, the full d1 pipeline consistently outperforms both the base model and individual SFT or RL approaches across four mathematical and logical reasoning benchmarks, with particularly significant gains on complex tasks like Countdown (+21.5%) and Sudoku (+10.4%). The results demonstrate that masked dLLMs can benefit from RL-based reasoning enhancement techniques previously limited to autoregressive models, opening new directions for improving non-autoregressive language models.</td>
                <td><a href="https://arxiv.org/pdf/2211.16750" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Score-Based Continuous-Time Discrete Diffusion Models</td>
                <td>Haoran Sun et al</td>
                <td>2023</td>
                <td>continuous, llm, diffusion, discrete</td>
                <td>ICLR</td>
                <td>Sun et al. introduce a breakthrough approach extending score-based diffusion models to discrete categorical data through a continuous-time Markov chain formulation. Their key innovation is developing &quot;categorical ratio matching,&quot; a discrete analog to score matching that effectively characterizes how discrete distributions evolve over time without requiring gradients. This theoretical advancement enables the formulation of a coherent stochastic jump process for the reverse (denoising) process with an analytical sampling method as an alternative to numerical simulation. The authors further introduce three architectural approaches—energy-based models, masked models, and a novel &quot;hollow Transformer&quot;—to parameterize conditional distributions while avoiding information leakage issues. Their experiments on synthetic data, CIFAR10 images, and music generation demonstrate superior performance compared to previous discrete diffusion approaches, particularly when sampling with fewer steps, highlighting the flexibility advantages of the continuous-time formulation.</td>
                <td><a href="https://arxiv.org/pdf/2211.16750" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>A Continuous Time Framework for Discrete Denoising Models</td>
                <td>Andrew Campbell et al</td>
                <td>2022</td>
                <td>continuous, llm, diffusion, discrete</td>
                <td>NeurIPS</td>
                <td>The paper &quot;A Continuous Time Framework for Discrete Denoising Models&quot; presents the first complete theoretical foundation for extending diffusion models to discrete data using continuous time. The authors formulate both the forward noising process and generative reverse process as Continuous Time Markov Chains (CTMCs), providing a mathematically rigorous alternative to previous discrete-time approaches. This continuous framework enables the derivation of a continuous time evidence lower bound (ELBO) for efficient training, and unlocks high-performance sampling strategies like tau-leaping—borrowed from chemical physics—which allows multiple dimensions to change simultaneously in a single simulation step. Particularly innovative is their predictor-corrector scheme that alternates between sampling steps guided by the reverse rate and corrector steps that push the distribution toward the true marginal, significantly improving sample quality on image generation tasks. The authors also contribute a novel theoretical error bound between generated and true data distributions, demonstrating how their approach maintains accuracy even in high-dimensional discrete spaces while offering greater flexibility than discrete time methods.</td>
                <td><a href="https://arxiv.org/pdf/2205.14987" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model</td>
                <td>Yizhe Zhang et al</td>
                <td>2023</td>
                <td>planning, llm, diffusion</td>
                <td>NeurIPS</td>
                <td>The paper introduces PLANNER, a novel two-stage latent text diffusion model for generating diversified paragraphs that addresses repetitive and low-quality output issues in autoregressive models. Unlike previous text diffusion models that operate directly on tokens, PLANNER combines an autoregressive &quot;decoding&quot; module with a &quot;planning&quot; module that uses latent diffusion to generate semantic paragraph embeddings in a coarse-to-fine manner. The approach first learns a variational paragraph embedder that condenses lengthy texts into a fixed number of semantic tokens, then applies a continuous-time latent diffusion model to learn the distribution of these embeddings. Experimental results across sentiment-guided generation, text completion, and summarization tasks demonstrate that PLANNER generates more fluent and diverse text with less repetition compared to both autoregressive methods and text diffusion baselines, while maintaining comparable relevance scores and offering computational advantages through batched processing of fixed-length latent codes.</td>
                <td><a href="https://openreview.net/pdf?id=SLwy8UVS8Y" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models</td>
                <td>Jiacheng Ye et al</td>
                <td>2024</td>
                <td>cot, llm, diffusion</td>
                <td>NeurIPS</td>
                <td>The paper introduces Diffusion-of-Thought (DoT), a novel approach that adapts chain-of-thought reasoning for diffusion language models. Unlike autoregressive language models that generate reasoning steps sequentially left-to-right, DoT allows reasoning to diffuse over time through parallel updates of latent variables, offering greater flexibility in trading computation for reasoning performance. Two key innovations include scheduled sampling during training to improve self-correction capabilities and a multi-pass variant (DoTMP) that generates reasoning steps sequentially to introduce causal bias. For training, the authors fine-tune pre-trained diffusion models (Plaid and SEDD) using classifier-free guidance and implement a conditional ODE solver to accelerate inference. Experimental results on multiplication, boolean logic, and grade school math problems demonstrate that DoT achieves comparable or better performance than autoregressive models while offering significant speed advantages on simpler tasks, with a small diffusion model outperforming a much larger autoregressive model in both efficiency and accuracy.</td>
                <td><a href="https://arxiv.org/pdf/2402.07754" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning</td>
                <td>Jiacheng Ye et al</td>
                <td>2025</td>
                <td>planning, llm, diffusion</td>
                <td>ICLR</td>
                <td>The authors introduce the concept of &quot;subgoal imbalance,&quot; demonstrating that autoregressive models struggle with difficult subgoals in planning tasks, often achieving near-random performance. They show that diffusion models effectively decompose these challenging subgoals into more manageable interrelated views within a multi-view learning framework, resulting in superior performance. Building on these insights, they propose Multi-Granularity Diffusion Modeling (MGDM), which prioritizes subgoals based on difficulty during learning, leading to more effective outcomes and faster convergence. Their experimental evaluation focuses on complex problem-solving tasks like Countdown (a mathematical reasoning challenge), Sudoku, and Boolean Satisfiability Problems. For problems like math reasoning or Sudoku where later steps directly depend on earlier ones, this creates a &quot;plan as you go&quot; approach that&#x27;s inherently flawed. Diffusion models transform hard subgoals into multiple interrelated &quot;views&quot; during the denoising process. Each view offers a different perspective on the same problem, creating a more manageable learning objective. The authors identified a phenomenon they call the &quot;Regretful Compromise&quot; where AR models, after making early mistakes, are forced to produce clearly incorrect calculations in final steps to reach the target answer. The iterative nature of diffusion models naturally promotes global consistency. Each refinement step considers the entire solution, allowing the model to make coordinated changes that maintain mathematical validity across all equations or puzzle constraints.</td>
                <td><a href="https://arxiv.org/pdf/2410.14157" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Do Language Models Plan Ahead for Future Tokens?</td>
                <td>Wilson Wu et al</td>
                <td>2024</td>
                <td>planning, llm</td>
                <td>COLM</td>
                <td>This paper investigates whether transformers &quot;think ahead&quot; during inference by preparing information in hidden states that will be useful for future tokens. The authors propose two hypotheses: the &quot;pre-caching&quot; hypothesis (where models deliberately compute features irrelevant to the current token but useful for future tokens) and the &quot;breadcrumbs&quot; hypothesis (where features helpful for current prediction naturally benefit future tokens without deliberate planning). To test these hypotheses, the researchers develop &quot;myopic training&quot;, where models are trained without propagating gradients to past timesteps. They first create a synthetic task that can only be solved via pre-caching, confirming that transformers can learn this capability when necessary. However, in natural language modeling with smaller models like GPT-2, they find minimal pre-caching, suggesting the breadcrumbs hypothesis predominates—models compute features relevant to the immediate next token that happen to benefit future tokens, without significant trade-offs. Interestingly, the authors discover that pre-caching increases with model scale, becoming more significant with larger models like Pythia 2.8B. This indicates that larger language models may indeed &quot;plan for the future&quot; in ways smaller models cannot. They also examine multiplication tasks, finding evidence that pre-caching enables computation on &quot;filler tokens&quot; that improves overall performance.</td>
                <td><a href="https://arxiv.org/pdf/2404.00859" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Categorical Reparameterization with Gumbel-Softmax</td>
                <td>Eric Jang et al</td>
                <td>2017</td>
                <td>gumbel-softmax</td>
                <td>ICLR</td>
                <td>This paper presents a method for efficiently training neural networks with discrete random variables. The Gumbel-Softmax is a continuous relaxation of categorical distributions that addresses the challenge of backpropagating through discrete random variables in neural networks. It builds upon the Gumbel-Max trick, which samples from categorical distributions by adding Gumbel noise to logits and taking the argmax, but replaces the non-differentiable argmax operation with a differentiable softmax function controlled by a temperature parameter τ. When the temperature approaches zero, the distribution approximates a categorical one-hot vector, while higher temperatures yield more uniform distributions. This temperature can be gradually annealed during training to balance exploration and discrete decision-making. For applications requiring truly discrete outputs, the Straight-Through Gumbel-Softmax variant uses argmax in the forward pass while preserving differentiable gradients through the softmax in the backward pass. This technique has enabled significant advances in training neural networks with discrete variables, categorical variational autoencoders, and efficient semi-supervised learning algorithms, solving a fundamental limitation in stochastic neural networks.</td>
                <td><a href="https://arxiv.org/pdf/1611.01144" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Training Verifiers to Solve Math Word Problems</td>
                <td>Karl Cobbe et al</td>
                <td>2021</td>
                <td>verifiers</td>
                <td>Arxiv</td>
                <td>This paper introduces GSM8K, a dataset of 8.5K high-quality linguistic diverse grade school math word problems, to address the challenges language models face with multi-step mathematical reasoning. Despite their success in many tasks, even large language models struggle with mathematics due to the sensitivity to individual errors in step-by-step reasoning. The authors propose a verification approach to improve performance: they first finetune a generator model, then train a separate verifier model to judge the correctness of potential solutions. At test time, they generate multiple candidate solutions and select the highest-ranked one. This verification method significantly outperforms basic finetuning, providing performance equivalent to a 30x increase in model size.</td>
                <td><a href="https://arxiv.org/pdf/2110.14168" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</td>
                <td>Yukang Chen et al</td>
                <td>2024</td>
                <td>lora, long context</td>
                <td>ICML</td>
                <td>LongLoRA efficiently extends the context window of large language models through a two-pronged methodology. First, the authors introduce Shifted Sparse Attention (S²-Attn), which divides the input sequence into multiple groups and performs attention only within each group, then critically shifts the group partitioning by half a group size in 50% of attention heads to ensure information flows between groups. This approximates full attention during training while dramatically reducing computational costs. Second, they enhance Low-Rank Adaptation (LoRA) by making embedding and normalization layers trainable—components that comprise less than 2% of model parameters but prove essential for long-context adaptation. Their experiments demonstrate this approach closes the performance gap between LoRA and full fine-tuning while maintaining significantly lower memory requirements. Importantly, models trained with S²-Attn retain standard attention during inference, ensuring compatibility with existing optimization techniques like Flash-Attention2, making LongLoRA a practical solution that enables extending Llama2 models to context lengths of up to 100K tokens on modest hardware setups.</td>
                <td><a href="https://arxiv.org/pdf/2309.12307" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference</td>
                <td>Hao (Mark) Chen et al</td>
                <td>2024</td>
                <td>llm, hardware, parallel decoding</td>
                <td>Arxiv</td>
                <td>This paper introduces Parallel Prompt Decoding (PPD), a memory-efficient approach for accelerating Large Language Model inference by using trainable prompt tokens to enable parallel token generation. Unlike existing speculative decoding methods that require separate draft models, PPD works by training only the embeddings of special prompt tokens (0.0002% of parameters) that are strategically inserted into the input sequence at positions where future tokens are expected to be generated. The prompt tokens partially recover conditional dependency information necessary for accurate multi-token prediction, with the authors using Knowledge Distillation to train the prompt token embeddings against the original LLM&#x27;s logits while keeping the base model frozen. They enhance accuracy through Ensemble Prompt Tokens (EPTs), where multiple trained embeddings represent each prompt position with specialized attention masking to ensure each EPT only depends on corresponding EPTs from preceding positions. To optimize performance across hardware, they developed a hardware-aware dynamic sparse tree technique that adaptively determines the optimal tree structure and balances the number of candidate tokens versus prompt tokens at each decoding step based on token acceptance probabilities and computational resources. Experiments across models from MobileLlama to Vicuna-13B demonstrate up to 2.49× inference speedup with minimal memory overhead (0.0004%) while maintaining output quality, and PPD can be combined with existing speculative decoding methods for further performance improvements.</td>
                <td><a href="https://arxiv.org/pdf/2405.18628" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Recurrent Memory Transformer</td>
                <td>Aydar Bulatov et al</td>
                <td>2022</td>
                <td>meta tokens, memory</td>
                <td>NeurIPS</td>
                <td>This paper introduces the Recurrent Memory Transformer (RMT), a novel architecture that combines memory tokens with segment-level recurrence to address two key limitations of standard Transformer models: the difficulty of storing global information across a sequence and the quadratic computational complexity that limits input sequence length. RMT works by adding special memory tokens to both the beginning and end of each input segment, with the end tokens capturing information as &quot;write memory&quot; and then being passed to the next segment as &quot;read memory&quot; creating a recurrent connection between segments. The key innovation is that both the memory mechanism and recurrence are implemented without modifying the core Transformer architecture - only the input and output sequences are changed by adding special tokens. This makes RMT compatible with any Transformer-based model.</td>
                <td><a href="https://arxiv.org/pdf/2207.06881" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Learning to Keep a Promise: Scaling Language Model Decoding Parallelism with Learned Asynchronous Decoding</td>
                <td>Tian Jin et al</td>
                <td>2025</td>
                <td>parallelism, asynchronous</td>
                <td>Arxiv</td>
                <td>This paper introduces PASTA (PArallel STructure Annotation LANGuage), a system that teaches large language models (LLMs) to identify semantic independence in their responses and leverage it for parallel decoding to improve inference speed. The authors observe that while autoregressive LLM decoding is inherently sequential and inefficient (often achieving less than 20% Model Flops Utilization during inference), many parts of an LLM&#x27;s response can actually be generated in parallel because they&#x27;re semantically independent. Unlike previous approaches that rely on hand-crafted heuristics tied to specific syntactic structures like lists and paragraphs, PASTA uses a learning-based approach. The system consists of three components: (1) a specialized annotation language (PASTA-LANG) that allows LLMs to express semantic independence in their own responses using tags like &lt;promise&gt; and &lt;sync/&gt;; (2) an interpreter that orchestrates asynchronous decoding at inference time based on these annotations; and (3) a two-stage finetuning process that teaches LLMs to generate effective annotations that optimize both response quality and decoding speed.</td>
                <td><a href="https://arxiv.org/pdf/2502.06901" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Enabling Autoregressive Models to Fill in Masked Tokens</td>
                <td>Daniel Israel et al</td>
                <td>2025</td>
                <td>autoregression, mlm</td>
                <td>Arxiv</td>
                <td>This paper introduces MARIA (Masked and Autoregressive Infilling Architecture), a novel approach that combines the strengths of both autoregressive (AR) and masked language modeling (MLM) paradigms. The method addresses a fundamental limitation of AR models, which cannot effectively perform masked infilling (predicting tokens between past and future context) despite their dominance in the field due to superior training and inference efficiency. MARIA works by taking a pre-trained MLM model (which can naturally handle infilling but is computationally inefficient) and an AR model, then training a simple linear decoder that operates on their concatenated hidden states. This minimal architectural modification enables the resulting model to perform masked infilling while retaining the computational benefits of AR models, particularly KV caching during inference.</td>
                <td><a href="https://arxiv.org/pdf/2502.06901" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Unlocking Guidance for Discrete State-Space Diffusion and Flow Models</td>
                <td>Hunter Nisonoff et al</td>
                <td>2025</td>
                <td>state space, discrete, diffusion</td>
                <td>ICLR</td>
                <td>This paper introduces &quot;Discrete Guidance&quot; a novel approach for applying guidance to generative models in discrete state-spaces. The key innovation is a mathematically rigorous framework for controlling the outputs of discrete diffusion and flow models by leveraging continuous-time Markov processes. The authors tackle a fundamental challenge in discrete guidance: the intractability of computing the normalizing constant in Bayes&#x27; theorem, which would normally require summing over an exponentially large state space. They overcome this by exploiting the fact that in continuous-time Markov chains, only one dimension of the state can change at any instant, making exact guidance computationally feasible. The paper develops both predictor guidance (PG) and predictor-free guidance (PFG) variants, along with a computationally efficient approximation called Taylor-approximated guidance (TAG). They demonstrate their method&#x27;s effectiveness across multiple domains including small-molecule generation, DNA sequence design, and protein engineering. Notably, their framework enables repurposing existing unconditional generative models by modulating their behavior with guidance signals, avoiding the need for retraining on limited labeled data. This approach creates new possibilities for controllable generation in scientific applications where discrete representations are essential, such as molecular and biological sequence design.</td>
                <td><a href="https://arxiv.org/pdf/2406.01572" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data</td>
                <td>Jingyang Ou et al</td>
                <td>2025</td>
                <td>dit, token, caching</td>
                <td>ICLR</td>
                <td>This paper created a simpler and faster way to generate text using AI. The researchers realized that previous text generation models were unnecessarily complicated because they included time as an input. They found that time could be separated into a simple formula multiplied by the actual prediction, allowing them to remove time completely from their model. This simplification made their model (called RADD) faster in two ways: it had fewer parameters to compute, and they could cache results when parts of the text stayed the same during generation. They also proved that their approach is mathematically equivalent to another type of text generation method, bringing together different research approaches.</td>
                <td><a href="https://arxiv.org/pdf/2406.03736" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Accelerating Diffusion Transformers with Token-wise Feature Caching</td>
                <td>Chang Zou et al</td>
                <td>2025</td>
                <td>dit, token, caching</td>
                <td>ICLR</td>
                <td>The ToCa (Token-wise Feature Caching) methodology accelerates diffusion transformers through a sophisticated token selection strategy for caching. The approach begins with cache initialization at the first timestep where all tokens are computed and stored, followed by selective computing in subsequent timesteps based on a pre-defined caching ratio R. Unlike traditional methods, ToCa updates the cache with newly computed values at every timestep to reduce error accumulation. The heart of ToCa is its caching score function, which combines four critical criteria: the token&#x27;s influence on other tokens through self-attention (s1), its importance for text conditioning via cross-attention entropy (s2), how frequently it has been cached (s3), and whether caching it maintains uniform spatial distribution (s4). ToCa further refines its approach by applying different caching ratios to different layers, with deeper layers receiving higher caching ratios since errors in them have less opportunity to propagate, while shallow layers get lower ratios. The method also distinguishes between layer types, handling self-attention layers differently from MLP layers to prevent error propagation. Additional engineering optimizations include timestep-dependent scheduling and consistent token selection between conditional and unconditional paths. By intelligently selecting which tokens can be safely cached based on their specific characteristics and position in the network, ToCa achieves significant speedups while maintaining generation quality across different diffusion transformer architectures.</td>
                <td><a href="https://arxiv.org/pdf/2410.05317" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Faith and Fate: Limits of Transformers on Compositionality</td>
                <td>Nouha Dziri et al</td>
                <td>2023</td>
                <td>transformers, compositionality</td>
                <td>Arxiv</td>
                <td>The paper reveals that transformer large language models (LLMs) struggle with seemingly simple compositional problems requiring multi-step reasoning, despite their impressive performance on complex tasks. Using a computation graph framework to measure task complexity across multi-digit multiplication, logic puzzles, and dynamic programming problems, the authors demonstrate that LLMs solve compositional tasks through pattern matching rather than systematic reasoning, performing well on in-distribution examples but failing dramatically on out-of-distribution cases even after extensive fine-tuning with explicit reasoning steps. Their error analysis shows that while models can perform single-step operations correctly, they fail to compose these steps into correct reasoning paths, with errors propagating through the computation graph because the autoregressive nature of transformers forces them to tackle problems sequentially through a greedy process of producing the next token without developing a global understanding of the task. Both empirical findings and mathematical analysis reveal a fundamental limitation where error probability increases exponentially with task complexity, suggesting transformers may be inherently limited in solving complex compositional tasks without additional mechanisms. The paper concludes that transformers are best suited for tasks with few reasoning steps, tasks allowing approximate solutions, or when combined with planning modules or refinement methods to overcome the limitations of their autoregressive architecture.</td>
                <td><a href="https://arxiv.org/pdf/2305.18654" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>The Generative AI Paradox: What it Can Create, It May Not Understand</td>
                <td>Peter West et al</td>
                <td>2023</td>
                <td>llm, intelligence, generation</td>
                <td>Arxiv</td>
                <td>This paper introduces the &quot;Generative AI Paradox&quot;, revealing that generative AI models can produce high-quality outputs while simultaneously failing to demonstrate understanding of those same outputs. Through experiments in both language and vision modalities, the authors demonstrate that models like GPT-4 and Midjourney often outperform humans in generation tasks but underperform in related discriminative and question-answering tasks about their own creations. This capability gap widens with increasing task complexity and presents more starkly than in human intelligence, where understanding typically precedes generation ability. The research suggests this divergence stems from fundamental differences in how AI systems and humans develop intelligence—models are trained directly on reproducing expert-like outputs without requiring deep understanding. This finding cautions against interpreting AI capabilities by analogy to human intelligence and suggests studying AI as a counterpoint rather than a parallel to human cognition.</td>
                <td><a href="https://arxiv.org/pdf/2311.00059" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements</td>
                <td>Jillian Fisher et al</td>
                <td>2024</td>
                <td>authorship, style, llm</td>
                <td>Arxiv</td>
                <td>STYLEREMIX operates through two distinct phases to effectively obscure authorship style. In the pre-obfuscation phase, which is performed just once, the system identifies key stylistic axes that differentiate authors, including text length, function word usage, grade level, formality, voice, sarcasm, and writing type. For each of these style axes, the researchers create parallel datasets where identical content is rewritten in different stylistic directions, which are then used to train lightweight Low-Rank Adaptation (LoRA) modules that can be integrated with a larger base language model. During the actual obfuscation phase, STYLEREMIX analyzes the author&#x27;s text to create an &quot;author vector&quot; measuring their style along various axes and compares this to the average style vector of all authors in the same domain to identify which style elements are most distinctive for this specific author. Based on this analysis, the system selectively applies the appropriate LoRA adapters with calculated weights to steer the text away from the author&#x27;s most recognizable style elements. These adapters can be applied sequentially, through adapter merging (combining weights before applying), or using LoraHub+ (which optimizes the weights through gradient-free optimization). What makes STYLEREMIX innovative is its adaptive approach to identifying and targeting the specific style elements that make an author recognizable, rather than applying generic transformations, resulting in more effective obfuscation while maintaining content integrity and fluency.</td>
                <td><a href="https://arxiv.org/pdf/2408.15666" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Scaling Diffusion Language Models via Adaptive From Autoregressive Models</td>
                <td>Shansan Gong et al</td>
                <td>2025</td>
                <td>diffusion, masked diffusion, mdm</td>
                <td>ICLR</td>
                <td>The paper introduces a method for converting autoregressive (AR) language models into diffusion language models (DLMs) through three key techniques. First, they unify the loss functions between AR and diffusion models by establishing connections between their modeling objectives. Second, they implement attention mask annealing, which gradually transitions from the causal (unidirectional) attention mask of AR models to the full bidirectional attention pattern needed for diffusion models. Third, they maintain the shift operation from AR models, where predictions at each position correspond to the next token, ensuring alignment between the pretrained AR model&#x27;s behavior and the diffusion objective. This approach allows them to efficiently adapt existing models like GPT2 and LLaMA to create DiffuGPT and DiffuLLaMA without time-embedding layers, preserving the original models&#x27; capabilities while enabling the non-autoregressive generation benefits of diffusion models.</td>
                <td><a href="https://arxiv.org/pdf/2410.17891" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling</td>
                <td>Kaiwen Zheng et al</td>
                <td>2025</td>
                <td>diffusion, masked diffusion, mdm</td>
                <td>ICLR</td>
                <td>The authors provide a comprehensive theoretical analysis revealing that masked diffusion models (MDMs) are functionally equivalent to simpler masked language models despite their diffusion-based formulation. They mathematically prove that MDMs&#x27; continuous-time Evidence Lower Bound (ELBO) can be reformulated based on the number of masked tokens rather than time, effectively showing that the time variable serves merely as a continuous relaxation of the masked ratio. Building on this insight, they develop the First-Hitting Sampler (FHS), which analytically calculates when each masked token should be unmasked, enabling token-by-token decoding that is provably equivalent to MDMs&#x27; original sampling process but 20 times faster. The authors also uncover a critical numerical precision issue in Gumbel-based categorical sampling where 32-bit floating-point representation truncates maximum Gumbel values, artificially lowering the effective temperature during sampling. Through closed-form mathematical analysis, they demonstrate how this truncation shifts probability distributions and creates unequal unmasking probabilities across token positions, explaining why MDMs appeared to outperform auto-regressive models in previous evaluations when measured by generative perplexity. Their high-order sampling variants (using extrapolation and predictor-corrector approaches) further enhance efficiency while maintaining theoretical equivalence to the original MDM sampling process.</td>
                <td><a href="https://arxiv.org/pdf/2409.02908" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Simple and Effective Masked Diffusion Language Models</td>
                <td>Subham Sekhar Sahoo et al</td>
                <td>2024</td>
                <td>diffusion, masked diffusion, ddlm</td>
                <td>NeurIPS</td>
                <td>Masked Diffusion Language Models (MDLMs) improve upon previous discrete diffusion approaches by implementing a simplified, Rao-Blackwellized objective that serves as a weighted average of masked language modeling losses, enabling principled generation from encoder-only language models. This approach combines a substitution-based (SUBS) parameterization of the reverse unmasking process with effective training recipes that significantly outperform previous diffusion models across language benchmarks, approaching autoregressive perplexity within 15-25%. MDLMs support variable-length generation through a semi-autoregressive sampling algorithm that outperforms previous methods, while the framework extends to other domains like biological sequence modeling where it preserves or improves downstream task performance compared to traditional masked language models. The core insight is that through well-engineered training and a tighter variational bound, masked diffusion can be more competitive with autoregressive approaches than previously thought, providing alternative pathways for controllable text generation.</td>
                <td><a href="https://openreview.net/pdf?id=L4uaAR4ArM" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Structured Denoising Diffusion Models in Discrete State-Spaces</td>
                <td>Jacob Austin et al</td>
                <td>2023</td>
                <td>diffusion, discrete</td>
                <td>NeurIPS</td>
                <td>This paper introduces Discrete Denoising Diffusion Probabilistic Models (D3PMs), extending diffusion models to discrete data like text and images. Unlike previous approaches with uniform transition probabilities, D3PMs incorporate structured corruption processes through various transition matrices that can better model domain-specific relationships. The paper explores several matrix types: uniform transitions, absorbing states (like BERT&#x27;s masking), discretized Gaussian transitions for ordinal data, and token embedding distance for semantic relationships in text. D3PMs achieve state-of-the-art results for discrete diffusion, approaching or exceeding continuous diffusion models on image tasks while enabling effective text generation. The authors also establish connections between D3PMs and existing models like BERT and autoregressive language models, showing how these seemingly different approaches can be unified under the diffusion framework. A key contribution is a new auxiliary loss that stabilizes training and improves sample quality by combining variational inference with cross-entropy loss.</td>
                <td><a href="https://arxiv.org/pdf/2107.03006" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models</td>
                <td>Marianne Arriola et al</td>
                <td>2025</td>
                <td>diffusion, lldm</td>
                <td>ICLR</td>
                <td>Block Diffusion Language Models (BD3-LMs) introduce a hybrid approach that combines autoregressive modeling over blocks of tokens with diffusion within each block, effectively bridging discrete diffusion and autoregressive language models. This innovative design overcomes key limitations of existing diffusion models by supporting flexible-length generation and improving inference efficiency through KV caching and parallel token sampling. The authors identify gradient variance as a critical factor in the performance gap between diffusion and autoregressive models, and propose data-driven &quot;clipped&quot; noise schedules that significantly reduce this variance during training. Through extensive experiments, BD3-LMs establish a new state-of-the-art in perplexity among diffusion language models and demonstrate the ability to generate arbitrary-length sequences with improved quality using fewer generation steps than alternative approaches.</td>
                <td><a href="https://openreview.net/pdf?id=tyEyYT267x" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>CLLMs: Consistency Large Language Models</td>
                <td>Siqi Kou et al</td>
                <td>2024</td>
                <td>llms, consistency models</td>
                <td>ICML</td>
                <td>This paper introduces Consistency Large Language Models (CLLMs), a new approach that significantly improves the efficiency of Jacobi decoding for language model inference. The key innovation is training language models to consistently predict the fixed point of a Jacobi trajectory in fewer steps, addressing the main limitation of standard Jacobi decoding which typically predicts only one correct token per iteration. By fine-tuning target LLMs using a consistency loss that maps arbitrary points on the Jacobi trajectory to the fixed point, CLLMs achieve 2.4× to 3.4× improvements in generation speed while maintaining quality across both domain-specific and open-domain tasks. Unlike alternative approaches like speculative decoding or Medusa, CLLMs don&#x27;t require auxiliary model components or architectural modifications, making them memory-efficient and highly adaptable. The authors identify two key acceleration mechanisms in CLLMs: &quot;fast forwarding,&quot; where multiple consecutive tokens are correctly predicted in a single step, and &quot;stationary tokens,&quot; which remain correct despite being preceded by inaccurate tokens. These phenomena are especially prominent in domain-specific tasks where text contains predictable collocations and syntactic structures.</td>
                <td><a href="https://arxiv.org/pdf/2403.00835" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Large Language Diffusion Models</td>
                <td>Shen Nie et al</td>
                <td>2025</td>
                <td>lldm</td>
                <td>Arxiv</td>
                <td>This paper introduces LLaDA (Large Language Diffusion with mAsking), a diffusion model for natural language trained from scratch that challenges the dominance of autoregressive models (ARMs) in large language modeling. LLaDA models distributions through a forward data masking process and a reverse prediction process using a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. The research demonstrates that diffusion models can be a viable alternative to autoregressive models, showing comparable or superior performance on various benchmarks. LLaDA addresses the reversal curse that affects autoregressive models, showing consistent performance in both forward and reverse reasoning tasks. After scaling to 8B parameters and supervised fine-tuning, LLaDA demonstrates strong capabilities in in-context learning, instruction following, and multi-turn dialogue generation, challenging the assumption that these capabilities are inherently tied to autoregressive architectures.</td>
                <td><a href="https://arxiv.org/pdf/2502.09992" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>DORY: Deliberative Prompt Recovery for LLM</td>
                <td>Lirong Gao et al</td>
                <td>2024</td>
                <td>inversion</td>
                <td>Arxiv</td>
                <td>This paper introduces a novel approach to recover original prompts from limited outputs of large language models. The authors discover a strong negative correlation between output probability-based uncertainty and prompt recovery success, showing that tokens with lower uncertainty are more likely to have appeared in the original prompt. Building on this insight, DORY recovers prompts through a three-step process: reconstructing a draft from output text, generating hints based on uncertainty, and reducing noise by comparing draft output with actual output. Unlike previous methods, DORY requires only a single LLM without any external resources or model training, making it a cost-effective solution for prompt recovery.</td>
                <td><a href="https://arxiv.org/pdf/2405.20657" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Weak-to-Strong Reasoning</td>
                <td>Yuqing Yang et al</td>
                <td>2024</td>
                <td>reasoning</td>
                <td>Arxiv</td>
                <td>This paper introduces a progressive learning framework for weak-to-strong reasoning, addressing the challenge of improving large language models (LLMs) without high-quality supervision. The authors demonstrate that naively fine-tuning a stronger model (like Llama2-70b) on outputs from weaker models (like Llama2-7b or Gemma-2b) is insufficient for complex reasoning tasks. Their proposed two-stage approach first uses selective data curation through a &quot;final answer consistency&quot; method to identify potentially correct examples, then applies preference optimization that enables the model to learn from contrasting examples. Experiments on mathematical reasoning datasets show substantial improvements over baseline approaches, with the framework proving particularly effective when the strong model learns to distinguish between correct and incorrect reasoning paths.</td>
                <td><a href="https://arxiv.org/pdf/2407.13647" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>The False Promise of Imitating Proprietary LLMs</td>
                <td>Arnav Gudibande et al</td>
                <td>2023</td>
                <td>knowledge distillation, imitation</td>
                <td>Arxiv</td>
                <td>This paper analyzes the phenomenon of imitating stronger proprietary language models (like ChatGPT) by finetuning weaker open-source models on outputs from the stronger models. The authors discover that while imitation models appear competitive in human evaluations because they successfully mimic ChatGPT&#x27;s style, they fail to close the capabilities gap on factual knowledge, coding, and problem-solving tasks unless the imitation dataset specifically targets those domains. Their experiments show that scaling up the base model size leads to much more significant improvements than increasing the amount of imitation data, suggesting that developing better base models is more valuable than collecting more imitation data. The researchers conclude that model imitation is not a &quot;free lunch&quot; and that there exists a substantial capabilities gap between open and closed-source language models that cannot be easily bridged through imitation alone.</td>
                <td><a href="https://arxiv.org/pdf/2305.15717" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Born Again Networks</td>
                <td>Tommaso Furlanello et al</td>
                <td>2018</td>
                <td>knowledge distillation, self distillation</td>
                <td>ICML</td>
                <td>This paper introduces Born-Again Neural Networks (BANs), a novel approach where knowledge distillation is applied to train student models with identical architectures to their teachers. Surprisingly, these student models consistently outperform their teachers on both computer vision and language modeling tasks, even when the student and teacher have the exact same architecture and capacity. The authors experiment with multiple generations of BANs, showing that performance continues to improve (though with diminishing returns), and explore the effect of &quot;dark knowledge&quot; by testing variations where they either weight examples by teacher confidence or permute non-argmax outputs. Their framework also allows for cross-architecture knowledge transfer, such as training ResNet students from DenseNet teachers, resulting in state-of-the-art performance on CIFAR datasets and demonstrating that the benefits of knowledge distillation extend beyond model compression.</td>
                <td><a href="https://arxiv.org/pdf/1805.04770" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>The Curious Case of Neural Text Degeneration</td>
                <td>Ari Holtzman et al</td>
                <td>2020</td>
                <td>nucleus sampling, generation</td>
                <td>ICLR</td>
                <td>This work identifies fundamental problems with traditional text generation methods: beam search creates repetitive text while pure sampling produces incoherent content. As a solution, the authors propose Nucleus Sampling, which dynamically truncates the probability distribution to include only the most likely tokens that constitute the vast majority of the probability mass, avoiding both repetition and incoherence issues. Through extensive evaluations comparing perplexity, vocabulary distribution, self-similarity, and human judgments, they demonstrate that Nucleus Sampling produces text that is both high-quality and diverse, closely matching human-written text distributions. The authors also make the important observation that human language rarely maximizes probability, suggesting that language models which optimize for likelihood may inherently struggle to generate natural text.</td>
                <td><a href="https://arxiv.org/pdf/1904.09751" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Data-Free Knowledge Distillation for Deep Neural Networks</td>
                <td>Raphael Gontijo Lopes et al</td>
                <td>2017</td>
                <td>knowledge distillation, network inversion</td>
                <td>Arxiv</td>
                <td>This paper introduces a novel method for data-free knowledge distillation, which enables the compression of deep neural networks without requiring access to the original training dataset. The authors propose using various forms of activation metadata collected during the initial model training to reconstruct synthetic datasets that can then be used to train smaller student networks. They explore different approaches to creating this activation metadata, including top-layer statistics, all-layers statistics with dropout filters, and spectral methods based on graph Fourier transforms. Experimental results on MNIST and CelebA datasets demonstrate that their spectral methods can achieve compression rates of approximately 50% with minimal accuracy loss, making this approach valuable for scenarios where the original training data cannot be shared due to privacy concerns, storage limitations, or proprietary restrictions.</td>
                <td><a href="https://arxiv.org/pdf/1710.07535" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion</td>
                <td>Hongxu Yu et al</td>
                <td>2020</td>
                <td>knowledge distillation, network inversion</td>
                <td>CVPR</td>
                <td>The paper introduces DeepInversion, a method that synthesizes realistic, high-fidelity images from a trained CNN without requiring access to the original training data by using information stored in batch normalization layers. The authors further enhance this technique with Adaptive DeepInversion, which improves image diversity by maximizing Jensen-Shannon divergence between teacher and student network outputs. With these methods, the paper demonstrates three data-free applications: network pruning, knowledge transfer between models, and continual learning for adding new classes to existing networks. The synthesized images show impressive realism and generalize well across different model architectures, enabling knowledge distillation and other tasks that typically require the original training dataset.</td>
                <td><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_Dreaming_to_Distill_Data-Free_Knowledge_Transfer_via_DeepInversion_CVPR_2020_paper.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Sequence-Level Knowledge Distillation</td>
                <td>Yoon Kim et al</td>
                <td>2016</td>
                <td>knowledge distillation</td>
                <td>Arxiv</td>
                <td>This paper introduces sequence-level knowledge distillation for neural machine translation, allowing smaller student models to achieve performance comparable to larger teacher models. The authors demonstrate that their approach works better than standard word-level knowledge distillation by having students learn from complete translations generated by the teacher rather than just matching word-level probabilities. Remarkably, their method enables student models to perform well even with greedy decoding, eliminating the need for computationally expensive beam search at inference time. Combining their distillation techniques with weight pruning, they produce models with 13× fewer parameters than the original teacher model while maintaining strong translation performance, making efficient NMT deployment possible even on mobile devices.</td>
                <td><a href="https://arxiv.org/pdf/1606.07947" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>The Mamba in the Llama: Distilling and Accelerating Hybrid Models</td>
                <td>Junxiong Wang et al</td>
                <td>2025</td>
                <td>knowledge distillation, llm</td>
                <td>Arxiv</td>
                <td>This paper demonstrates how large Transformer models can be effectively distilled into hybrid models that incorporate linear RNNs like Mamba while maintaining much of their generation quality, notably by reusing the weights from attention layers. The researchers developed a multistage distillation approach combining progressive distillation, supervised fine-tuning, and directed preference optimization, which outperforms models trained from scratch with trillions of tokens. They also introduced a hardware-aware speculative decoding algorithm that significantly accelerates inference speed for both Mamba and hybrid architectures, achieving impressive throughput for large language models. The resulting hybrid models show comparable performance to the original Transformers on chat benchmarks while requiring less computational resources for deployment, highlighting how transformer knowledge can be effectively transferred to other architectures with customized inference profiles.</td>
                <td><a href="https://arxiv.org/pdf/2408.15237" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Compact Language Models via Pruning and Knowledge Distillation</td>
                <td>Saurav Muralidharan et al</td>
                <td>2024</td>
                <td>pruning, knowledge distillation</td>
                <td>Arxiv</td>
                <td>This paper investigates whether large language models (LLMs) can be efficiently compressed by pruning an existing model and retraining it with a fraction of the original training data, rather than training smaller variants from scratch. The authors develop and empirically explore best practices for structured LLM pruning across multiple dimensions (depth, width, attention, and MLP) combined with knowledge distillation-based retraining. Their approach produces the MINITRON family of models (8B and 4B variants) from the Nemotron-4 15B model using up to 40× fewer training tokens than training from scratch, while maintaining competitive performance compared to similarly-sized models like Mistral 7B, Gemma 7B, and Llama-3 8B. The methodology demonstrates significant compute savings (1.8×) for training a full model family and outperforms state-of-the-art compression techniques in the literature.</td>
                <td><a href="https://arxiv.org/pdf/2407.14679" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>MergeNet: Knowledge Migration across Heterogeneous Models, Tasks, and Modalities</td>
                <td>Kunxi Li et al</td>
                <td>2024</td>
                <td>model merging, knowledge distillation</td>
                <td>Arxiv</td>
                <td>The paper introduces MergeNet, a novel framework for knowledge transfer between heterogeneous models, tasks, and modalities. Unlike traditional methods like knowledge distillation that require similar model architectures or tasks, MergeNet facilitates knowledge transfer by operating directly on model parameters through low-rank decomposition and a specialized adapter that bridges different parameter spaces. The authors demonstrate MergeNet&#x27;s effectiveness through extensive experiments across challenging scenarios including cross-structure (different model architectures), cross-modal (image-text), and cross-task (classification-QA) knowledge transfer, consistently outperforming baseline methods. Their approach enables previously difficult knowledge transfers by allowing models to extract only the knowledge they need from source models, effectively addressing the issue of knowledge incompatibility between heterogeneous models.</td>
                <td><a href="https://arxiv.org/pdf/2404.13322" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Reuse, Don&#x27;t Retrain: A Recipe for Continued Pretraining of Language Models</td>
                <td>Jupinder Parmar et al</td>
                <td>2024</td>
                <td>pretraining</td>
                <td>Arxiv</td>
                <td>This paper introduces a recipe for effectively continuing the pretraining of large language models (LLMs) without having to retrain them from scratch. The authors demonstrate that using a two-phase data distribution approach—starting with general data similar to pretraining and transitioning to specialized data focused on model weaknesses—produces the best results when combined with a specific learning rate schedule that starts at the pretrained model&#x27;s minimum learning rate and decays with cosine annealing. They find that the optimal point to switch between data distributions occurs at one-fifth of the maximum learning rate, and demonstrate that their approach yields a 9% improvement in model accuracy compared to simply continuing training on the pretraining dataset. The recipe proves effective across different training scales (from 100B to 1T tokens) and includes innovations like document mining to identify the most useful examples for continued training, enabling developers to improve model capabilities without the massive computational costs of retraining from scratch.</td>
                <td><a href="https://arxiv.org/pdf/2407.07263" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>DistiLLM: Towards Streamlined Distillation for Large Language Models</td>
                <td>Jongwoo Ko et al</td>
                <td>2024</td>
                <td>knowledge distillation, llm</td>
                <td>ICML</td>
                <td>This paper introduces DISTILLM, a new knowledge distillation framework for large language models that addresses critical limitations in efficiency and effectiveness through two key components: a novel skew Kullback-Leibler divergence loss with strong theoretical foundations, and an adaptive off-policy approach that efficiently utilizes student-generated outputs. The skew KLD is mathematically proven to provide more stable gradients and minimal approximation errors compared to standard KLD objectives, while the adaptive off-policy approach uses a replay buffer to dramatically improve sample efficiency and reduce training time. Through extensive experiments on various tasks like instruction-following and text summarization, DISTILLM achieves state-of-the-art performance while requiring up to 4.3x less training time than existing methods. The framework demonstrates strong scalability across different model sizes (120M to 13B parameters) and model families, making it a practical solution for compressing large language models.</td>
                <td><a href="https://arxiv.org/pdf/2402.03898" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>MiniLLM: Knowledge Distillation of Large Language Models</td>
                <td>Yuxian Gu et al</td>
                <td>2024</td>
                <td>knowledge distillation, llm</td>
                <td>ICLR</td>
                <td>This paper introduces MiniLLM, a novel approach to knowledge distillation for large language models that uses reverse Kullback-Leibler divergence (KLD) instead of forward KLD, which prevents student models from overestimating low-probability regions of the teacher&#x27;s distribution. The authors develop an optimization approach using policy gradient with three key improvements: single-step decomposition to reduce variance, teacher-mixed sampling to prevent reward hacking, and length normalization to eliminate length bias. Through extensive experiments on various model sizes (120M to 13B parameters) and instruction-following tasks, they demonstrate that MiniLLM produces more precise responses with higher quality, lower exposure bias, better calibration, and improved long-text generation compared to baselines. Most importantly, they show the approach scales effectively across different model families and sizes while requiring significantly fewer training tokens than traditional methods, making it a practical solution for compressing large language models.</td>
                <td><a href="https://arxiv.org/pdf/2306.08543" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective</td>
                <td>Helong Zhou et al</td>
                <td>2021</td>
                <td>knowledge distillation</td>
                <td>ICLR</td>
                <td>The paper analyzes knowledge distillation through the lens of bias-variance tradeoff, discovering that soft labels create a sample-wise tradeoff where some training examples reduce variance at the cost of increased bias while others have different effects. The authors identify &quot;regularization samples&quot; where distillation primarily acts as a regularizer and find that their quantity negatively correlates with model performance in standard knowledge distillation. To address this, they propose &quot;weighted soft labels&quot; that adaptively weight each training sample&#x27;s contribution to optimally balance the bias-variance tradeoff, leading to improved distillation performance. The key insight is that regularization samples shouldn&#x27;t be completely ignored but rather have their influence carefully modulated through weighting, which the authors validate through both theoretical analysis and extensive experiments establishing new state-of-the-art results.</td>
                <td><a href="https://arxiv.org/pdf/2102.00650" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>The Unreasonable Ineffectiveness of the Deeper Layers</td>
                <td>Andrey Gromov et al</td>
                <td>2024</td>
                <td>layer analysis, pruning</td>
                <td>Arxiv</td>
                <td>This paper presents an empirical study of layer pruning in large language models, demonstrating that many layers can be removed without significant performance degradation until a critical threshold. The authors introduce a novel pruning approach that identifies optimal layers to remove by analyzing the similarity between layer representations, combined with a small amount of parameter-efficient finetuning to &quot;heal&quot; the model after pruning. They discover that LLMs are surprisingly robust to removing up to half of their layers, suggesting either that current pretraining methods don&#x27;t fully utilize deeper layers or that shallow layers play a crucial role in storing knowledge. A key finding is that while question-answering performance shows a sharp transition after removing critical layers, autoregressive loss changes smoothly, indicating an interesting disconnect between these different measures of model capability.</td>
                <td><a href="https://arxiv.org/pdf/2403.17887" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>LLM Pruning and Distillation in Practice: The Minitron Approach</td>
                <td>Sharath Turuvekere Sreenivas et al</td>
                <td>2024</td>
                <td>llm, pruning, distillation</td>
                <td>Arxiv</td>
                <td>The paper introduces an improved approach to LLM model compression by combining structured pruning with knowledge distillation, notably adding a &quot;teacher correction&quot; phase that allows the teacher model to adapt to new data distributions when the original pretraining dataset is unavailable. The authors explore two distinct pruning strategies - depth pruning (removing entire layers) and width pruning (reducing hidden/attention/MLP dimensions), along with a new task-based saliency criteria for depth pruning. They demonstrate this approach by successfully compressing Mistral NeMo 12B and Llama 3.1 8B models to 8B and 4B parameters respectively, using significantly fewer training tokens than training from scratch. The methodology is particularly valuable because it removes the dependency on accessing the original pretraining dataset, making it more practical for compressing proprietary models.</td>
                <td><a href="https://arxiv.org/pdf/2408.11796" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</td>
                <td>Rafael Rafailov et al</td>
                <td>2023</td>
                <td>rlhf, reward modeling</td>
                <td>NeurIPS</td>
                <td>The key innovation is that the paper eliminates the need for reinforcement learning in training language models from human preferences by introducing Direct Preference Optimization (DPO). DPO achieves this by leveraging a mathematical mapping between reward functions and optimal policies to transform preference learning into a simple classification problem. This allows language models to be trained directly from human preference data using a straightforward binary cross-entropy loss, while theoretically maintaining the same optimization objective as traditional RLHF methods.</td>
                <td><a href="https://arxiv.org/pdf/2408.16737" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling</td>
                <td>Hritik Bansal et al</td>
                <td>2024</td>
                <td>llm, compute-optimal, test-time compute</td>
                <td>Arxiv</td>
                <td>The paper&#x27;s key innovation is challenging the common practice of using larger language models to generate synthetic training data for reasoning tasks. It shows that at a fixed compute budget, generating more samples from a smaller, cheaper model can be more effective than fewer samples from a larger, more expensive model. The authors introduce a novel &quot;weak-to-strong improvement&quot; paradigm where a stronger model can be improved using data from a weaker model, while also providing a theoretical framework for compute-matched sampling between different model sizes.</td>
                <td><a href="https://arxiv.org/pdf/2408.16737" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                <td>Shunyu Yao et al</td>
                <td>2023</td>
                <td>llm, reasoning, acting</td>
                <td>Arxiv</td>
                <td>This paper introduces ReAct, a novel approach that combines reasoning and acting capabilities in large language models through prompting. The key innovation is having language models generate both verbal reasoning traces and task-specific actions in an interleaved manner, allowing models to maintain high-level plans while gathering information from external sources. Through experiments on question answering, fact verification, and interactive decision-making tasks, ReAct outperforms baselines that use only reasoning or only actions, while providing interpretable decision traces that enable human oversight of the model&#x27;s behavior.</td>
                <td><a href="https://arxiv.org/pdf/2210.03629" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>STaR: Self Taught Reasoner</td>
                <td>Eric Zelikman et al</td>
                <td>2022</td>
                <td>reasoning, llm</td>
                <td>Arxiv</td>
                <td>This paper proposes a method of iteratively leveraging a small number of rationale examples and a large dataset w/o rationales to bootstrap the ability to perform successively more complex reasoning. Basically it is a loop: if generated answers are wrong, try again to generate a rationale given the correct answer; finetune on all rationales that ultimately yielded correct answers, and repeat.</td>
                <td><a href="https://arxiv.org/pdf/2203.14465" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries</td>
                <td>Eden Biran et al</td>
                <td>2024</td>
                <td>multi-hop reasoning, llm</td>
                <td>Arxiv</td>
                <td>The paper shows that large language models solve multi-hop queries (like &quot;spouse of performer of Imagine&quot;) through a sequential process where early layers resolve the first hop (&quot;performer of Imagine&quot; → &quot;John Lennon&quot;), middle layers propagate this information, and later layers resolve the second hop (&quot;spouse of John Lennon&quot; → &quot;Yoko Ono&quot;), with performance limitations arising because later layers sometimes lack the knowledge needed for the second hop.</td>
                <td><a href="https://arxiv.org/pdf/2406.12775" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Distributed Reasoning in LLMs: Parallel Reasoning Processes in Multi-Hop Reasoning</td>
                <td>Yuval Shalev et al</td>
                <td>2024</td>
                <td>multi-hop reasoning, llm</td>
                <td>Arxiv</td>
                <td>The paper demonstrates that large language models perform multi-hop reasoning by generating interpretable distributions of potential intermediate answers in their middle layers, which are then linearly transformed into final answers through parallel reasoning paths - providing novel insights into how artificial neural networks implement both associative and structured reasoning processes.</td>
                <td><a href="https://arxiv.org/pdf/2406.13858" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Training Large Language Models to Reason in Continuous Latent Space</td>
                <td>Shibo Hao et al</td>
                <td>2024</td>
                <td>test-time compute, latent space</td>
                <td>Arxiv</td>
                <td>The paper introduces &quot;Coconut&quot; (Chain of Continuous Thought), which lets language models perform reasoning in continuous latent space rather than through discrete language tokens, leading to improved performance through implicit breadth-first search capabilities, especially on tasks requiring complex planning.</td>
                <td><a href="https://arxiv.org/pdf/2412.06769" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</td>
                <td>Eric Zelikman et al</td>
                <td>2024</td>
                <td>test-time compute, star</td>
                <td>Arxiv</td>
                <td>The paper presents Quiet-STaR, a technique that trains language models to generate internal rationales at each token position by using parallel sampling, learned meta-tokens, and reinforcement learning to optimize thoughts that improve future token prediction, demonstrating significant zero-shot performance gains on reasoning tasks.</td>
                <td><a href="https://arxiv.org/pdf/2403.09629" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step</td>
                <td>Yuntian Deng et al</td>
                <td>2024</td>
                <td>test-time compute, cot</td>
                <td>Arxiv</td>
                <td>The authors present &quot;Stepwise Internalization&quot; a simple yet effective method that gradually removes chain-of-thought reasoning steps during training while maintaining high performance, enabling a small GPT-2 model to solve complex multiplication problems with 99% accuracy and achieving state-of-the-art results on GSM8K math problems without using intermediate reasoning steps.</td>
                <td><a href="https://arxiv.org/pdf/2405.14838" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Deliberation in Latent Space via Differentiable Cache Augmentation</td>
                <td>Luyang Liu et al</td>
                <td>2024</td>
                <td>test-time compute, latent space, coprocessor</td>
                <td>Arxiv</td>
                <td>The paper introduces a differentiable coprocessor that augments a frozen LLM&#x27;s key-value cache with learned latent embeddings, enabling improved reasoning capabilities through asynchronous &quot;thinking&quot; in latent space rather than generating explicit intermediate steps as text.</td>
                <td><a href="https://arxiv.org/pdf/2412.17747" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Training Language Models to Self-Correct via Reinforcement Learning</td>
                <td>Aviral Kumar et al</td>
                <td>2024</td>
                <td>self-correct, reinforcement learning</td>
                <td>Arxiv</td>
                <td>The paper introduces SCoRe, a novel multi-turn reinforcement learning approach that successfully teaches language models to self-correct their own mistakes without external feedback by addressing two key failure modes of previous methods: distribution shift and behavior collapse.</td>
                <td><a href="https://arxiv.org/pdf/2409.12917" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Improving Factuality and Reasoning in Language Models through Multiagent Debate</td>
                <td>Yilun Du et al</td>
                <td>2023</td>
                <td>multiagent, agentic</td>
                <td>Arxiv</td>
                <td>The paper demonstrates that having multiple large language model instances debate and critique each other&#x27;s responses over multiple rounds leads to improved factual accuracy and reasoning capabilities compared to single-model approaches, without requiring access to model internals.</td>
                <td><a href="https://arxiv.org/pdf/2310.01798" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Large Language Models Cannot Self-Correct Reasoning Yet</td>
                <td>Jie Huang et al</td>
                <td>2024</td>
                <td>self-correction</td>
                <td>Arxiv</td>
                <td>This paper demonstrates that large language models cannot effectively self-correct their reasoning without external feedback, showing that existing self-correction methods either rely on oracle labels, perform worse than simpler alternatives, or benefit mainly from improved prompting rather than actual self-correction.</td>
                <td><a href="https://arxiv.org/pdf/2310.01798" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Think Before You Speak: Training Language Models with Pause Tokens</td>
                <td>Sachin Goyal et al</td>
                <td>2024</td>
                <td>test-time compute, meta-tokens</td>
                <td>Arxiv</td>
                <td>This paper introduces &quot;Pause Tokens&quot; which are a way of appending a sequence of tokens to the input prefix, and then delaying the output until the last pause token is seen.</td>
                <td><a href="https://arxiv.org/pdf/2310.02226" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</td>
                <td>Charlie Snell et al</td>
                <td>2024</td>
                <td>test-time compute</td>
                <td>Arxiv</td>
                <td>This paper explores the question of &quot;If an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenge prompt?&quot;. Good for references on various test-time compute strategies.</td>
                <td><a href="https://arxiv.org/pdf/2408.03314" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</td>
                <td>Tianle Cai et al</td>
                <td>2024</td>
                <td>speculative decoding, drafting, llm</td>
                <td>ICML</td>
                <td>This paper presents Medusa which augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. They also introduce a form of tree-based attention to process candidates. Through the Medusa heads, they obtain probability predictions for the subsequent K+1 tokens. These predictions enable them to create length-K+1 continuations as the candidates. In order to process multiple cnadidates concurrently, they structure their attention such that only tokens from the same continuation are regarded as historical data.For instance, they have in Figure 2 an example where the first Medusa head and generates some top two predictions while the second medusa head generates a top three for each of the top two from the first head. Instead of filling the entire attention mask, they only consider the mask from these 2*3 = 6 tokens, plus the standard identity line.</td>
                <td><a href="https://arxiv.org/pdf/2401.10774" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Recurrent Drafter for Fast Speculative Decoding in Large Language Models</td>
                <td>Yunfei Cheng et al</td>
                <td>2024</td>
                <td>speculative decoding, drafting, llm</td>
                <td>Arxiv</td>
                <td>This paper introduces ReDrafter (Recurrent Drafter) that uses an RNN as the draft model and conditions on the LLM&#x27;s hidden states. They use a beam search to explore the candidate seqeunces and then apply a dynamic tree attention alg to remove duplicated prefixes among the candidates to improve the speedup. They also train via knowledge distillation from LLMs to improve the alignment of the draft model&#x27;s predictions with those of the LLM.</td>
                <td><a href="https://arxiv.org/pdf/2403.09919" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>QuIP: 2-Bit Quantization of Large Language Models With Guarantees</td>
                <td>Jerry Chee et al</td>
                <td>2024</td>
                <td>quantization, block-wise</td>
                <td>Arxiv</td>
                <td>QuIP (quantization with incoherence processing) is a method based on the insight that quantization benefits from incoherent weight and Hessian mats, meaning that they benefit from the weights being even in magnitude and benefit from having the directions in whcih they are rounded to being unaligned with the coordinate axes. </td>
                <td><a href="https://arxiv.org/pdf/2307.13304" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient LLM Inference</td>
                <td>Wonsuk Jang et al</td>
                <td>2024</td>
                <td>quantization, block-wise</td>
                <td>Arxiv</td>
                <td>This paper introduces a block-wise quantization scheme that assigns a per-block optimal number format from a format book (they make their own format book called &quot;DialectFP4&quot;). &quot;Focusing on how to represent over how to scale&quot;.</td>
                <td><a href="https://arxiv.org/pdf/2501.01144v2" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>SpinQuant: LLM Quantization with Learned Rotations</td>
                <td>Zechun Liu et al</td>
                <td>2024</td>
                <td>quantization, spins, rotation</td>
                <td>Arxiv</td>
                <td>This paper uses two mergeable rotation matrices (R1, R2) that make rotationally invariant full-precision networks, and then apply two online Hadamard rotations (G3, R4) to further reduce the outliers so they can quantize activations and KV-cache quantizations. They then show how one can optimize these rotation matrices on Stiefel manifolds (orthogonal manifolds) using Cayley SGD. The reason for Cayley SGD and Stiefel manifolds is bc they need to optimize rotation matrices (R1, R2) such that they stay orthogonal during optimization. Regular SGD would break this constraint. By optimizing on Stiefel manifolds (space of all orthonormal matrices), they can specifc that the optimizations stays on a specific surface that only contains rotation matrices.</td>
                <td><a href="https://arxiv.org/pdf/2405.16406" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>SnapKV: LLM Knows What You are Looking for Before Generation</td>
                <td>Yuhong Li et al</td>
                <td>2024</td>
                <td>llm, kv cache</td>
                <td>Arxiv</td>
                <td>This paper identifies and selects the most important features per head to create compressed KV cache. It works in two stages: 1) vote for important previous features by taking the last segment of the prompt (&quot;observation window&quot;) and uses this window to analyze which parts of the earlier text (prefix) are most important. For each attn head, we aggregate the attn weights from queries in the observation window. Then we select the top-k positions based on the aggregated weights (k=p*L_prefix, where p is the compression weight) 2) then cluster and perform context preservation: we then use a pooling layer to cluster the selected important features. The last part of the prompt is kept as the observation window because they note that the attention patterns observed in the last window of the input sequence have high overlap rates (~80-90%) with the actual attention patterns used during generation.</td>
                <td><a href="https://arxiv.org/pdf/2404.14469" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</td>
                <td>Angelos Katharopoulos et al</td>
                <td>2020</td>
                <td>attention, transformer</td>
                <td>ICML</td>
                <td>This paper rephrases transformers as RNNs (title). They express the self-attention mechanism as a linear dot-product of kernel feature maps to make the complexity go from O(N^2) to O(N). Personal note: this is the 200th paper recorded on here, and the last of 2024! Summer of 2024 was when I began studying machine learning. Let&#x27;s keep it up!</td>
                <td><a href="https://arxiv.org/pdf/2006.16236" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Prefix-Tuning: Optimizing Continuous Prompts for Generation</td>
                <td>Xiang Lisa Li et al</td>
                <td>2021</td>
                <td>prefix-tuning, prompting, llm</td>
                <td>Arxiv</td>
                <td>This paper proposes prefix-tuning, which keeps language model params frozen but optimizes a continuous task-specific vector (prefix).</td>
                <td><a href="https://arxiv.org/pdf/2101.00190" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>The Power of Scale for Parameter-Efficient Prompt Tuning</td>
                <td>Brian Lester et al</td>
                <td>2021</td>
                <td>prompting, llm</td>
                <td>Arxiv</td>
                <td>This paper explores adding soft prompts to condition frozen language models. Basically, soft prompts are learned through back-propagation and can be used to finetune language models without fully retraining. They also introduce the idea of &quot;prompt ensembling&quot; which is basically using multiple soft prompts on a model and ensembling their outputs.</td>
                <td><a href="https://arxiv.org/pdf/2104.08691" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs</td>
                <td>Nguyen Nhat Minh et al</td>
                <td>2024</td>
                <td>sampling, llm</td>
                <td>Arxiv</td>
                <td>This paper introduces a neat trick to sample the next token. Min-p sampling basically adjusts the sampling threshold based on the model&#x27;s confidence. It does so by scaling according to the top token&#x27;s probability. This is a compelling alternative to other common sampling methods, like nucleus sampling.</td>
                <td><a href="https://arxiv.org/pdf/2407.01082" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>LASER: Attention with Exponential Transformation</td>
                <td>Sai Surya Duvvuri et al</td>
                <td>2024</td>
                <td>attention, gradients</td>
                <td>Arxiv</td>
                <td>This paper identifies that gradients backpropagated through the softmax operation often can be quite small. To mitigate this, they propose doing a dot-product attention with an exp()-transformed value matrix V (meaning, they do the attention calculation on exp(V)), which allows for a larger Jacobian (mitigating the small gradient issue).</td>
                <td><a href="https://arxiv.org/pdf/2411.03493" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Hyper-Connections</td>
                <td>Defa Zhu et al</td>
                <td>2024</td>
                <td>residual connections, hyper-connections</td>
                <td>Arxiv</td>
                <td>This paper introduces hyper-connections, which is a novel alternative to residual connections. Basically, they introduce learnable depth and width connections.</td>
                <td><a href="https://arxiv.org/pdf/2409.19606" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Remix-DiT: Mixing Diffusion Transformers for Multi-Expert Denoising</td>
                <td>Gongfan Fang et al</td>
                <td>2024</td>
                <td>dit, diffusion, moe</td>
                <td>NeurIPS</td>
                <td>This paper introduces a method of mixing diffusion models for multi-expert denoising. Basically, they increase the width of the linear layers by a factor of K, and then modify the forward pass to support it. This allows for K experts that are initialized from the original weights. </td>
                <td><a href="https://arxiv.org/pdf/2412.05628" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Hymba: A Hybrid-head Architecture for Small Language Models</td>
                <td>Xin Dong et al</td>
                <td>2024</td>
                <td>llm, hybrid, meta-tokens</td>
                <td>Arxiv</td>
                <td>This paper introduces a family of small language models that have a hybrid attention-ssm head parallel architecture. There are many interesting architectural designs to note here, but my favoriate is the use of &quot;meta tokens&quot;, learnable tokens that are prepended to prompts. These tokens help reduce the entropy of attention and ssm heads, and can be seen as a good initialization for KV cache and the SSM state.</td>
                <td><a href="https://arxiv.org/pdf/2411.13676" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>All are Worth Words: A ViT Backbone for Diffusion Models</td>
                <td>Fan Bao et al</td>
                <td>2023</td>
                <td>diffusion, vit</td>
                <td>Arxiv</td>
                <td>This paper designs a general ViT-based architecture for diffusion models. Notably, it treats all inputs (time, condition, noisy image patches) as tokens and uses long skip connections between the shallow and deep layers.</td>
                <td><a href="https://arxiv.org/pdf/2209.12152" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>SliceGPT: Compress Large Language Models by Deleting Rows and Columns</td>
                <td>Saleh Ashkboos et al</td>
                <td>2024</td>
                <td>pruning, llm</td>
                <td>ICLR</td>
                <td>The authors propose a method of slicing off entire rows or columns of weight matrices. They do this by applying a transformation that leaves the predictions invariant prior to the slice. The authors also introduce the notion of &quot;computational invariance&quot;, AKA that one can apply orthogonal matrix transformations to each weight matrix in the transformer without changing the model, which they use to edit the blocks in a transformer to project the activation matrix between blocks onto its principal components, and then slice. They make the key insight that if you insert linear layers with the orthogonal matrix Q before RMSNorm and Q^{T} after, the network remains unchanged, i.e. RMSNorm(XQ)Q^{T} = RMSNorm(X). They also note that since LayerNorm can be converted to RMSNorm, LayerNorm is the same story. To find the Qs they use a calibration dataset from the training set and run it thru the model. They then use the output of the network to find the orthogonal matrices of the next layers by computing the covariance matrix and then getting the eigenvalues (read the paper for more).</td>
                <td><a href="https://arxiv.org/pdf/2401.15024" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Visual Autoregressive Modeling: Scaling Image Generation via Next-Scale Prediction</td>
                <td>Key Tian et al</td>
                <td>2024</td>
                <td>tokens, reference model</td>
                <td>NeurIPS</td>
                <td>The paper proposes Visual AutoRegressive (VAR) modeling, which shifts the paradigm of autoregressive learning for image generation from sequential &quot;next-token prediction&quot; to &quot;next-scale prediction.&quot; This approach treats entire token maps at progressively finer resolutions as the autoregressive units, reflecting the coarse-to-fine manner in which humans perceive images. Unlike traditional models that flatten 2D spatial structures into 1D sequences, VAR preserves spatial locality and leverages multi-scale visual representations to reduce computational inefficiencies. By adopting hierarchical generation aligned with natural image structures, VAR overcomes the limitations of standard autoregressive models, such as mathematical premise violations and loss of spatial coherence. Its design integrates autoregressive transformers with multi-scale tokenization, creating a framework that is theoretically scalable and generalizable across diverse visual generation tasks.</td>
                <td><a href="https://openreview.net/pdf?id=gojL67CfS8" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Rho-1: Not All Tokens Are What You Need</td>
                <td>Zhenghao Lin et al</td>
                <td>2024</td>
                <td>tokens, reference model</td>
                <td>NeurIPS</td>
                <td>This paper scores tokens using a reference model and then trains a language model to focus on the tokens with higher scores. They find that they can improve performance while training on less tokens.</td>
                <td><a href="https://arxiv.org/pdf/2404.07965" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs</td>
                <td>Saleh Ashkboos et al</td>
                <td>2024</td>
                <td>quantization, rotation</td>
                <td>NeurIPS</td>
                <td>This paper introduces a quantization scheme based on rotations, that allows quantization of down to 4-bits for weights, activations, and KV cache. They rotate LLMs in such a way that removes outliers from hidden state w/o changing the output. In particular, they use randomized Hadamard transformations on the weight matrices to remove outlier features and make activations easier to quantize. They then extend this to apply online Hadamard transformations to attention model to remove outlier features in keys and values, which allows the KV cache to be quantized.</td>
                <td><a href="https://arxiv.org/pdf/2404.00456" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch</td>
                <td>Le Yu et al</td>
                <td>2024</td>
                <td>model merging</td>
                <td>ICML</td>
                <td>This paper shows that language models (LMs) can get new abilities via assimilating params from homologous models. They also note that LMs after Supervised Fine-Tuning (SFT) have many redundant delta parameters (i.e, the alteration of the model params before and after SFT). They then present DARE (Drop And REscale) as a means of setting delta parameters to zero with drop rate of p and then rescaling the remaining ones by a factor of 1/(1-p). They then use DARE to remove redundant delta parameters in each model prior to merging, which they find can help mitigate the interference of params among multiple models. Then they use standard model merging techniqes to merge the models.</td>
                <td><a href="https://arxiv.org/pdf/2311.03099" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch</td>
                <td>Le Yu et al</td>
                <td>2024</td>
                <td>model merging</td>
                <td>ICML</td>
                <td>This paper shows that language models (LMs) can get new abilities via assimilating params from homologous models. They also note that LMs after Supervised Fine-Tuning (SFT) have many redundant delta parameters (i.e, the alteration of the model params before and after SFT). They then present DARE (Drop And REscale) as a means of setting delta parameters to zero with drop rate of p and then rescaling the remaining ones by a factor of 1/(1-p). They then use DARE to remove redundant delta parameters in each model prior to merging, which they find can help mitigate the interference of params among multiple models. Then they use standard model merging techniqes to merge the models.</td>
                <td><a href="https://arxiv.org/pdf/2311.03099" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Training-Free Pretrained Model Merging</td>
                <td>Zhengqi Xu et al</td>
                <td>2024</td>
                <td>model merging</td>
                <td>CVPR</td>
                <td>This paper introduces Merging under Dual-Space Constraints (MuDSC), a novel framework for merging pretrained neural network models without additional training or requiring the same pretraining initialization. Unlike prior approaches that operate solely in either the weight space or the activation space, MuDSC addresses inconsistencies between these two spaces by combining their similarity measures into a unified objective using a weighted linear combination. This approach ensures that merged units are similar in both their structure and behavior, leading to more consistent and effective merging outcomes. The framework also adapts to networks with group structures, such as those using multi-head attention or group normalization, by proposing modifications to unit-matching algorithms. Overall, MuDSC simplifies model merging while enhancing performance across diverse architectures and tasks, enabling merged models to achieve balanced and overlapping multi-task performance.</td>
                <td><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Training-Free_Pretrained_Model_Merging_CVPR_2024_paper.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Similarity of Neural Network Representations Revisited</td>
                <td>Simon Kornblith et al</td>
                <td>2019</td>
                <td>network similarity</td>
                <td>ICML</td>
                <td>This paper examines methods for comparing neural network representations and proposes Centered Kernel Alignment (CKA) as a more effective similarity measure. The authors provide key theoretical insights about what properties a similarity metric should have - arguing it should be invariant to orthogonal transformations and isotropic scaling, but not to arbitrary invertible linear transformations, as neural network training itself isn&#x27;t invariant to such transformations. They show that for representations with more dimensions than training examples, any metric invariant to arbitrary invertible transformations will give meaningless results. CKA works by first measuring the similarity between every pair of examples in each representation separately (creating representational similarity matrices), then comparing these similarity structures - when using inner products, this reduces to computing normalized Hilbert-Schmidt Independence Criterion between the representations. They demonstrate theoretically that CKA is closely related to canonical correlation analysis (CCA) and regression, but incorporates feature scale information that CCA discards. Finally, they show that unlike previous methods like CCA and SVCCA, CKA can reliably identify corresponding layers between networks trained from different initializations and reveal meaningful relationships between different architectures.</td>
                <td><a href="https://arxiv.org/pdf/1905.00414" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>What is being transferred in transfer learning?</td>
                <td>Behnam Neyshabur et al</td>
                <td>2020</td>
                <td>transfer learning</td>
                <td>NeurIPS</td>
                <td>This paper investigated what exactly gets transferred during transfer learning in neural networks through a comprehensive series of analyses. Through experiments with block-shuffled images, the researchers demonstrated that successful transfer learning relies on both feature reuse and low-level statistics of the data, showing that even when visual features are disrupted, transfer learning still provides benefits. The study revealed that models fine-tuned from pre-trained weights tend to stay in the same basin in the loss landscape, make similar mistakes, and remain close to each other in parameter space, while models trained from scratch end up in different basins with more diverse behaviors. By analyzing module criticality, they found that lower layers handle more general features while higher layers become more specialized, confirming previous theories about feature hierarchy in neural networks. Finally, they showed that transfer learning can begin from earlier checkpoints of the pre-trained model without losing accuracy, suggesting that the benefits of pre-training emerge before the model fully converges on the source task.</td>
                <td><a href="https://arxiv.org/pdf/2008.11687" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>ZipIt! Merging Models from Different Tasks without Training</td>
                <td>George Stoica et al</td>
                <td>2024</td>
                <td>model merging</td>
                <td>ICLR</td>
                <td>This paper presents a novel approach to model merging that significantly improves upon previous methods by recognizing that similar features can exist within the same model, not just across different models. The key insight is that when merging models trained on different tasks, it&#x27;s often better to combine similar features within each model first, rather than forcing dissimilar features from different models to merge, as these features may have developed to solve fundamentally different problems. Their method first concatenates the feature spaces of both models and computes a comprehensive correlation matrix between all features (both within and across models), using these correlations to guide intelligent feature merging decisions. To handle the multi-layer nature of neural networks, they introduce an &quot;unmerge&quot; operation that allows the merged features to remain compatible with later layers in both original networks, essentially decompressing the merged features before they&#x27;re processed by subsequent layers. Theoretically, they prove that this approach provides better guarantees than traditional cross-model merging, showing that when models have internal redundancy (which is common in practice), their method can achieve perfect merging with zero performance loss.</td>
                <td><a href="https://arxiv.org/pdf/2305.03053" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts</td>
                <td>Ruida Wang et al</td>
                <td>2024</td>
                <td>llm, llm agent</td>
                <td>Arxiv</td>
                <td>This research introduces TheoremLlama, a framework that transforms general-purpose Large Language Models (LLMs) into expert theorem provers for the Lean4 formal mathematics language, addressing a significant challenge in automated theorem proving. The key innovation is their &quot;NL-FL bootstrapping&quot; method, which integrates natural language reasoning steps directly into formal mathematical proofs as comments during training, helping LLMs bridge the gap between natural language understanding and formal mathematical reasoning. The researchers also contribute the Open Bootstrapped Theorems (OBT) dataset, containing over 100,000 theorem-proof pairs with aligned natural and formal language, helping address the scarcity of training data in this field. The framework introduces specialized training techniques like block training and curriculum learning that help LLMs gradually build theorem-proving capabilities, potentially offering a blueprint for adapting LLMs to other specialized domains that lack extensive training data.</td>
                <td><a href="https://arxiv.org/pdf/2407.03203" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>A Simple Early Exiting Framework for Accelerating Sampling in Diffusion Models</td>
                <td>Taehong Moon et al</td>
                <td>2024</td>
                <td>diffusion, early exit</td>
                <td>ICML</td>
                <td>This paper presents Adaptive Score Estimation (ASE), a novel framework that accelerates diffusion model sampling by adaptively allocating computational resources based on the time step being processed. The authors observe that score estimation near the noise distribution (t→1) requires less computational power than estimation near the data distribution (t→0), leading them to develop a time-dependent early-exiting scheme where more neural network blocks are skipped during the noise-phase sampling steps. Their approach differs between architectures - for DiT models they skip entire blocks, while for U-ViT models they preserve the linear layers connected to skip connections while dropping other block components to maintain the residual pathway information. The authors fine-tune their models using a specially designed training procedure that employs exponential moving averages and weighted coefficients to ensure minimal information updates near t→0 while allowing more updates near t→1.</td>
                <td><a href="https://arxiv.org/pdf/2408.05927" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Active Prompting with Chain-of-Thought for Large Language Models</td>
                <td>Shizhe Diao et al</td>
                <td>2023</td>
                <td>prompting, cot</td>
                <td>Arxiv</td>
                <td>The paper introduces Active-Prompt, a novel method that improves chain-of-thought (CoT) prompting by strategically selecting which examples to use as demonstrations for large language models. Rather than using randomly selected or manually crafted examples, Active-Prompt identifies the most informative examples by measuring the model&#x27;s uncertainty on different potential prompts through metrics like disagreement and entropy across multiple model outputs. The key insight is that by systematically choosing examples where the model shows high uncertainty, and then having humans provide detailed reasoning chains for those specific cases, the resulting prompts will be more effective at teaching the model how to approach challenging problems. This approach shifts the human effort from trying to intuitively guess good examples to a more principled selection process guided by the model&#x27;s own uncertainty signals.</td>
                <td><a href="https://arxiv.org/pdf/2302.12246" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment</td>
                <td>Hanze Dong et al</td>
                <td>2023</td>
                <td>watermark, offset learning</td>
                <td>TMLR</td>
                <td>The paper introduces RAFT (Reward rAnked FineTuning), a simpler alternative to RLHF for aligning generative models with human preferences. The key insight is decoupling the data generation and model fine-tuning steps - instead of using complex reinforcement learning, RAFT generates multiple samples for each prompt, ranks them by reward, and then fine-tunes the model on only the highest-scoring samples in an iterative process. This approach is more stable and efficient than RLHF because it uses standard supervised learning techniques rather than RL, while being less sensitive to reward scaling issues since it only uses relative rankings rather than absolute reward values. Additionally, the decoupled nature of RAFT means it requires less memory (only needs to load one model at a time vs multiple for RLHF) and allows more flexibility in data collection and processing.</td>
                <td><a href="https://arxiv.org/pdf/2304.06767" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection</td>
                <td>Minzhou Pan et al</td>
                <td>2024</td>
                <td>watermark, offset learning</td>
                <td>Arxiv</td>
                <td>The key insight of this paper centers on using &quot;offset learning&quot; to detect invisible watermarks in images. The intuition is that by having a clean reference dataset of similar images, you can effectively &quot;cancel out&quot; the normal image features that are common between clean and watermarked images, leaving only the watermark perturbations. They design an asymmetric loss function where clean images use exponential/softmax loss (to focus on hard examples) while detection dataset uses linear loss (to give equal weight to all examples), helping isolate the watermark signal. This is combined with an iterative pruning strategy that gradually removes likely-clean images from the detection set, allowing the model to better focus on and learn the watermark patterns. By formulating watermark detection this way, they avoid needing any prior knowledge of watermarking techniques or labeled data, making it a truly black-box approach.</td>
                <td><a href="https://arxiv.org/pdf/2403.15955" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Mitigating the Alignment Tax of RLHF</td>
                <td>Yong Lin et al</td>
                <td>2024</td>
                <td>rlhf, alignment</td>
                <td>Arxiv</td>
                <td>This paper investigates the &quot;alignment tax&quot; problem where large language models lose some of their pre-trained abilities when aligned with human preferences through RLHF. The key insight is that model averaging (interpolating between pre-RLHF and post-RLHF model weights) is surprisingly effective at mitigating this trade-off because tasks share overlapping feature spaces, particularly in lower layers of the model. Building on this understanding, they propose Heterogeneous Model Averaging (HMA) which applies different averaging ratios to different layers of the transformer model, allowing optimization of the alignment-forgetting trade-off. The intuition is that since different layers capture different levels of features and task similarities, they should not be averaged equally, and finding optimal layer-specific averaging ratios can better preserve both alignment and pre-trained capabilities.</td>
                <td><a href="https://arxiv.org/pdf/2309.06256" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising</td>
                <td>Zigeng Chen et al</td>
                <td>2024</td>
                <td>diffusion, parallelization, denoising</td>
                <td>Arxiv</td>
                <td>This paper introduces AsyncDiff, a novel approach to accelerate diffusion models through parallel processing across multiple devices. The key insight is that hidden states between consecutive diffusion steps are highly similar, which allows them to break the traditional sequential dependency chain of the denoising process by transforming it into an asynchronous one. They execute this by dividing the denoising model into multiple components distributed across different devices, where each component uses the output from the previous component&#x27;s prior step as an approximation of its input, enabling parallel computation. To further enhance efficiency, they introduce stride denoising, which completes multiple denoising steps simultaneously through a single parallel computation batch and reduces the frequency of communication between devices. This solution is particularly elegant because it&#x27;s universal and plug-and-play, requiring no model retraining or architectural changes to achieve significant speedups while maintaining generation quality.</td>
                <td><a href="https://arxiv.org/pdf/2406.06911" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>DoRA: Weight-Decomposed Low-Rank Adaptation</td>
                <td>Shih-Yang Liu et al</td>
                <td>2024</td>
                <td>peft, lora</td>
                <td>Arxiv</td>
                <td>This paper introduces DoRA (Weight-Decomposed Low-Rank Adaptation), a novel parameter-efficient fine-tuning method that decomposes pre-trained weights into magnitude and direction components for separate optimization. Through a detailed weight decomposition analysis, the authors reveal that LoRA and full fine-tuning exhibit distinct learning patterns, with LoRA showing proportional changes in magnitude and direction while full fine-tuning demonstrates more nuanced, independent adjustments between these components. Based on this insight, DoRA uses LoRA specifically for directional updates while allowing independent magnitude optimization, which simplifies the learning task compared to having LoRA learn both components simultaneously. The authors also provide theoretical analysis showing how this decomposition benefits optimization by aligning the gradient&#x27;s covariance matrix more closely with the identity matrix and demonstrate mathematically why DoRA&#x27;s learning pattern more closely resembles full fine-tuning.</td>
                <td><a href="https://arxiv.org/pdf/2402.09353" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>SphereFed: Hyperspherical Federated Learning</td>
                <td>Xin Dong et al</td>
                <td>2022</td>
                <td>federated learning</td>
                <td>Arxiv</td>
                <td>This paper presents a novel approach to addressing the non-i.i.d. (non-independent and identically distributed) data challenge in federated learning by introducing hyperspherical federated learning (SphereFed). The key insight is that instead of letting clients independently learn their classifiers, which leads to inconsistent learning targets across clients, they should share a fixed classifier whose weights span a unit hypersphere, ensuring all clients work toward the same learning objectives. The approach normalizes features to project them onto this same hypersphere and uses mean squared error loss instead of cross-entropy to avoid scaling issues that arise when working with normalized features. Finally, after federated training is complete, they propose a computationally efficient way to calibrate the classifier using a closed-form solution that can be computed in a distributed manner without requiring direct access to private client data.</td>
                <td><a href="https://arxiv.org/pdf/2207.09413" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>A deeper look at depth pruning of LLMs</td>
                <td>Shoaib Ahmed Siddiqui et al</td>
                <td>2024</td>
                <td>pruning, depth pruning, llm</td>
                <td>ICML</td>
                <td>This paper explored different approaches to pruning large language models, revealing that while static metrics like cosine similarity work well for maintaining MMLU performance, adaptive metrics like Shapley values show interesting trade-offs between different tasks. A key insight was that self-attention layers are significantly more amenable to pruning compared to feed-forward layers, suggesting that models can maintain performance even with substantial attention layer reduction. The paper also demonstrated that simple performance recovery techniques, like applying an average update in place of removed layers, can be as effective or better than more complex approaches like low-rank adapters. Finally, the work highlighted how pruning affects different tasks unequally - while some metrics preserve performance on certain tasks like MMLU, they may significantly degrade performance on others like mathematical reasoning tasks.</td>
                <td><a href="https://www.arxiv.org/pdf/2407.16286" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Editing Models with Task Arithmetic</td>
                <td>Gabriel Ilharco et al</td>
                <td>2023</td>
                <td>task arithmetic, finetuning, task</td>
                <td>ICLR</td>
                <td>This paper introduces a novel method for model editing called task arithmetic, where &quot;task vectors&quot; represent specific tasks by capturing the difference between pre-trained and fine-tuned model weights. Task vectors can be manipulated mathematically, such as being negated to unlearn tasks or added together to enable multi-tasking or improve performance in novel settings. A standout finding is the ability to create new task capabilities through analogies (e.g., &quot;A is to B as C is to D&quot;), which allows performance improvement on tasks with little or no data. This method is computationally efficient, leveraging linear operations on model weights without incurring extra inference costs, providing a flexible and modular framework for modifying models post-training. The approach highlights significant advantages in adapting existing models while bypassing costly re-training or data access constraints.</td>
                <td><a href="https://arxiv.org/pdf/2212.04089" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales</td>
                <td>Tianyang Xu et al</td>
                <td>2024</td>
                <td>confidence estimation, llm</td>
                <td>Arxiv</td>
                <td>The SaySelf framework trains large language models (LLMs) to produce fine-grained confidence estimates and self-reflective rationales by focusing on internal uncertainties. It consists of two stages: supervised fine-tuning and reinforcement learning (RL). In the first stage, multiple reasoning chains are sampled from the LLM, clustered for semantic similarity, and analyzed by an advanced LLM to generate rationales summarizing uncertainties. The model is fine-tuned on a dataset that pairs questions with reasoning chains, rationales, and confidence estimates, using a loss function that optimizes the generation of all three outputs. In the second stage, RL refines the confidence predictions using a reward function that encourages accurate, high-confidence outputs while penalizing overconfidence in incorrect responses. The framework ensures that LLMs not only generate confidence scores but also provide explanations for their uncertainty, making their outputs more interpretable and calibrated.</td>
                <td><a href="https://arxiv.org/pdf/2405.20974" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Deep Reinforcement Learning from Human Preferences</td>
                <td>Paul F Christiano et al</td>
                <td>2016</td>
                <td>rl, rlhf</td>
                <td>Arxiv</td>
                <td>This paper introduces a method to train reinforcement learning (RL) systems using human preferences over trajectory segments rather than traditional reward functions. The approach allows agents to learn tasks that are hard to define programmatically, enabling non-expert users to provide feedback on agent behavior through comparisons of short video clips. By learning a reward model from these preferences, the method dramatically reduces the need for human oversight while maintaining adaptability to large-scale and complex RL environments. This paradigm bridges the gap between human-defined objectives and scalable RL systems, addressing challenges in alignment and usability for real-world applications.</td>
                <td><a href="https://arxiv.org/pdf/1706.03741" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning</td>
                <td>Tian Jin et al</td>
                <td>2023</td>
                <td>pruning, icl</td>
                <td>Arxiv</td>
                <td>This paper explores the effects of scaling the parameter count of large language models (LLMs) on two distinct capabilities: fact recall from pre-training and in-context learning (ICL). By investigating both dense scaling (training models of varying sizes) and pruning (removing weights), the authors identify that these approaches disproportionately affect fact recall while preserving ICL abilities. They demonstrate that a model&#x27;s ability to learn from in-context information remains robust under significant parameter reductions, whereas the ability to recall pre-trained facts degrades with even moderate scaling down. This dichotomy highlights a fundamental difference in how these capabilities rely on model size and opens avenues for more efficient model design and deployment, emphasizing trade-offs between memory augmentation and parameter efficiency.</td>
                <td><a href="https://arxiv.org/pdf/2310.04680" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Fine-Tuning Language Models with Just Forward Passes</td>
                <td>Sadhika Malladi et al</td>
                <td>2024</td>
                <td>finetuning, zo, optimization</td>
                <td>Arxiv</td>
                <td>The paper introduces MeZO, a memory-efficient zeroth-order optimization method, to fine-tune large language models using forward passes alone. Classical zeroth-order methods scale poorly with model size, but MeZO adapts these approaches to leverage structured pre-trained model landscapes, avoiding catastrophic slowdown even with billions of parameters. The authors theoretically show that MeZO’s convergence depends on the local effective rank of the Hessian, not the number of parameters, enabling efficient optimization despite prior bounds suggesting otherwise. Furthermore, MeZO’s flexibility allows optimization of non-differentiable objectives (e.g., accuracy or F1 score) and compatibility with parameter-efficient tuning methods like LoRA and prefix-tuning.</td>
                <td><a href="https://arxiv.org/pdf/2305.17333" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</td>
                <td>Hanshi Sun et al</td>
                <td>2024</td>
                <td>kv cache</td>
                <td>Arxiv</td>
                <td>The key insight of this paper lies in optimizing long-context large language model  inference by addressing the memory and latency bottlenecks associated with managing the key-value (KV) cache. The authors observe that pre-Rotary Position Embedding (RoPE) keys exhibit a low-rank structure, allowing them to be compressed without accuracy loss, while value caches lack this property and are therefore offloaded to the CPU to reduce GPU memory usage. To minimize decoding latency, they leverage landmarks—compact representations of the low-rank key cache—and identify a small set of outliers to be retained on the GPU, enabling efficient reconstruction of sparse KV pairs on-the-fly. This approach allows the system to handle significantly longer contexts and larger batch sizes while maintaining inference throughput and accuracy.</td>
                <td><a href="https://arxiv.org/pdf/2410.21465" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning</td>
                <td>Rui Pan et al</td>
                <td>2024</td>
                <td>peft, finetuning, sampling</td>
                <td>Arxiv</td>
                <td>The key insight of this paper is the discovery of a skewed weight-norm distribution across layers during LoRA fine-tuning, where the majority of updates occur in the bottom (embedding) and top (language modeling head) layers, leaving middle layers underutilized. This highlights that different layers have varied importance and suggests that selectively updating layers could improve efficiency without sacrificing performance. Building on this, the authors propose Layerwise Importance Sampling AdamW (LISA), which randomly freezes most middle layers during training, using importance sampling to emulate LoRA’s fast learning pattern while avoiding its low-rank constraints. This approach achieves significant memory savings, faster convergence, and superior performance compared to LoRA and full-parameter fine-tuning, particularly in large-scale and domain-specific tasks.</td>
                <td><a href="https://arxiv.org/pdf/2403.17919" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion</td>
                <td>Muyang Li et al</td>
                <td>2024</td>
                <td>quantization, diffusion</td>
                <td>Arxiv</td>
                <td>SVDQuant introduces a novel approach to 4-bit quantization of diffusion models by using a low-rank branch to absorb outliers in both weights and activations, making quantization more feasible at such aggressive bit reduction. The method first consolidates outliers from activations to weights through smoothing, then decomposes the weights using Singular Value Decomposition (SVD) to separate the dominant components into a 16-bit low-rank branch while keeping the residual in 4 bits. To make this practical, they developed an inference engine called Nunchaku that fuses the low-rank and low-bit branch kernels together, eliminating redundant memory access that would otherwise negate the performance benefits. The approach is designed to work across different diffusion model architectures and can seamlessly integrate with existing low-rank adapters (LoRAs) without requiring re-quantization.</td>
                <td><a href="https://arxiv.org/pdf/2411.05007" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>One Weight Bitwidth to Rule Them All</td>
                <td>Ting-Wu Chin et al</td>
                <td>2020</td>
                <td>quantization, bitwidth</td>
                <td>Arxiv</td>
                <td>This paper examines weight quantization in deep neural networks and challenges the common assumption that using the lowest possible bitwidth without accuracy loss is optimal. The key insight is that when considering model size as a constraint and allowing network width to vary, some bitwidths consistently outperform others - specifically, networks with standard convolutions work better with binary weights while networks with depthwise convolutions prefer higher bitwidths. The authors discover that this difference is related to the number of input channels (fan-in) per convolutional kernel, with higher fan-in making networks more resilient to aggressive quantization. Most surprisingly, they demonstrate that using a single well-chosen bitwidth throughout the network can outperform more complex mixed-precision quantization approaches when comparing networks of equal size, suggesting that the traditional focus on minimizing bitwidth without considering network width may be suboptimal.</td>
                <td><a href="https://arxiv.org/pdf/2008.09916" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Consistency Models</td>
                <td>Yang Song et al</td>
                <td>2023</td>
                <td>diffusion, ode, consistency</td>
                <td>ICML</td>
                <td>This paper introduces consistency models, a new family of generative models that can generate high-quality samples in a single step while preserving the ability to trade compute for quality through multi-step sampling. The key innovation is training models to map any point on a probability flow ODE trajectory to its origin point, enforcing consistency across different time steps through either distillation from pre-trained diffusion models or direct training. The models support zero-shot data editing capabilities like inpainting, colorization, and super-resolution without requiring explicit training on these tasks, similar to diffusion models. The authors provide two training approaches - consistency distillation which leverages existing diffusion models, and consistency training which allows training from scratch without any pre-trained models, establishing consistency models as an independent class of generative models.</td>
                <td><a href="https://arxiv.org/pdf/2303.01469" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>One Step Diffusion via ShortCut Models</td>
                <td>Kevin Frans et al</td>
                <td>2024</td>
                <td>diffusion, ode, flow-matching</td>
                <td>Arxiv</td>
                <td>This paper introduces shortcut models, a new type of diffusion model that enables high-quality image generation in a single forward pass by conditioning the model not only on the timestep but also on the desired step size, allowing it to learn larger jumps during the denoising process. Unlike previous approaches that require multiple training phases or complex scheduling, shortcut models can be trained end-to-end in a single phase by leveraging a self-consistency property where one large step should equal two consecutive smaller steps, combined with flow-matching loss as a base case. The key insight is that by conditioning on step size, the model can account for future curvature in the denoising path and jump directly to the correct next point rather than following the curved path naively, which would lead to errors with large steps. The approach simplifies the training pipeline while maintaining flexibility in inference budget, as the same model can generate samples using either single or multiple steps after training.</td>
                <td><a href="https://arxiv.org/abs/2410.12557" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models</td>
                <td>Hongjie Wang et al</td>
                <td>2024</td>
                <td>diffusion, training-free, attention, token pruning</td>
                <td>CVPR</td>
                <td>This paper introduces AT-EDM, a training-free framework to accelerate diffusion models by pruning redundant tokens during inference without requiring model retraining. The key innovation is a Generalized Weighted PageRank (G-WPR) algorithm that uses attention maps to identify and prune less important tokens, along with a novel similarity-based token recovery method that fills in pruned tokens based on attention patterns to maintain compatibility with convolutional layers. The authors also propose a Denoising-Steps-Aware Pruning (DSAP) schedule that prunes fewer tokens in early denoising steps when attention maps are more chaotic and less informative, and more tokens in later steps when attention patterns are better established. The overall approach focuses on making diffusion models more efficient by leveraging the rich information contained in attention maps to guide token pruning decisions while maintaining image generation quality.</td>
                <td><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Attention-Driven_Training-Free_Efficiency_Enhancement_of_Diffusion_Models_CVPR_2024_paper.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</td>
                <td>Tim Salimans et al</td>
                <td>2016</td>
                <td>normalization, gradient descent</td>
                <td>Arxiv</td>
                <td>This paper introduces weight normalization, a simple reparameterization technique that decouples a neural network&#x27;s weight vectors into their direction and magnitude by expressing w = (g/||v||)v, where g is a scalar and v is a vector. The key insight is that this decoupling improves optimization by making the conditioning of the gradient better - the direction and scale of weight updates can be learned somewhat independently, which helps avoid problems with pathological curvature in the optimization landscape. While inspired by batch normalization, weight normalization is deterministic and doesn&#x27;t add noise to gradients or create dependencies between minibatch examples, making it well-suited for scenarios like reinforcement learning and RNNs where batch normalization is problematic. The authors also propose a data-dependent initialization scheme where g and bias terms are initialized to normalize the initial pre-activations of neurons, helping ensure good scaling of activations across layers at the start of training.</td>
                <td><a href="https://arxiv.org/pdf/1602.07868" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models</td>
                <td>Tuomas Kynkäänniemi et al</td>
                <td>2024</td>
                <td>diffusion, cfg, guidance</td>
                <td>Arxiv</td>
                <td>This paper&#x27;s key insight is that classifier-free guidance (CFG) in diffusion models should only be applied during a specific interval of noise levels in the middle of the sampling process, rather than throughout the entire sampling chain as traditionally done. The intuition is that guidance is harmful at high noise levels (where it causes mode collapse and template-like outputs), largely unnecessary at low noise levels, and only truly beneficial in the middle range. They demonstrate this theoretically using a 1D synthetic example where they can visualize how guidance at high noise levels causes sampling trajectories to drift far from the smoothed data distribution, leading to mode dropping. Beyond this theoretical demonstration, they propose a simple solution of making the guidance weight a piecewise function that only applies guidance within a specific noise level interval.</td>
                <td><a href="https://arxiv.org/pdf/2404.07724" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Cache Me if You Can: Accelerating Diffusion Models through Block Caching</td>
                <td>Felix Wimbauer et al</td>
                <td>2024</td>
                <td>diffusion, caching, distillation</td>
                <td>Arxiv</td>
                <td>This paper introduces &quot;block caching&quot; to accelerate diffusion models by reusing computations across denoising steps. The key insight is that many layer blocks (particularly attention blocks) in diffusion models change very gradually during the denoising process, making their repeated computation redundant. The authors propose automatically determining which blocks to cache and when to refresh them based on measuring the relative changes in block outputs across timesteps. They also introduce a lightweight scale-shift adjustment mechanism that uses a student-teacher setup, where the student (cached model) learns additional scale and shift parameters to better align its cached block outputs with those of the teacher (uncached model), while keeping the original model weights frozen.</td>
                <td><a href="https://arxiv.org/pdf/2312.03209" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads</td>
                <td>Guangxuan Xiao et al</td>
                <td>2024</td>
                <td>llm, kv cache, attention</td>
                <td>Arxiv</td>
                <td>The key insight of DuoAttention is the observation that attention heads in LLMs naturally fall into two distinct categories: retrieval heads that need to access the full context to make connections across long distances, and streaming heads that mainly focus on recent tokens and attention sinks. This dichotomy makes intuitive sense because not all parts of language processing require long-range dependencies - while some aspects like fact recall or logical reasoning need broad context, others like local grammar or immediate context processing can work with nearby tokens. The paper&#x27;s approach of using optimization to identify these heads (rather than just looking at attention patterns) is clever because it directly measures the impact on model outputs, capturing the true functional role of each head rather than just its surface behavior. Finally, the insight to maintain two separate KV caches (full for retrieval heads, minimal for streaming heads) is an elegant way to preserve the model&#x27;s capabilities while reducing memory usage, since it aligns the memory allocation with each head&#x27;s actual needs.</td>
                <td><a href="https://arxiv.org/pdf/2410.10819" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Efficient Streaming Language Models with Attention Sinks</td>
                <td>Guangxuan Xiao et al</td>
                <td>2024</td>
                <td>llm, kv cache, attention</td>
                <td>ICLR</td>
                <td>This paper introduces StreamingLLM, a framework that enables large language models to process infinitely long text sequences efficiently without fine-tuning, based on a key insight about &quot;attention sinks.&quot; The authors discover that LLMs allocate surprisingly high attention scores to initial tokens regardless of their semantic relevance, which they explain is due to the softmax operation requiring attention scores to sum to one - even when a token has no strong matches in context, the model must distribute attention somewhere, and initial tokens become natural &quot;sinks&quot; since they&#x27;re visible to all subsequent tokens during autoregressive training. Building on this insight, StreamingLLM maintains just a few initial tokens (as attention sinks) along with a sliding window of recent tokens, achieving up to 22.2x speedup compared to baselines while maintaining performance on sequences up to 4 million tokens long. Additionally, they show that incorporating a dedicated learnable &quot;sink token&quot; during model pre-training can further improve streaming capabilities by providing an explicit token for collecting excess attention.</td>
                <td><a href="https://arxiv.org/pdf/2309.17453" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>MagicPIG: LSH Sampling for Efficient LLM Generation</td>
                <td>Zhuoming Chen et al</td>
                <td>2024</td>
                <td>llm, kv cache</td>
                <td>Arxiv</td>
                <td>This paper challenges the common assumption that attention in LLMs is naturally sparse, showing that TopK attention (selecting only the highest attention scores) can significantly degrade performance on tasks that require aggregating information across the full context. The authors demonstrate that sampling-based approaches to attention can be more effective than TopK selection, leading them to develop MagicPIG, a system that uses Locality Sensitive Hashing (LSH) to efficiently sample attention keys and values. A key insight is that the geometry of attention in LLMs has specific patterns - notably that the initial attention sink token remains almost static regardless of input, and that query and key vectors typically lie in opposite directions - which helps explain why simple TopK selection is suboptimal. Their solution involves a heterogeneous system design that leverages both GPU and CPU resources, with hash computations on GPU and attention computation on CPU, allowing for efficient processing of longer contexts while maintaining accuracy.</td>
                <td><a href="https://arxiv.org/pdf/2410.16179" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Guiding a Diffusion Model with a Bad Version of Itself</td>
                <td>Tero Karras et al</td>
                <td>2024</td>
                <td>diffusion, guidance</td>
                <td>Arxiv</td>
                <td>The paper makes two key contributions: First, they show that Classifier-Free Guidance (CFG) improves image quality not just through better prompt alignment, but because the unconditional model D0 learns a more spread-out distribution than the conditional model D1, causing the guidance term ∇x log(p1/p0) to push samples toward high-probability regions of the data manifold. Second, based on this insight, they introduce &quot;autoguidance&quot; - using a smaller, less-trained version of the model itself as the guiding model D0 rather than an unconditional model, which allows for quality improvements without reducing variation and works even for unconditional models.</td>
                <td><a href="https://arxiv.org/pdf/2406.02507" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>LLM-Pruner: On the Structural Pruning of Large Language Models</td>
                <td>Xinyin Ma et al</td>
                <td>2023</td>
                <td>llm, structural pruning</td>
                <td>Arxiv</td>
                <td>The authors introduce LLM-Pruner, a novel approach for compressing large language models that operates in a task-agnostic manner while requiring minimal access to the original training data. Their key insight is to first automatically identify groups of interdependent neural structures within the LLM by analyzing dependency patterns, ensuring that coupled structures are pruned together to maintain model coherence. The method then estimates the importance of these structural groups using both first-order gradients and approximated Hessian information from a small set of calibration samples, allowing them to selectively remove less critical groups while preserving the model&#x27;s core functionality. Finally, they employ a rapid recovery phase using low-rank adaptation (LoRA) to fine-tune the pruned model with a limited dataset in just a few hours, enabling efficient compression while maintaining the LLM&#x27;s general-purpose capabilities.</td>
                <td><a href="https://arxiv.org/pdf/2305.11627" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</td>
                <td>Guangxuan Xiao et al</td>
                <td>2023</td>
                <td>llm, quantization, activations</td>
                <td>ICML</td>
                <td>The key insight of SmoothQuant is that in large language models, while weights are relatively easy to quantize, activations are much harder due to outliers. They observed that these outliers persistently appear in specific channels across different tokens, suggesting that the difficulty could be redistributed. Their solution is to mathematically transform the model by scaling down problematic activation channels while scaling up the corresponding weight channels proportionally, which maintains mathematical equivalence while making both weights and activations easier to quantize. This &quot;difficulty migration&quot; approach allows them to balance the quantization challenges between weights and activations using a tunable parameter α, rather than having all the difficulty concentrated in the activation values.</td>
                <td><a href="https://arxiv.org/pdf/2211.10438" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>ESPACE: Dimensionality Reduction of Activations for Model Compression</td>
                <td>Charbel Sakr et al</td>
                <td>2024</td>
                <td>llm, dimensionality reduction, activations, compression</td>
                <td>NeurIPS</td>
                <td>Instead of decomposing weight matrices as done in previous work, ESPACE reduces the dimensionality of activation tensors by projecting them onto a pre-calibrated set of principal components using a static projection matrix P, where for an activation x, its projection is x̃ = PPᵀx. The projection matrix P is carefully constructed (using eigendecomposition of activation statistics) to preserve the most important components while reducing dimensionality, taking advantage of natural redundancies that exist in activation patterns due to properties like the Central Limit Theorem when stacking sequence/batch dimensions. During training, the weights remain uncompressed and fully trainable (maintaining model expressivity), while at inference time, the weight matrices can be pre-multiplied with the projection matrix (PTWᵀ) to achieve compression through matrix multiplication associativity: Y = WᵀX ≈ Wᵀ(PPᵀX) = (PTWᵀ)(PᵀX). This activation-centric approach is fundamentally different from previous methods because it maintains full model expressivity during training while still achieving compression at inference time, and it takes advantage of natural statistical redundancies in activation patterns rather than trying to directly compress weights.</td>
                <td><a href="https://arxiv.org/pdf/2410.05437" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model</td>
                <td>Chunting Zhou et al</td>
                <td>2024</td>
                <td>diffusion, transformer, multi-modal</td>
                <td>Arxiv</td>
                <td>The key insight of this paper is that a single transformer model can effectively handle both discrete data (like text) and continuous data (like images) by using different training objectives for each modality within the same model. They introduce &quot;Transfusion,&quot; which uses traditional language modeling (next token prediction) for text sequences while simultaneously applying diffusion modeling for image sequences, combining these distinct objectives into a unified training approach. The architecture employs a novel attention pattern that allows for causal attention across the entire sequence while enabling bidirectional attention within individual images, letting image patches attend to each other freely while maintaining proper causality for text generation. This unified approach avoids the need for separate specialized models or complex architectures while still allowing each modality to be processed according to its most effective paradigm.</td>
                <td><a href="https://arxiv.org/pdf/2408.11039" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</td>
                <td>Jiawei Zhao et al</td>
                <td>2024</td>
                <td>lora, low-rank projection</td>
                <td>ICML</td>
                <td>This paper introduces GaLore, a memory-efficient approach for training large language models that exploits the inherent low-rank structure of gradients rather than imposing low-rank constraints on the model weights themselves. The key insight is that while weight matrices may need to be full-rank for optimal performance, their gradients naturally become low-rank during training due to the specific structure of backpropagated gradients in neural networks, particularly in cases where the batch size is smaller than the matrix dimensions or when the gradients follow certain parametric forms. Building on this observation, GaLore projects gradients into low-rank spaces for memory-efficient optimization while still allowing full-parameter learning, contrasting with previous approaches like LoRA that restrict the weight updates to low-rank spaces. By periodically switching between different low-rank subspaces during training, GaLore maintains the flexibility of full-rank training while significantly reducing memory usage, particularly in storing optimizer states.</td>
                <td><a href="https://arxiv.org/pdf/2403.03507" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Neural Discrete Representation Learning</td>
                <td>Aaron van der Oord et al</td>
                <td>2017</td>
                <td>generative models, vae</td>
                <td>NeurIPS</td>
                <td>The key innovation of this paper is the introduction of the Vector Quantised-Variational AutoEncoder (VQ-VAE), which combines vector quantization with VAEs to learn discrete latent representations instead of continuous ones. Unlike previous approaches to discrete latent variables which struggled with high variance or optimization challenges, VQ-VAE uses a simple but effective nearest-neighbor lookup system in the latent space, along with a straight-through gradient estimator, to learn meaningful discrete codes. This approach allows the model to avoid the common posterior collapse problem where latents are ignored when paired with powerful decoders, while still maintaining good reconstruction quality comparable to continuous VAEs. The discrete nature of the latent space enables the model to focus on capturing important high-level features that span many dimensions in the input space (like objects in images or phonemes in speech) rather than local details, and these discrete latents can then be effectively modeled using powerful autoregressive priors for generation.</td>
                <td><a href="https://arxiv.org/pdf/1711.00937" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Improved Precision and Recall Metric for Assessing Generative Models</td>
                <td>Tuomas Kynkaanniemi et al</td>
                <td>2019</td>
                <td>generative models, precision, recall</td>
                <td>NeurIPS</td>
                <td>This paper introduces an improved metric for evaluating generative models by separately measuring precision (quality of generated samples) and recall (coverage/diversity of generated distribution) using k-nearest neighbors to construct non-parametric manifold approximations of real and generated data distributions. The authors demonstrate their metric&#x27;s effectiveness using StyleGAN and BigGAN, showing how it provides more nuanced insights than existing metrics like FID, particularly in revealing tradeoffs between image quality and variation that other metrics obscure. They use their metric to analyze and improve StyleGAN&#x27;s architecture and training configurations, identifying new variants that achieve state-of-the-art results, and perform the first principled analysis of truncation methods. Finally, they extend their metric to evaluate individual sample quality, enabling quality assessment of interpolations and providing insights into the shape of the latent space that produces realistic images.</td>
                <td><a href="https://arxiv.org/pdf/1904.06991" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Generative Pretraining from Pixels</td>
                <td>Mark Chen et al</td>
                <td>2020</td>
                <td>pretraining, gpt</td>
                <td>PMLR</td>
                <td>The paper demonstrates that transformer models can learn high-quality image representations by simply predicting pixels in a generative way, without incorporating any knowledge of the 2D structure of images. They show that as the generative models get better at predicting pixels (measured by log probability), they also learn better representations that can be used for downstream image classification tasks. The authors discover that, unlike in supervised learning where the best representations are in the final layers, their generative models learn the best representations in the middle layers - suggesting the model first builds up representations before using them to predict pixels. Finally, while their approach requires significant compute and works best at lower resolutions, it achieves competitive results with other self-supervised methods and shows that generative pre-training can be a promising direction for learning visual representations without labels.</td>
                <td><a href="https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Why Does Unsupervised Pre-Training Help Deep Learning?</td>
                <td>Dumitru Erhan et al</td>
                <td>2010</td>
                <td>pretraining, unsupervised</td>
                <td>JMLR</td>
                <td>This paper argues that standard training schemes place parameters in regions of the parameter space that generalize poorly, while greedy layer-wise unsupervised pre-training allows each layer to learn a nonlinear transformation of its input that captures the main variations in the input, which acts as a regularizer: minimizing variance and introducing bias towards good initializations for the parameters. They argue that defining particular initialization points implicitly imposes constraints on the parameters in that it specifies which minima (out of many possible minima) of the cost function are allowed. They further argue that small perturbations in the trajectory of the parameters have a larger effect early on, and hint that early examples have larger influence and may trap model parameters in particular regions of parameter space corresponding to the arbitrary ordering of training examples (similar to the &quot;critical period&quot; in developmental psychology).</td>
                <td><a href="https://jmlr.org/papers/volume11/erhan10a/erhan10a.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Improving Language Understanding by Generative Pre-Training</td>
                <td>Alec Radford et al</td>
                <td>2020</td>
                <td>pretraining</td>
                <td>Arxiv</td>
                <td>The key insight of this paper is that language models can learn deep linguistic and world knowledge through unsupervised pre-training on large corpora of contiguous text, which can then be effectively transferred to downstream tasks. The authors demonstrate this by using a Transformer architecture that can capture long-range dependencies, pre-training it on a books dataset that contains extended narratives rather than shuffled sentences, making it particularly effective at understanding context. Their innovation extends to how they handle transfer learning - rather than creating complex task-specific architectures, they show that simple input transformations can adapt their pre-trained model to various tasks while preserving its learned capabilities. This elegant approach proves remarkably effective, with their single task-agnostic model outperforming specially-designed architectures across nine different natural language understanding tasks, suggesting that their pre-training method captures fundamental aspects of language understanding.</td>
                <td><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Learning Transferable Visual Models from Natural Language Supervision</td>
                <td>Alec Radford et al</td>
                <td>2021</td>
                <td>CLIP</td>
                <td>Arxiv</td>
                <td>CLIP (Contrastive Language-Image Pre-training) works by simultaneously training two neural networks - one that encodes images and another that encodes text - to project their inputs into a shared multi-dimensional space where similar concepts end up close together. During training, CLIP takes a batch of image-text pairs and learns to identify which text descriptions actually match which images, doing this by maximizing the cosine similarity between embeddings of genuine pairs while minimizing similarity between mismatched pairs. The training data consists of hundreds of millions of (image, text) pairs collected from the internet, which helps CLIP learn broad visual concepts and their relationships to language without requiring hand-labeled data. What makes CLIP particularly powerful is its zero-shot capability - after training, it can make predictions about images it has never seen before by comparing them against any arbitrary text descriptions, rather than being limited to a fixed set of predetermined labels.</td>
                <td><a href="https://arxiv.org/pdf/2103.00020" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Adam: A Method for Stochastic Optimization</td>
                <td>Diederik Kingma et al</td>
                <td>2015</td>
                <td>optimizers</td>
                <td>ICLR</td>
                <td>Adam combines momentum (through exponential moving average of gradients mt) and adaptive learning rates (through exponential moving average of squared gradients vt) to create an efficient optimizer, where mt captures the direction of updates while vt adapts the step size for each parameter based on its gradient history. The optimizer corrects initialization bias in these moving averages by scaling them with factors 1/(1-β₁ᵗ) and 1/(1-β₂ᵗ) respectively, ensuring unbiased estimates even in early training. The parameter update θt ← θt-1 - α·mt/(√vt + ϵ) is invariant to gradient scaling because it uses the ratio mt/√vt, while the adaptive learning rate 1/√vt approximates the diagonal of the Fisher Information Matrix&#x27;s square root, making it a more conservative version of natural gradient descent that works well with sparse gradients and non-stationary objectives. The hyperparameters β₁ = 0.9 and β₂ = 0.999 mean the momentum term considers roughly the last 10 steps while the variance term considers the last 1000 steps, allowing Adam to both move quickly in consistent directions while being careful in directions with high historical variance.</td>
                <td><a href="https://arxiv.org/pdf/1412.6980" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Simplifying Neural Networks by Soft Weight-Sharing</td>
                <td>Steven Nowlan et al</td>
                <td>1992</td>
                <td>soft weight sharing, mog</td>
                <td>Neural Computation</td>
                <td>This paper tackles the challenge of penalizing complexity and preventing overfitting in neural networks. Traditional methods, like L2 regularization, penalize the sum of squared weights but can favor multiple weak connections over a single strong one, leading to suboptimal weight configurations. To address this, the authors propose a mixture of Gaussians (MoG) prior: a narrow Gaussian encourages small weights to shrink to zero, while a broad Gaussian preserves large weights essential for modeling the data accurately. By clustering weights into near-zero and larger groups, this data-driven regularization avoids forcing all weights toward zero equally and demonstrates better generalization on 12 toy tasks compared to early stopping and traditional squared-weight penalties.</td>
                <td><a href="https://www.cs.toronto.edu/~hinton/absps/sunspots.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</td>
                <td>Muyang Li et al</td>
                <td>2024</td>
                <td>diffusion, distributed inference</td>
                <td>Arxiv</td>
                <td>DistriFusion introduces *displaced patch parallelism*, where the input image is split into patches, each processed independently by different GPUs. To maintain fidelity and reduce communication costs, the method reuses activations from the previous timestep as context for the current step, ensuring interaction between patches without excessive synchronization. Synchronous communication is only used at the initial step, while subsequent steps leverage asynchronous communication, hiding communication overhead within computation. This technique allows each device to process only a portion of the workload efficiently, avoiding artifacts and achieving scalable parallelism tailored to the sequential nature of diffusion models.</td>
                <td><a href="https://arxiv.org/pdf/2402.19481" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching</td>
                <td>Xinyin Ma et al</td>
                <td>2024</td>
                <td>diffusion, caching</td>
                <td>Arxiv</td>
                <td>This paper proposes interpolation between computationally inexpensive solutions that are suboptimal and optimal solutions that are expensive by training a router the learn how to cache layers of the diffusion transformer.</td>
                <td><a href="https://arxiv.org/pdf/2406.01733" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Flash Attention</td>
                <td>Tri Dao et al</td>
                <td>2022</td>
                <td>attention, transformer</td>
                <td>Arxiv</td>
                <td>This introduces FlashAttention, which is an IO-aware exact attention algo that uses tiling. Basically, they use tiling to prevent needing to put the large NxN attention matrix on GPU HBM; FlashAttention goes through blocks of the K and V matrices, loads them to on-chip SRAM, which increases speed! Neat!</td>
                <td><a href="https://arxiv.org/pdf/2205.14135" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Token Merging for Fast Stable Diffusion</td>
                <td>Daniel Bolya et al</td>
                <td>2023</td>
                <td>diffusion, token merging</td>
                <td>Arxiv</td>
                <td>This paper seeks to apply ToMe (https://arxiv.org/pdf/2210.09461) to diffusion models, introducing techniques for token partitioning (by changing the way src and dst is merged) and a token unmerging operation (which is basically just setting the two merged tokens equal to their average, and then resetting back the two tokens with that average). Remarkably, this works very well!</td>
                <td><a href="https://arxiv.org/pdf/2303.17604" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>DeepCache: Accelerating Diffusion Models for Free</td>
                <td>Xinyin Ma et al</td>
                <td>2023</td>
                <td>diffusion, cache</td>
                <td>Arxiv</td>
                <td>Similarly to Faster Diffusion (Senma Li et al, 2024), this paper uses the temporal redundancy in the denoising stages. They then cache features across the UNet by skipping some of the skip branches / paths. Basically, for timesteps t and t+1 that are similar, we can cache some of the high level features between them and directly use them. Also smart!</td>
                <td><a href="https://arxiv.org/pdf/2312.00858" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models</td>
                <td>Senmao Li et al</td>
                <td>2024</td>
                <td>diffusion, encoder</td>
                <td>NeurIPS</td>
                <td>This paper notes that the UNet decoder in diffusion models has similar output between timesteps. Thus, they seek to basically cyclically reuse encoder features for the decoder. Smart!</td>
                <td><a href="https://arxiv.org/pdf/2312.09608" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Improved Denoising Diffusion Probabilistic Models</td>
                <td>Alex Nichol et al</td>
                <td>2021</td>
                <td>diffusion, precision, recall</td>
                <td>Arxiv</td>
                <td>This paper is the first to show that DDPMs can get competitive log-likelihoods. They use a reparameterization and a hybrid learning objective to more tightly optimize the variational lower bound, and find that their objective has less gradient noise during training. They use learned variances and find that they can get convincing samples using fewer steps. They also use the improved precision and recall metrics (Kynkaanniemi et al 2019) to show that diffusion models have higher recall for similar FID, which suggests they cover a large portion of the target distribution. They focused on optimizing log-likelihood as it is believed that optimizing ll forces the model to capture all models of data distribution (Razavi et al 2019). Heninghan et al 2020 has also shown that small improvements in ll can dramatically impact sample quality / learned feature representations. The authors argue that fixing \sigma_{t} (as Ho et al 2020 does) is reasonable in terms of sample quality, but does not explain much about the ll. Thus, to improve ll they think of finding a better choice for \Sigma_{\theta}(x_{t},t), so they choose to try to learn it. They note that it is better to parameterize the var as an interpolation between \beta_{t} and \tilde{\beta_{t}} in the log domain. Remember that \beta_{t} is the noise schedule, which is typically a small value that increases over time following some schedule. \tilde{\beta is a reparameterization of \beta_{t} used to simplify calculations. They are related via \alpha, which is 1-eta_{t}. Finally, they note that a linear schedule for noise leads to faster destroying of information than is necessary, and propose a different noise scheduler. Lots of insights!</td>
                <td><a href="https://arxiv.org/pdf/2102.09672" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Improving Training Efficiency of Diffusion Models via Multi-Stage Framework and Tailored Multi-Decoder Architecture</td>
                <td>Huijie Zhang et al</td>
                <td>2024</td>
                <td>diffusion, multi-stage</td>
                <td>CVPR</td>
                <td>This paper proposes a multi-stage framework for diffusion models that uses a shared encoder and separate decoders for different timestep intervals, along with an optimal denoiser-based timestep clustering method, to improve training and sampling efficiency while maintaining or enhancing image generation quality.</td>
                <td><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Improving_Training_Efficiency_of_Diffusion_Models_via_Multi-Stage_Framework_and_CVPR_2024_paper.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Temporal Dynamic Quantization for Diffusion Models</td>
                <td>Junhyuk So et al</td>
                <td>2023</td>
                <td>diffusion, quantization</td>
                <td>NeurIPS</td>
                <td>Temporal Dynamic Quantization (TDQ) addresses the challenge of quantizing diffusion models by dynamically adjusting quantization parameters based on the denoising time step. TDQ employs a trainable module consisting of frequency encoding, a multi-layer perceptron (MLP), and a SoftPlus activation to predict optimal quantization intervals for each time step. This module maps the temporal information to appropriate quantization parameters, allowing the method to adapt to the varying activation distributions across different stages of the diffusion process. By pre-computing these quantization intervals, TDQ avoids the runtime overhead associated with traditional dynamic quantization methods while still providing the necessary flexibility to handle the temporal dynamics of diffusion models.</td>
                <td><a href="https://arxiv.org/pdf/2306.02316v2" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Learning Efficient Convolutional Networks through Network Slimming</td>
                <td>Zhuang Liu et al</td>
                <td>2017</td>
                <td>pruning, importance</td>
                <td>CVPR</td>
                <td>This paper introduces *network slimming*, a method to reduce the size, memory footprint, and computation of CNNs by enforcing channel-level sparsity without sacrificing accuracy. It works by identifying and pruning insignificant channels during training, leveraging the γ scaling factors in Batch Normalization (BN) layers to effectively determine channel importance. The approach introduces minimal training overhead and is compatible with modern CNN architectures, eliminating the need for specialized hardware or software. Using the BN layer’s built-in scaling properties makes this pruning efficient, avoiding redundant scaling layers or issues that arise from linear transformations in convolution layers.</td>
                <td><a href="https://arxiv.org/pdf/1708.06519" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Q-Diffusion: Quantizing Diffusion Models</td>
                <td>Xiuyu Li et al</td>
                <td>2023</td>
                <td>diffusion, sampling</td>
                <td>ICCV</td>
                <td>This paper tackles the inefficiencies of diffusion models, such as slow inference and high computational cost, by proposing a post-training quantization (PTQ) method designed specifically for their multi-timestep process. The key innovation includes a *time step-aware calibration data sampling* approach, which uniformly samples inputs across multiple time steps to better reflect real inference data, addressing quantization errors and varying activation distributions without the need for additional data. Additionally, the paper introduces *shortcut-splitting quantization* to handle the bimodal activation distributions caused by the concatenation of deep and shallow feature channels in shortcuts, quantizing them separately before concatenation for improved accuracy with minimal extra resources.</td>
                <td><a href="https://arxiv.org/pdf/2302.04304" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Mixture of Efficient Diffusion Experts Through Automatic Interval and Sub-Network Selection</td>
                <td>Alireza Ganjdanesh et al</td>
                <td>2024</td>
                <td>diffusion, sampling</td>
                <td>Arxiv</td>
                <td>This paper reduces the cost of sampling via pruning a pretrained diffusion model into a mixture of experts (MoE) for their respective time intervals, via a routing agent that predicts the architecture needed to generate the experts.</td>
                <td><a href="https://arxiv.org/pdf/2409.15557" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training</td>
                <td>Kai Wang et al</td>
                <td>2024</td>
                <td>diffusion, sampling</td>
                <td>Arxiv</td>
                <td>This paper introduces SpeeD, a novel approach for accelerating the training of diffusion models without compromising performance. The authors analyze the diffusion process and identify three distinct areas: acceleration, deceleration, and convergence, each with different characteristics and importance for model training. Based on these insights, SpeeD implements two key components: asymmetric sampling, which reduces the sampling of less informative time steps in the convergence area, and change-aware weighting, which gives more importance to the rapidly changing areas between acceleration and deceleration. The authors&#x27; key insight is that not all time steps in the diffusion process are equally valuable for training, with the convergence area providing limited benefits despite occupying a large proportion of time steps, while the rapidly changing area between acceleration and deceleration is crucial but often undersampled. To address this, SpeeD introduces an asymmetric sampling strategy using a two-step probability function: $P(t) = \begin{cases} \frac{k}{T + \tau(k-1)}, &amp; 0 &lt; t \leq \tau \ \frac{1}{T + \tau(k-1)}, &amp; \tau &lt; t \leq T \end{cases}$, where τ is a carefully selected threshold marking the beginning of the convergence area, k is a suppression intensity factor, T is the total number of time steps, and t is the current time step. This function increases sampling probability before τ and suppresses it after. Additionally, SpeeD employs a change-aware weighting scheme based on the gradient of the process increment&#x27;s variance, assigning higher weights to time steps with faster changes. By combining these strategies, SpeeD aims to focus computational resources on the most informative parts of the diffusion process, potentially leading to significant speedups in training time without sacrificing model quality.</td>
                <td><a href="https://arxiv.org/pdf/2405.17403" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>HyperGAN: A Generative Model for Diverse, Performant Neural Networks</td>
                <td>Neale Ratzlaff et al</td>
                <td>2019</td>
                <td>gan, ensemble</td>
                <td>ICML</td>
                <td>This paper introduces HyperGAN, a novel generative model designed to learn a distribution of neural network parameters, addressing the issue of overconfidence in standard neural networks when faced with out-of-distribution data. Unlike traditional approaches, HyperGAN doesn&#x27;t require restrictive prior assumptions and can rapidly generate large, diverse ensembles of neural networks. The model employs a unique &quot;mixer&quot; component that projects prior samples into a correlated latent space, from which layer-specific generators create weights for a deep neural network. Experimental results show that HyperGAN can achieve competitive performance on datasets like MNIST and CIFAR-10 while providing improved uncertainty estimates for out-of-distribution and adversarial data compared to standard ensembles. NOTE: There has actually been a diffusion variant of this idea: https://arxiv.org/pdf/2402.13144</td>
                <td><a href="https://arxiv.org/pdf/2405.17403" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Diffusion Models Already Have a Semantic Latent Space</td>
                <td>Mingi Kwon et al</td>
                <td>2023</td>
                <td>diffusion, latent space</td>
                <td>ICLR</td>
                <td>This paper introduces Asymmetric Reverse Process (Asyrp), a method that discovers a semantic latent space (h-space) in pretrained diffusion models, enabling controlled image manipulation with desirable properties such as homogeneity, linearity, and consistency across timesteps, while also proposing a principled design for versatile editing and quality enhancement in the generative process. The authors propose Asymmetric Reverse Process (Asyrp). It modifies only the P_{t} term while preserving the D_{t} term in the reverse process. This makes sense because it a) breaks the destructive interference seen in previous methods, b) allows for controlled modification of the generation process towards target attributes, and c) maintains the overall structure and quality of the diffusion process.</td>
                <td><a href="https://arxiv.org/pdf/2210.10960" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale</td>
                <td>Fan Bao et al</td>
                <td>2023</td>
                <td>diffusion, multi-model</td>
                <td>ICML</td>
                <td>The authors present a method of sampling from joint and conditional distributions using a small modification on diffusion models. UniDiffuser’s proposed method involves handling multiple modalities (such as images and text) within a single diffusion model. Here is in general what they do: 1. Perturb data in all modalities: For a given data point (x0,y0), where x0 is an image and y0 is text, UniDiffuser adds noise to both simultaneously. The noisy versions are represented as xt_{x} and yt_{y}, where t_{x} and t_{y} are the respective timesteps. 2. Use of individual timesteps for different modalities: Instead of using a single timestep t for both modalities, UniDiffuser uses separate timesteps t_{x} and t_{y}. This allows for more flexibility in handling the different characteristics of each modality. 3. Predicting noise for all modalities simultaneously: UniDiffuser uses a joint noise prediction network \epsilon_{\theta}(xt_{x},yt_{y},t_{x},t_{y}) that takes in the noisy versions of both modalities and their respective timesteps. The network then outputs predicted noise for both modalities in one forward pass.</td>
                <td><a href="https://arxiv.org/pdf/2303.06555" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Diffusion Models as a Representation Learner</td>
                <td>Xingyi Yang et al</td>
                <td>2023</td>
                <td>diffusion, representation learner</td>
                <td>ICCV</td>
                <td>This paper (smartly!) notices that one of the major reasons for long training and poor results of diffusion models is the lack of fast learning of relationships. For instance, they remark on the learning of one eye of a dog before both eyes. They propose to mask the input image in the latent space and learn how to predict the masks, and then diffuse these masks. Brilliant!</td>
                <td><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Masked_Diffusion_Transformer_is_a_Strong_Image_Synthesizer_ICCV_2023_paper.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Masked Diffusion Transformer is a Strong Image Synthesizer</td>
                <td>Shanghua Gao et al</td>
                <td>2023</td>
                <td>diffusion, masking, transformer</td>
                <td>ICCV</td>
                <td>This paper (smartly!) notices that one of the major reasons for long training and poor results of diffusion models is the lack of fast learning of relationships. For instance, they remark on the learning of one eye of a dog before both eyes. They propose to mask the input image in the latent space and learn how to predict the masks, and then diffuse these masks. Brilliant!</td>
                <td><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Masked_Diffusion_Transformer_is_a_Strong_Image_Synthesizer_ICCV_2023_paper.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Generative Modeling by Estimating Gradients of the Data Distribution</td>
                <td>Yang Song et al</td>
                <td>2019</td>
                <td>diffusion, score matching</td>
                <td>NeurIPS</td>
                <td>This paper introduces Noise Conditional Score Networks (NCSNs), a novel approach to generative modeling that learns to estimate the score function of a data distribution at multiple noise levels. NCSNs are trained using score matching, avoiding the need to compute normalizing constants, and generate samples using annealed Langevin dynamics. The method addresses challenges in modeling complex, high-dimensional data distributions, particularly for data lying on or near low-dimensional manifolds.</td>
                <td><a href="https://arxiv.org/pdf/1907.05600" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>LAPTOP-Diff: Layer Pruning and Normalized Distillation for Compression Diffusion Models</td>
                <td>Dingkun Zhang et al</td>
                <td>2024</td>
                <td>diffusion, pruning</td>
                <td>Arxiv</td>
                <td>This paper proposes layer pruning and normalized distillation for pruning diffusion models. They use a surrogate function and show that their surrogate implies a property called &quot;additivity&quot;, where the output distortion caused by many perturbations approximately equals the sum of the output distortion caused by each single perturbation. They then show that their computation can be formed as a 0-1 Knapsack problem. They then analyze what is the important objective for retraining, and see that there is an imbalance in previous feature distillation approaches employed in the retraining phase. They note that the L2-Norms of feature maps at the end of different stages and the values of different feature loss terms vary significantly, for instance, the highest loss term is ~10k times greater than the lowest one throughout the distillation process, and produces about 1k times larger gradients. This dilutes the gradients of the numerically insignificant feature loss terms. So, they opt to normalize the feature loss.</td>
                <td><a href="https://arxiv.org/pdf/2404.11098" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Classifier-Free Diffusion Guidance</td>
                <td>Jonathan Ho et al</td>
                <td>2022</td>
                <td>diffusion, guidance</td>
                <td>NeurIPS</td>
                <td>This paper introduces classifier-free guidance, a novel technique for improving sample quality in conditional diffusion models without using a separate classifier. Unlike traditional classifier guidance, which relies on gradients from an additional classifier model, classifier-free guidance achieves similar results by combining score estimates from jointly trained conditional and unconditional diffusion models. The method involves training a single neural network that can produce both conditional and unconditional score estimates, and then using a weighted combination of these estimates during the sampling process. This approach simplifies the training pipeline, avoids potential issues associated with training classifiers on noisy data, and eliminates the need for adversarial attacks on classifiers during sampling. The authors demonstrate that classifier-free guidance can achieve a similar trade-off between Fréchet Inception Distance (FID) and Inception Score (IS) as classifier guidance, effectively boosting sample quality while reducing diversity. The key difference is that classifier-free guidance operates purely within the generative model framework, without relying on external classifier gradients. This method provides an intuitive explanation for how guidance works: it increases conditional likelihood while decreasing unconditional likelihood, pushing generated samples towards more characteristic features of the desired condition.</td>
                <td><a href="https://arxiv.org/pdf/2207.12598" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>LD-Pruner: Efficient Pruning of Latent Diffusion Models using Task-Agnostic Insights</td>
                <td>Thibault Castells et al</td>
                <td>2024</td>
                <td>pruning, diffusion, ldm</td>
                <td>CVPR</td>
                <td>This paper presents LD-Pruner. The main interesting part is how the frame the pruning problem. Basically, they define an &quot;operator&quot; (any fundamental building block of a net, like convolutional layers, activation functions, transformer blocks), and try to either 1) remove it or 2) replace it with a less demanding operation. As they operate on the latent space, this work can be applied to any generation that uses diffusion (task agnostic). It is interesting to note their limitations: the approach does not extend to pruning the decoder, and their approach does not consider dependencies between operators (which is a big deal I think). Finally, their score function seems a bit arbitrary (maybe this could be learned?).</td>
                <td><a href="https://openaccess.thecvf.com/content/CVPR2024W/EDGE/papers/Castells_LD-Pruner_Efficient_Pruning_of_Latent_Diffusion_Models_using_Task-Agnostic_Insights_CVPRW_2024_paper.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>RoFormer: Enhanced Transformer with Rotary Position Embedding</td>
                <td>Jianlin Su et al</td>
                <td>2021</td>
                <td>attention, positional embedding</td>
                <td>Arxiv</td>
                <td>This paper introduces Rotary Position Embedding (RoPE), a method for integrating positional information into transformer models by using a rotation matrix to encode absolute positions and incorporating relative position dependencies.</td>
                <td><a href="https://arxiv.org/pdf/2104.09864" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</td>
                <td>Alex Nichol et al</td>
                <td>2022</td>
                <td>text-conditioned diffusion, inpainting</td>
                <td>Arxiv</td>
                <td>This paper explores text-conditional image synthesis using diffusion models, comparing CLIP guidance and classifier-free guidance, and finds that classifier-free guidance produces more photorealistic and caption-aligned images.</td>
                <td><a href="https://arxiv.org/pdf/2112.10741" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>LLM Inference Unveiled: Survey and Roofline Model Insights</td>
                <td>Roger Waleffe et al</td>
                <td>2024</td>
                <td>llms, survey</td>
                <td>Arxiv</td>
                <td>This paper surveys some recent advancements in LLC inference, like speculative decoding or operator fusion. They also analyze the findings using the Roofline model, which is likely the first paper to do such a thing for LLM inference. Good for checking out other papers that have recently been published.</td>
                <td><a href="https://arxiv.org/pdf/2402.16363" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>An Empirical Study of Mamba-based Language Models</td>
                <td>Roger Waleffe et al</td>
                <td>2024</td>
                <td>mamba, llms, transformer</td>
                <td>Arxiv</td>
                <td>This paper compares Mamba-based, Transformer-based, and hybrid-based language models in a controlled setting where sizes and datasets are larger than the past (8B-params / 3.5T tokens). They find that Mamba and Mamba-2 lag behind Transformer models on copying and in-context learning tasks. They then see that a hybrid architecture of 43% Mamba, 7% self attention, and 50% MLP layers performs better than all others.</td>
                <td><a href="https://arxiv.org/pdf/2406.07887" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Diffusion Models Beat GANs on Image Synthesis</td>
                <td>Prafulla Dhariwal et al</td>
                <td>2021</td>
                <td>diffusion, gan</td>
                <td>Arxiv</td>
                <td>This work demonstrates that diffusion models surpass the current state-of-the-art generative models in image quality, achieved through architecture improvements and classifier guidance, which balances diversity and fidelity. The model attains FID scores of 2.97 on ImageNet 128×128 and 4.59 on ImageNet 256×256, matching BigGAN-deep with as few as 25 forward passes while maintaining better distribution coverage. Additionally, combining classifier guidance with upsampling diffusion models further enhances FID scores to 3.94 on ImageNet 256×256 and 3.85 on ImageNet 512×512.</td>
                <td><a href="https://arxiv.org/pdf/2105.05233" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Progressive Distillation for Fast Sampling of Diffusion Models</td>
                <td>Tim Salimans et al</td>
                <td>2022</td>
                <td>diffusion, distillation, sampling</td>
                <td>ICLR</td>
                <td>Diffusion models excel in generative modeling, surpassing GANs in perceptual quality and autoregressive models in density estimation, but they suffer from slow sampling times. This paper introduces two key contributions: new parameterizations that improve stability with fewer sampling steps and a distillation method that progressively reduces the number of required steps by half each time. Applied to benchmarks like CIFAR-10 and ImageNet, the approach distills models from 8192 steps down to as few as 4 steps, maintaining high image quality while offering a more efficient solution for both training and inference.</td>
                <td><a href="https://arxiv.org/pdf/2202.00512" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>On Distillation of Guided Diffusion Models</td>
                <td>Chenlin Meng et al</td>
                <td>2023</td>
                <td>diffusion, classifier-free guidance</td>
                <td>Arxiv</td>
                <td>Classifier-free guided diffusion models are effective for high-resolution image generation but are computationally expensive during inference due to the need to evaluate both conditional and unconditional models many times. This paper proposes a method to distill these models into faster ones by learning a single model that approximates the combined outputs, then progressively reducing the number of sampling steps. The approach significantly accelerates inference, generating images with comparable quality to the original model using as few as 1-4 denoising steps, achieving up to 256× speedup on datasets like ImageNet and LAION.</td>
                <td><a href="https://arxiv.org/pdf/2210.03142" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Diffusion Probabilistic Models Made Slim</td>
                <td>Xingyi Yang et al</td>
                <td>2022</td>
                <td>diffusion, dpms, spectral diffusion</td>
                <td>Arxiv</td>
                <td>Diffusion Probabilistic Models (DPMs) produce impressive visual results but suffer from high computational costs, limiting their use on resource-limited platforms. This paper introduces Spectral Diffusion (SD), a lightweight model designed to address DPMs&#x27; bias against high-frequency generation, which smaller networks struggle to capture. SD incorporates wavelet gating for frequency dynamics and spectrum-aware distillation to enhance high-frequency recovery, achieving 8-18× computational efficiency while maintaining competitive image fidelity.</td>
                <td><a href="https://arxiv.org/pdf/2211.17106" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Structural Pruning for Diffusion Models</td>
                <td>Gongfan Fang et al</td>
                <td>2023</td>
                <td>diffusion, pruning</td>
                <td>NeurIPS</td>
                <td>Generative modeling has advanced significantly with Diffusion Probabilistic Models (DPMs), but these models often require substantial computational resources. To address this, Diff-Pruning is introduced as a compression method that reduces the computational load by pruning unnecessary diffusion steps, using a Taylor expansion to identify key weights without extensive re-training. Empirical results show that Diff-Pruning can cut FLOPs by around 50%, while maintaining consistent generative performance at only 10-20% of the original training cost.</td>
                <td><a href="https://arxiv.org/pdf/2305.10924" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Diffusion Models: A Comprehensive Survey of Methods and Applications</td>
                <td>Ling Yang et al</td>
                <td>2024</td>
                <td>diffusion, survey</td>
                <td>ACM</td>
                <td>Diffusion models are a powerful class of deep generative models known for their success in tasks like image synthesis, video generation, and molecule design. This survey categorizes diffusion model research into efficient sampling, improved likelihood estimation, and handling specialized data structures, while also discussing the potential for combining them with other generative models. The review highlights their broad applications across fields such as computer vision, NLP, temporal data modeling, and interdisciplinary sciences, suggesting areas for further exploration.</td>
                <td><a href="https://arxiv.org/pdf/2209.00796" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</td>
                <td>Martin Heusel et al</td>
                <td>2017</td>
                <td>gan, equilibrium, fid, is</td>
                <td>NeurIPS</td>
                <td>This paper introduces a two time-scale update rule (TTUR) for GANs, and proves that this makes GANs converge to a local Nash equilibrium. More cited is the FID score introduced here. FID improves on IS by comparing the distributions of real and generated images directly. This is done by using the Inception model to extract features from images and then assuming these features follow a multidimensional Gaussian distribution. FID measures the difference between the Gaussians (representing the real and generated images) using the Frechet distance, which effectively captures differences in the mean and covariance (the first two moments) of the distributions. FID makes sense as it directly compares the distributions of real and generated images by using the extracted features from Inception. These features are assumed to follow some multidimensional Gaussian, which simplifies the comparison. The Guassian is chosen as it is the maximum entropy distribution for a given mean and covariance (proof: https://medium.com/mathematical-musings/how-gaussian-distribution-maximizes-entropy-the-proof-7f7dcb2caf4d) -- maximum entropy is important, because this means that the Gaussian makes the fewest additional assumptions about the data, making sure the model is as non-committal as possible given the available information. Then, we calculate the statistics between the real and generated image features, like their mean and covariances. Finally, we compute the FID score using Frechet AKA Wasserstein-2 distance.</td>
                <td><a href="https://arxiv.org/pdf/2212.09748" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Scalable Diffusion Models with Transformers</td>
                <td>William Peebles et al</td>
                <td>2023</td>
                <td>diffusion,ddpm, dit</td>
                <td>CVPR</td>
                <td>The authors explore using transformers in the latent space, rather than U-Nets. They find that their methods can lead to lower FID scores compared to prior SOTA. In this paper, their image generation pipeline is roughly: 1) Input high resolution image x 2) Encoder z = E(x), where E is a pre-trained frozen VAE encoder, and z is the latent representation 3) The DiT model operates on z 4) New latent representation z’ is sampled from the diffusion model 5) We then decode the z’ using the pre-trained frozen VAE decoder D, and x’ is now the generated high resolution image.</td>
                <td><a href="https://arxiv.org/pdf/2212.09748" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Max-Affine Spline Insights Into Deep Network Pruning</td>
                <td>Haoran You et al</td>
                <td>2022</td>
                <td>early-bird, lottery-hypothesis, pruning, low-precision</td>
                <td>TMLR</td>
                <td>The authors make connections from spline-theory (AKA, consdering DNNs as a continuous piecewise affline mapping) and pruning. Basically, they say that pruning removes redundant decision boundaries in layers that are pruned, and that we can compare the decision boundaries of unpruned networks to their pruned counterparts to show this (they have some nice visualizations). They also note that the final decision boundary often does not always depend on existing subdivision lines. Finally, they demonstrate another way of finding EB tickets using this spline formulation.</td>
                <td><a href="https://arxiv.org/pdf/2101.02338" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Drawing Early-Bird Tickets: Towards More Efficient Training of Deep Networks</td>
                <td>Haoran You et al</td>
                <td>2020</td>
                <td>early-bird, lottery-hypothesis, pruning, low-precision</td>
                <td>ICLR</td>
                <td>The authors show that there exist early-bird (EB) tickets: small, but critical subnetworks for dense randomly intialized networks, that can be found using low-cost training schemes (low precision, early stopping). They also design a practical low compute method for finding these. They use mask distance. Basically, for each pruning iteration, a binary mask is created. This mask represents which parts of the network are kept (the &quot;ticket&quot;, or pruned subnet) and which parts are removed. They then consider the scaling factor &quot;r&quot; in BN layers as indicators of significance. This r is learned during training and is used to scale normalized activations. The magnitude of r is an indicator of how important the channel is to the network&#x27;s performance. After deciding which channels to prune based on r, the binary mask is created. If the channel is kept (not pruned), marked as 1 in the mask. Else, 0. For any two subnets, they then compute the &quot;mask distance&quot; (AKA Hamming distance) between the two ticketmasks. They measure the mask distance between consequtive epochs and draw EB tickets when such distance is smaller than some threshold.</td>
                <td><a href="https://arxiv.org/pdf/1909.11957" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Learning both Weights and Connections for Efficient Neural Networks</td>
                <td>Song Han et al</td>
                <td>2015</td>
                <td>pruning, compression, regularization</td>
                <td>NeurIPS</td>
                <td>The authors show a method of pruning neural networks in three steps: 1) train the network to learn what connections are important, 2) prune unimportant connections, 3) retrain and fine-tune. In order to train for learning what connections are important, they do not focus on learning the final weight values, but rather just focus on the importance of connections. They don&#x27;t explicitly mention how this is done, but one could look at the Hessian of the loss or the magnitude of the weights. I&#x27;d imagine you could do this within only a few training iterations. In their &quot;Regularization&quot; section, it is interesting to note that L1 regularization (penalizes non-zero params resulting in more params near zero) gave better accuracy after pruning, but before retraining. But, these remaining connections are not as good as with using L2. The authors also present a discussion of what dropout rate to use.</td>
                <td><a href="https://arxiv.org/pdf/1506.02626" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference</td>
                <td>Jiaming Tang et al</td>
                <td>2024</td>
                <td>KV cache, sparsity, LLM</td>
                <td>ICML</td>
                <td>Long context LLM inference is slow and the speed decreases significantly as sequence lengths grow. This is mainly due to needing to load a big KV cache during self-attention. Prior works have use methods to evict tokens in the attention maps to promote sparsity, but the Han lab (smartly!) found that the criticality of tokens strongly correlates with the current query token. Thus, they employ a KV Cache eviction method that retains all KV cache (since past evicted tokens may be needed to handle future queries), while being able to select the top K relevant tokens to a particular query. This allows for speedups in self-attention at low cost to accuracy.</td>
                <td><a href="https://arxiv.org/pdf/2406.10774" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models</td>
                <td>Jiahui Yu et al</td>
                <td>2020</td>
                <td>NAS, one-shot</td>
                <td>Arxiv</td>
                <td>Most NAS frameworks train some one-shot model to rank the quality of different child architectures. However, these rankings often are different than reality, so frameworks typically finetune architecture after finding them. BigNAS proposes that this fine-tuning / post-processing is not necessary. They find some interesting points, such as that &quot;big models converge faster while small child models converge slower&quot;. Thus, at some training step t when the performance of a big model peaks, the small child models are not yet fully-trained, and at a t&#x27; where the child models are fully trained, the big model is overfitting. Thus, they use an exponentially decaying with constant ending learning rate scheduler, which has constant learning rate at the end of training when it reaches 5% of initial learning rate. Another point they bring up is a &quot;coarse-to-fine&quot; strategy where one first finds a rough sketch of promising network candidates, and then samples multiple finer grained variations around the sketch of interest.</td>
                <td><a href="https://arxiv.org/pdf/2003.11142" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Meta-Learning of Neural Architectures for Few-Shot Learning</td>
                <td>Thomas Elsken et al</td>
                <td>2021</td>
                <td>NAS, meta-learning, few-shot, fsl</td>
                <td>Arxiv</td>
                <td>The authors propose MetaNAS, which is the first method that fully integrates NAS with gradient-based meta-learning. Basically, they learn a method of joint learning gradient-based NAS methods like DARTS and meta-learning the architecture itself. Their goal is thus: meta-learn an architecture \alpha_{meta} with corresponding meta-learned weights w_{meta}. When given a new task \mathcal{T}_{i}, both \alpha_{meta} and w_{meta} adapt quickly to \mathcal{T}_{i} based on a few samples. One interesting technique they do is add a temperature term that is annealed to 0 over the course of task training; this is to help with sparsity of the mixture weights of the operations when using the DARTS search.</td>
                <td><a href="https://arxiv.org/pdf/1911.11090" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>MetAdapt: Meta-Learned Task-Adaptive Architecture for Few-Shot Classification</td>
                <td>Sivan Doveh et al</td>
                <td>2020</td>
                <td>NAS, meta-learning, few-shot, fsl</td>
                <td>Arxiv</td>
                <td>The authors propose a method using a DARTS-like search for FSL architectures. &quot;Our goal is to learn a neural network where connections are controllable and adapt to the few-shot task with novel categories... However, unlike DARTS, our goal is not to learn a one time architecture to be used for all tasks... we need to make our architecture task adaptive so it would be able to quickly rewire for each new target task.&quot;. Basically, they design a thing called a MetAdapt Controller that changes the connection in the main network according to some given task.</td>
                <td><a href="https://arxiv.org/pdf/1912.00412" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Distilling the Knowledge in a Neural Network</td>
                <td>Geoffry Hinton et al</td>
                <td>2015</td>
                <td>distillation, ensemble, MoE</td>
                <td>Arxiv</td>
                <td>The first proposal of knowledge distillation. The main interesting point I found was that they change the temperature of the softmax to be higher to allow for softer targets. This allows for understanding what 2&#x27;s look like 3&#x27;s (in an MNIST example). Basically, adds a sort of regularization since more information can be carried in these softer targets compared to a single 0 or 1. They also propose the idea of having an ensemble of models, and then learning a distilled model that is smaller. The biological example of having a clumsy larvae that then becomes a more specialized bug was good.</td>
                <td><a href="https://arxiv.org/pdf/1503.02531" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>HyperTuning: Toward Adapting Large Language Models without Back-propagation</td>
                <td>Jason Phang et al</td>
                <td>2023</td>
                <td>hypernetworks, adaptation, tuning, LoRA, LLMs</td>
                <td>ICML</td>
                <td>The authors show that we can a hypernetwork for model adaptation in order to generate task-specific parameters. They try two approaches: generating prefixs and generating LoRA parameters for a frozen T5 model using few-shot examples. They also note the importance of hyperpretraining, i.e., an additional stage to adapt the hypernet to generate parameters for the downstream model. They also propose a scheme for this. NOTE! &quot;We also observe a consistent trend where HyperT5-Prefix outperforms HyperT5-LoRA. We speculate that it is easier for hypermodels to learn to generate soft prefixes as compared to LoRA weights...&quot;</td>
                <td><a href="https://arxiv.org/pdf/2211.12485" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</td>
                <td>Armen Aghajanyan et al</td>
                <td>2020</td>
                <td>fine-tuning, intrinsic dimension, lora</td>
                <td>Arxiv</td>
                <td>Large models with billions of parameters can be fine-tuned using only a few hundred examples. Why is this? Furthermore, large models often allow for significant sparsification, which implies that there is much redundancy. This paper targets both of these ideas, by showing that many common models have an &quot;intrinsic dimension&quot; much less than the full parameterization.</td>
                <td><a href="https://arxiv.org/pdf/2012.13255" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>LoRA: Low-Rank Adaptation of Large Language Models</td>
                <td>Edward Hu et al</td>
                <td>2021</td>
                <td>low rank adaptation, lora, llm, fine-tuning</td>
                <td>Arxiv</td>
                <td>Fine-tuning large models is expensive, because we update all the original parameters. LoRA, taking inspiration from Aghajanyan et al, 2020 (pre-trained language models have a low &quot;intrinsic dimension&quot;), the authors thought that the weight updates would also have low intrinsic rank. Thus, they decompose Delta W = BA, where B and A are lower rank. The A and B are trainable. They initialize A with Gaussian, and B as zero, so Delta W = BA is zero initialy. They then optimize and find this method to be more efficient in terms of both time and space.</td>
                <td><a href="https://arxiv.org/pdf/2106.09685" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Learning to Compress Prompts with Gist Tokens</td>
                <td>Jesse Mu et al</td>
                <td>2023</td>
                <td>llms, prompting, compression, tokens</td>
                <td>NeurIPS</td>
                <td>The authors describe a method of using a distilling function G (similar to a hypernet) that is able to compress LM prompts into a smaller set of &quot;gist&quot; tokens. These tokens can then be cached and reused. The neat trick is that they reuse the LM itself as G, so gisting itself incurs no additional training cost. Note that in their &quot;Failure Cases&quot; section, they mention &quot;... While it is unclear why only the gist models exhibit this behavior (i.e. the fail example behavior), these issues can likely be mitigated with more careful sampling techniques.</td>
                <td><a href="https://arxiv.org/pdf/2304.08467" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Once-For-All: Train One Network and Specialize it For Efficient Deployment</td>
                <td>Han Cai et al</td>
                <td>2020</td>
                <td>nas, supernets</td>
                <td>ICLR</td>
                <td>The authors proposed training one large supernetwork and then sampling subnetworks as an approach for NAS. This method allows for the simultaneous generation of many different subnetworks that could satisfy different constraints (i.e. hardware, latency, accuracy, etc). The authors also propose a progressive shrinking method to train the net (start by training the big supernet, then progressively shrink down), which can be seen as a generalized pruning method. Furthermore, they introduce an idea of training a twin neural network to help estimate latency / accuracy given some architecture, which allows for fast feedback when conducting the search for subnetworks.</td>
                <td><a href="https://arxiv.org/abs/1908.09791" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Dataless Knowledge Fusion by Merging Weights</td>
                <td>Xisen Jin et al</td>
                <td>2023</td>
                <td>knowledge fusion, weight merging</td>
                <td>ICLR</td>
                <td>The paper introduces RegMean, a method for merging pre-trained language models from different datasets by solving a linear optimization problem, which improves generalization across domains without requiring the original training data. Compared to existing methods like Simple Averaging and Fisher Averaging, RegMean offers higher computational efficiency and comparable memory overhead, while achieving better or equivalent performance across various natural language tasks, including out-of-domain generalization. The method is evaluated using GLUE datasets and demonstrates superior performance in most tasks, outperforming traditional model ensembling and multi-task learning approaches.</td>
                <td><a href="https://arxiv.org/pdf/2212.09849" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Superposition of Many Models into One</td>
                <td>Cheung et al</td>
                <td>2019</td>
                <td>superposition, online learning, tasks, continual learning</td>
                <td>NeurIPS</td>
                <td>A method of storing multiple models using only one set of parameters via parameter superposition is provided; it shares similarities to superposition in the fourier analysis for signal processing.</td>
                <td><a href="https://arxiv.org/pdf/1902.05522" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation</td>
                <td>Yoshua Bengio et al</td>
                <td>2013</td>
                <td>gradients, stochasticy, backpropagation</td>
                <td>Arxiv</td>
                <td>The authors introduce a several methods of estimation / propagation for networks that have stochastic neurons. This is used often in networks that are quantization-aware, as they sometimes have decision-boundaries in the neurons that are not differentiable regularly. The paper also introduces the &quot;Straight Through Estimator&quot;, which was actually first introduced in one of Hinton&#x27;s lectures. One interesting idea they present (that I think may have also been introduced in Kingma&#x27;s VAE paper?) is that we can model the output h_{i} of some stochastic neuron as the application of a deterministic function that also depends on some noise source z_{i}: h_{i} = f(a_{i},z_{i}). TLDR: Straight through units are typically the go-to due to ease of use and good performance.</td>
                <td><a href="https://arxiv.org/pdf/1308.3432" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>DoReFaNet: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients</td>
                <td>Shuchang Zhou et al</td>
                <td>2018</td>
                <td>quantization, cnn, gradients</td>
                <td>Arxiv</td>
                <td>The authors introduce a method to train CNNs with low bitwidth weights and activations using low bitwidth param gradients. They use deterministic quantization for weights and activations, while stochastically quantizing gradients. Note that they do not quantize the weights of the first CNN layer for the most part, as they noted that it would often degrade performance (Han et al. 2015 also notes a similar thing). Another interesting thing they do is add noise to the gradient after quantization to increase performance. This paper also uses the straight through estimator (Bengio et al 2013) for propagating gradients when using their quantization scheme.</td>
                <td><a href="https://arxiv.org/pdf/1606.06160" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Training Deep Neural Networks with 8-bit Floating Point Numbers</td>
                <td>Naigang Wang et al</td>
                <td>2018</td>
                <td>quantization, floating-point, precision</td>
                <td>NeurIPS</td>
                <td>The authors show that it is possible to train DNNs with 8-bit fp values while maintaining decent accuracy. To do this, they make a new FP8 format, develop a technique &quot;chunk-based computations&quot; that allow matrix and convolution ops to be computed using 8-bit multiplications and 16 bit additions, and use fp stochastic rounding in weight updates. One interesting point they make is that swamping (the issue of truncation in large-to-small number addition) is a serious problem in DNN bit-precision reduction.</td>
                <td><a href="https://arxiv.org/pdf/1812.08011" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</td>
                <td>Benoit Jacob et al</td>
                <td>2017</td>
                <td>quantization, quantization schemes, efficient inference, floating-point</td>
                <td>Arxiv</td>
                <td>The authors propose a quantization scheme that allows us to only use integer arithmetic to approximate fp computations in a neural network. They also describe a training approach that simulates the effect of quantization in the forward pass. Backprop still occurs, but all weights and biases are stored in fp. The forward prop pass then simulates quantized inference by rounding off using the quantization scheme they describe that changes fp to int.</td>
                <td><a href="https://arxiv.org/pdf/1712.05877" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>PACT: Parameterized Clipping Activation for Quantized Neural Networks</td>
                <td>Jungwook Choi et al</td>
                <td>2018</td>
                <td>quantization, clipping, activations</td>
                <td>ICLR</td>
                <td>The authors present a method of quantization by clipping activations using a learnable parameter, alpha. They show that this can lead to lower decreases in accuracy compared to other quantization methods. They also note that activations have been hard to quantize compared to weights in the past. They also prove that PACT is as expressive as ReLU, by showing it can reach the same solution as ReLU if SGD is used. They also describe the hardware benefits that can be incurred.</td>
                <td><a href="https://arxiv.org/pdf/1805.06085" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>SMASH: One-Shot Model Architecture Search through Hypernetworks</td>
                <td>Andrew Brock et al</td>
                <td>2017</td>
                <td>hypernetworks, nas, one-shot, few-shot</td>
                <td>Arxiv</td>
                <td>The authors propose a technique to speed up NAS by using a hypernet. Basically, they train a hypernet to generate weights of a main model that has variable architecure. The input to the hypernet is a binarized representation of model architecture. The hypernet takes this representation in, and then outputs weights. They then train only for a few epochs, and compare the validation scores obtained across different representations. Then, they fully train the model that had the best validation score.</td>
                <td><a href="https://arxiv.org/abs/1708.05344" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Example-based Hypernetworks for Multi-source Adaptation to Unseen Domains</td>
                <td>Tomer Volk et al</td>
                <td>2023</td>
                <td>hypernetworks, multi-source adaptation, unseen domains, NLP</td>
                <td>EMNLP</td>
                <td>The authors apply hypernets to unsupervised domain adaptation in NLP. They use example-based adaptation. The main idea is that they use an encoder-decoder to initially create the unique signatures from an input example, and then they embed it within the source domain&#x27;s semantic space. The signature is then used by a hypernet to generate the task classifier&#x27;s weights. The paper focuses on improving generalization to unseen domains by explicitly modeling the shared and domain specific characteristics of the input. To allow for parameter sharing, they propose modeling based on hypernets, which allow soft weight sharing. </td>
                <td><a href="https://aclanthology.org/2023.findings-emnlp.610.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Meta-Learning via Hypernetworks</td>
                <td>Dominic Zhao et al</td>
                <td>2020</td>
                <td>hypernetworks, meta-learning</td>
                <td>NeurIPS</td>
                <td>The authors propose a soft weight-sharing hypernet architecture that performs well on meta-learning tasks. A good paper to show efforts in meta-learning with regards to hypernets, and comparing them to SOTA methods like Model-Agnostic Meta-Learning (MAML).</td>
                <td><a href="https://neurips.cc/virtual/2020/20189" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>HyperDynamics: Meta-Learning Object and Agent Dynamics with Hypernetworks</td>
                <td>Zhou Xian et al</td>
                <td>2021</td>
                <td>hypernetworks, meta-learning, dynamics</td>
                <td>ICLR</td>
                <td>The authors present a dynamics meta-learning framework which conditions on an agent&#x27;s interations w/ env and (optionally) the visual input from it. From this, they can generate params of a neural dynamics model. The three modules they use are 1) an encoding module that encodes a few agent-env interations / agent&#x27;s visual observations into a feature code, 2) a hypernet that conditions on the latent feature code to generate params of a dynamic model dedicated to this observed system, and 3) a target dynamics model that is made using the generated parameters, and takes input as a low-dim system state / agent action and outputs the prediction of next system state.</td>
                <td><a href="https://arxiv.org/pdf/2103.09439" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Principled Weight Initialization for Hypernetworks</td>
                <td>Oscar Chang et al</td>
                <td>2020</td>
                <td>hypernetworks, weight initialization</td>
                <td>ICLR</td>
                <td>Classical weight initialization techniques don&#x27;t really work on hypernets, because they fail to produce weights for the mainnet in the correct scale. The authors derive formulas for hyperfan-out and hyperfan-in weight initialization, and show that it works well for the mainnet.</td>
                <td><a href="https://arxiv.org/pdf/2312.08399" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Continual Learning with Hypernetworks</td>
                <td>Johannes von Oswald et al</td>
                <td>2020</td>
                <td>hypernetworks, continual learning, meta learning</td>
                <td>ICLR</td>
                <td>The authors present a method of preventing catastrophic forgetting, by using task-conditioned hypernets (i.e., hypernets that generate weights of target model based on some task embedding). Thus, rather than memorizing many data characteristics, we can split the problem into just learning a single point per task, given the task embedding.</td>
                <td><a href="https://arxiv.org/pdf/1906.00695" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Stochastic Hyperparameter Optimization through Hypernetworks</td>
                <td>Jonathan Lorraine et al</td>
                <td>2018</td>
                <td>hypernetworks, hyperparameters</td>
                <td>ICLR</td>
                <td>Using hypernetworks to learn hyperparameters. They replace the training optimization loop in favor of a differentiable hypernetwork to allow for tuning of hyperparameters using grad descent.</td>
                <td><a href="https://arxiv.org/pdf/1802.09419" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Playing Atari with Deep Reinforcement Learning</td>
                <td>Volodymyr Mnih et al</td>
                <td>2013</td>
                <td>q-learning, reinforcement learning</td>
                <td>Arxiv</td>
                <td>The authors present the first deep learning model that can learn complex control policies, and they teach it to play Atari 2600 games using Q-learning. Their goal was to create one net that can play as many games as possible.</td>
                <td><a href="https://arxiv.org/pdf/1312.5602" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Encoding</td>
                <td>Song Han et al</td>
                <td>2016</td>
                <td>quantization, encoding, pruning</td>
                <td>ICML</td>
                <td>A three-pronged approach to compressing nets. They prune networks, then quantize and share weights, and then apply Huffman encoding.</td>
                <td><a href="https://arxiv.org/pdf/1510.00149" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or -1</td>
                <td>Matthieu Courbariaux et al</td>
                <td>2016</td>
                <td>quantization, efficiency, binary</td>
                <td>Arxiv</td>
                <td>Introduction of training Binary Neural Networks, or nets with binary weights and activations. They also present experiments on deterministic vs stochastic binarization. They use the deterministic one for the most part, except for activations.</td>
                <td><a href="https://arxiv.org/pdf/1602.02830" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</td>
                <td>Mingxing Tan et al</td>
                <td>2020</td>
                <td>efficiency, scaling</td>
                <td>ICML</td>
                <td>A study of model scaling is presented. They propose a novel scaling method to uniformly scale all dimensions of depth/width/resolution using a compound coefficient. This paper presents a method for scaling width/depth/resolution; for instance, if you want to use 2^{N} more compute resources, then you can scale by their coefficients to do so. They also quantify the relationship between width, depth, and resolution.</td>
                <td><a href="https://arxiv.org/pdf/1905.11946" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>The wake-sleep algorithm for unsupervised neural networks</td>
                <td>Geoffry Hinton et al</td>
                <td>1995</td>
                <td>representation, generative</td>
                <td>Arxiv</td>
                <td>One of the first generative neural networks that kind of resembles diffusion.</td>
                <td><a href="https://www.cs.toronto.edu/~hinton/csc2535/readings/ws.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design</td>
                <td>Haoran You et al</td>
                <td>2022</td>
                <td>vit, accelerator, attention</td>
                <td>Arxiv</td>
                <td>Co-deisng for ViTs. Prunes and polarizes attention maps to have denser/sparser patterns. Development of hardware accelerator as well.</td>
                <td><a href="https://arxiv.org/pdf/2210.09573" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Evolving Neural Networks through Augmenting Topologies</td>
                <td>Kenneth O. Stanley et al</td>
                <td>2002</td>
                <td>nas, evolution</td>
                <td>Arxiv</td>
                <td>Evolution for NAS.</td>
                <td><a href="https://www.cse.unr.edu/~sushil/class/gas/papers/NEAT.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>A Brief Review of Hypernetworks in Deep Learning</td>
                <td>Vinod Kumar Chauhan et al</td>
                <td>2024</td>
                <td>hypernetwork</td>
                <td>Arxiv</td>
                <td>Review of hypernets.</td>
                <td><a href="https://arxiv.org/pdf/2306.06955" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>HyperNetworks</td>
                <td>David Ha et al</td>
                <td>2016</td>
                <td>hypernetwork</td>
                <td>Arxiv</td>
                <td>Looking at HyperNetworks: networks that generate weights for other networks.</td>
                <td><a href="https://arxiv.org/pdf/1609.09106" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Deep Learners Benefit More from Out-of-Distribution Examples</td>
                <td>Yoshio Bengio et al</td>
                <td>2024</td>
                <td>ood</td>
                <td>ICML</td>
                <td>Evidence that ood samples can help learning. They also argue that intermediate levels of representation can benefit the models in multi-task settings.</td>
                <td><a href="http://proceedings.mlr.press/v15/bengio11b/bengio11b.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance</td>
                <td>Chiraag Kaushik et al</td>
                <td>2024</td>
                <td>spectra, class imbalance</td>
                <td>ICML</td>
                <td>Introduction of the idea of &quot;spectral imbalance&quot;, which can affect classification accuracy even when classes are balanced. Basically, they look at how the distributions of eigenvalues in different classes affect classification accuracy.</td>
                <td><a href="https://arxiv.org/pdf/2402.11742" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>DeepArchitect: Automatically Designing and Training Deep Architectures</td>
                <td>Renato Negrinho et al</td>
                <td>2017</td>
                <td>nas</td>
                <td>Arxiv</td>
                <td>Proposal of a language to describe neural networks architectures. Can then describe them as trees to search through. Show different search methods for going through the trees (Monte Carlo tree search, random, use of surrogate function, etc.).</td>
                <td><a href="https://arxiv.org/pdf/1704.08792" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Graph neural networks: A review of methods and applications</td>
                <td>Jie Zhou et al</td>
                <td>2020</td>
                <td>gnn</td>
                <td>AI Open</td>
                <td>What graph neural networks are, what they are made of, how to train them. And examples. They describe a general design pipeline (Find graph structure, specify graph type and scale, design loss function) and explain the main modules in GNNs (propagation to propagate information between notes, sampling module to conduct the propagation, pooling module to extract information from notes).</td>
                <td><a href="https://arxiv.org/pdf/1812.08434" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>1D convolution neural networks and applications: A survey</td>
                <td>Serkan Kiranyaz et al</td>
                <td>2020</td>
                <td>cnn, survey</td>
                <td>Mechanical Systems and Signal Processing</td>
                <td>A brief overview of applications of 1D CNNs is performed. It is largely focused on medicine (for instance, ECG) and fault detection (for instance, vibration based structural damage).</td>
                <td><a href="https://arxiv.org/pdf/1905.03554" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>2 in 1 Accelerator: Enabling Random Precision Switch for Winning Both Adversarial Robustness and Efficiency</td>
                <td>Yonggan Fu et al</td>
                <td>2021</td>
                <td>quantization, accelerator</td>
                <td>ACM</td>
                <td>The most interesting point of this paper (among many things!) is the smart idea to use quantization as a way to boost DNN robustness. Cool!</td>
                <td><a href="https://arxiv.org/pdf/2109.05223" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Token Picker: Accelerating Attention in Text Generation with Minimized Memory Transfer via Probability Estimation</td>
                <td>Junyoung Park et al</td>
                <td>2024</td>
                <td>efficiency, hardware, accelerator, attention</td>
                <td>DAC</td>
                <td>In autoregressive models with attention, off chip memory accesses need to be minimized. The authors note that there have been efforts to prune unimportant tokens, but these do not do much for removing tokens with attention scores near zero. The authors (smartly!) notice this issue, and provide a fast method of estimating the decision to prune or not based on estimation of the probability if a token is or is not important. An architecture for this is also provided.</td>
                <td><a href="https://arxiv.org/pdf/2407.15131" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Maxout Networks</td>
                <td>Ian Goodfellow et al</td>
                <td>2013</td>
                <td>dropout, maxout</td>
                <td>ICML</td>
                <td>The authors note that dropout is &quot;most effective when taking relatively large steps in parameter space. In this regime, each update can be seen as making a significat update to a different model on a different subset of the training set&quot;. I really liked that quote. They then develop the maxout unit, which iessentially takes the maxmimum across some number of affine transformations, allowing for learning of piecewise linear approximations of nonlinear functions.</td>
                <td><a href="https://arxiv.org/pdf/1302.4389" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Geometric deep learning: Going beyond Euclidean data</td>
                <td>Michael Bronstein et al</td>
                <td>2017</td>
                <td>geometric deep learning</td>
                <td>IEEE SIG</td>
                <td>Provides an overview of geometric deep learning, which are methods of generalizing DNNs to non Euclidean domains (graphs, manifolds, etc).</td>
                <td><a href="https://arxiv.org/pdf/1611.08097" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Sampling in Constrained Domains with Orthogonal Space Variational Gradient Descent</td>
                <td>Ruqi Zhang et al</td>
                <td>2022</td>
                <td>variational gradient descent, gradient flow</td>
                <td>NeurIPS</td>
                <td>The authors propose a new variational framework called O Gradient for sampling in implicitly defined constrained domains, using two orthogonal directions to drive the sampler towards the domain and explore it by decreasing a KL divergence. They prove the convergence of O Gradient and apply it to both Langevin dynamics and Stein Variational Gradient Descent (SVGD), demonstrating its effectiveness on various machine learning tasks.</td>
                <td><a href="https://arxiv.org/pdf/2210.06447" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Entropy MCMC: Sampling from Flat Basins with Ease</td>
                <td>Bolian Li et al</td>
                <td>2024</td>
                <td>sampling, bayesian, flat basins</td>
                <td>ICML</td>
                <td>The authors propose a practical MCMC algorithm for sampling from flat basins of DNN posterior distributions, using a guiding variable based on local entropy to steer the sampler. They prove the fast convergence rate of their method compared to existing flatness aware methods and demonstrate its superior performance on various tasks through comprehensive experiments. The method is mathematically simple and computationally efficient, making it suitable as a drop in replacement for standard sampling methods like SGLD.</td>
                <td><a href="https://arxiv.org/pdf/2310.05401" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>AdderNet: Do We Really Need Multiplications in Deep Learning?</td>
                <td>Hanting Chen et al</td>
                <td>2021</td>
                <td>multiplication-less, efficiency</td>
                <td>CVPR</td>
                <td>The authors show that with a cost of accuracy you can use additions instead of multiplications. They mainly tested CNNs.</td>
                <td><a href="https://arxiv.org/pdf/1912.13200" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Explaining and Harnessing Adversarial Examples</td>
                <td>Ian Goodfellow et al</td>
                <td>2015</td>
                <td>adversarial examples</td>
                <td>ICLR</td>
                <td>Adversarial examples (adding &quot;small but intentially worst case perturbations to examples from the dataset&quot;) proves to be an interesting method to train models. The authors also (smartly!) describe a method to generate adversarial examples by a linear method.</td>
                <td><a href="https://arxiv.org/pdf/1412.6572" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Identifying and attacking the saddle point problem in high dimensional non convex optimization</td>
                <td>Yann Dauphin et al</td>
                <td>2014</td>
                <td>saddle points, optimization</td>
                <td>NeurIPS</td>
                <td>The authors argue that saddle points, rather than local minima, are the primary challenge in minimizing non convex error functions in high dimensional spaces, based on insights from various scientific fields and empirical evidence. They explain that saddle points surrounded by high error plateaus can significantly slow down learning and create the illusion of local minima, particularly in high dimensional problems of practical interest. To address this challenge, the authors propose a new approach called the saddle free Newton method, designed to quickly escape high dimensional saddle points, unlike traditional gradient descent and quasi Newton methods.</td>
                <td><a href="https://arxiv.org/pdf/1406.2572" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</td>
                <td>Sergey Ioffe et al</td>
                <td>2015</td>
                <td>batch, normalization</td>
                <td>PMLR</td>
                <td>The authors identify internal covariate shift as a significant challenge in training deep neural networks, where the distribution of each layer&#x27;s inputs changes during training due to parameter updates in previous layers. To address this issue, they propose Batch Normalization, a method that normalizes layer inputs as part of the model architecture, performing normalization for each training mini batch. Batch Normalization enables the use of much higher learning rates, reduces sensitivity to initialization, and acts as a regularizer, sometimes eliminating the need for Dropout.|</td>
                <td><a href="https://arxiv.org/pdf/1502.03167" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Bayesian Deep Learning and a Probabilistic Perspective of Generalization</td>
                <td>Andrew Wilson et al</td>
                <td>2020</td>
                <td>bayesian, marginalization</td>
                <td>NeurIPS</td>
                <td>The authors emphasize that marginalization, rather than using a single set of weights, is the key distinguishing feature of a Bayesian approach, which can significantly improve the accuracy and calibration of modern deep neural networks. They demonstrate that deep ensembles provide an effective mechanism for approximate Bayesian marginalization and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction. The paper investigates the prior over functions implied by a vague distribution over neural network weights, explaining neural network generalization from a probabilistic perspective and showing that seemingly mysterious results (like fitting random labels) can be reproduced with Gaussian processes. The authors demonstrate that Bayesian model averaging mitigates the double descent phenomenon, leading to monotonic performance improvements as model flexibility increases.</td>
                <td><a href="https://arxiv.org/pdf/2002.08791" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>A Practical Bayesian Framework for Backpropagation Networks</td>
                <td>David MacKay et al</td>
                <td>1992</td>
                <td>bayesian</td>
                <td>Neural Computation</td>
                <td>The authors present a quantitative and practical Bayesian framework for learning mappings in feedforward networks, enabling objective comparisons between different network architectures and providing stopping rules for network pruning or growing procedures. This framework allows for objective selection of weight decay terms or regularizers, measures the effective number of well determined parameters in a model, and provides quantified estimates of error bars on network parameters and outputs. The approach helps detect poor underlying assumptions in learning models and demonstrates a good correlation between generalization ability and Bayesian evidence for well matched learning models.</td>
                <td><a href="https://ieeexplore.ieee.org/document/6796869" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Bayesian Neural Network Priors Revisited</td>
                <td>Vincent Fortuin et al</td>
                <td>2022</td>
                <td>bayesian, priors</td>
                <td>ICLR</td>
                <td>Isotropic Gaussian priors are the standard for modern Bayesian neural network inference, but their accuracy and optimal performance are uncertain. By studying summary statistics of neural network weights trained with stochastic gradient descent (SGD), the authors find that CNN and ResNet weights exhibit strong spatial correlations, while FCNNs display heavy tailed weight distributions. Incorporating these observations into priors improves performance on various image classification datasets, mitigating the cold posterior effect in FCNNs but slightly increasing it in ResNets.</td>
                <td><a href="https://arxiv.org/pdf/2102.06571" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Hands on Bayesian Neural Networks   A Tutorial for Deep Learning Users</td>
                <td>Laurent Jospin et al</td>
                <td>2022</td>
                <td>bayesian</td>
                <td>IEEE</td>
                <td>A good summary / tutorial for using Bayesian Nets. Also provides some good paper references within.</td>
                <td><a href="https://arxiv.org/pdf/2007.06823" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Position Paper: Bayesian Deep Learning in the Age of Large Scale AI</td>
                <td>Theodore Papamarkou et al</td>
                <td>2024</td>
                <td>bayesian, mcmc</td>
                <td>ICML</td>
                <td>A good summary of strengths of BDL (Bayesian Deep Learning) with regards to modern deep learning, while also addressing some weaknesses. A good paper if need to do an overview of modern challenges (as of 2024).|</td>
                <td><a href="https://arxiv.org/pdf/2402.00809" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>A Neural Probabilistic Language Model</td>
                <td>Bengio et al</td>
                <td>2003</td>
                <td>statistical language modeling</td>
                <td>JMLR</td>
                <td>One of the first papers about modern methods in using neural systems to estimate probability functions of word sequences. They show that MLPs can model better than the SOTA (at that time). A classic.|</td>
                <td><a href="https://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Bit Fusion: Bit Level Dynamically Composable Architecture for Accelerating Deep Neural Networks</td>
                <td>Hardik Sharma et al</td>
                <td>2018</td>
                <td>accelerator, quantization, bit fusion</td>
                <td>ISCA</td>
                <td>Hardware acceleration of Deep Neural Networks (DNNs) aims to address their high compute intensity, with the paper focusing on the potential of reducing bitwidth in operations without compromising classification accuracy. To prevent accuracy loss, the bitwidth varies significantly across DNNs, and a fixed bitwidth accelerator may lead to limited benefits or degraded accuracy. The authors introduce Bit Fusion, a bit flexible accelerator that dynamically adjusts bitwidth for individual DNN layers, resulting in significant speedup and energy savings compared to state of the art accelerators, Eyeriss and Stripes, and achieving performance close to a 250 Watt Titan Xp GPU while consuming much less power.</td>
                <td><a href="https://arxiv.org/pdf/1712.01507" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>A Framework for the Cooperation of Learning Algorithms</td>
                <td>Leon Bottou et al</td>
                <td>1990</td>
                <td>learning algorithms, modules</td>
                <td>NeurIPS</td>
                <td>Cooperative training of modular systems offers a unified approach to many learning algorithms and hybrid systems, allowing the design and implementation of complex learning systems that incorporate structural a priori knowledge about tasks. The authors introduce a framework using a statistical formulation of learning systems to define and combine modules into cooperative systems, enabling the creation of hybrid systems that combine the advantages of connectionist and other learning algorithms. By decomposing complex tasks into simpler subtasks, modular architectures can be built, where each module corresponds to a subtask, facilitating easier achievement of the learning goal by introducing a modular decomposition of the global task.</td>
                <td><a href="https://proceedings.neurips.cc/paper/1990/file/a8c88a0055f636e4a163a5e3d16adab7-Paper.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>CNP: An FPGA Based Processor for Convolutional Networks</td>
                <td>Clement Farabet et al</td>
                <td>2009</td>
                <td>fpga, cnn</td>
                <td>IEEE</td>
                <td>One of the first attempts (that I have found) at putting a CNN into an FPGA and showing it can be done to perform some task (face detection).</td>
                <td><a href="https://yann.lecun.com/exdb/publis/pdf/farabet-fpl-09.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>A Complete Recipe for Stochastic Gradient MCMC</td>
                <td>Yi An Ma et al</td>
                <td>2015</td>
                <td>hamiltonian, mcmc</td>
                <td>NeurIPS</td>
                <td>Recent Markov chain Monte Carlo (MCMC) samplers use continuous dynamics and scalable variants with stochastic gradients to efficiently explore target distributions, but proving convergence with stochastic gradient noise remains challenging. The authors provide a general framework for constructing MCMC samplers, including stochastic gradient versions, based on continuous Markov processes defined by two matrices, demonstrating that any such process can be represented within this framework. Using this framework, they propose a new state adaptive sampler, stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC), which combines the benefits of Riemann HMC with the scalability of stochastic gradient methods, as shown in experiments with simulated data and a streaming Wikipedia analysis.</td>
                <td><a href="https://arxiv.org/pdf/1506.04696" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>CPT: Efficient Deep Neural Network Training via Cyclic Precision</td>
                <td>Yonggan Fu et al</td>
                <td>2021</td>
                <td>precision, efficiency, wide minima</td>
                <td>ICLR</td>
                <td>Low precision deep neural network (DNN) training is an effective method for improving training time and energy efficiency, with this paper proposing a new perspective: that DNN precision may act similarly to the learning rate during training. The authors introduce Cyclic Precision Training (CPT), which cyclically varies precision between two boundary values identified through a simple precision range test in the initial training epochs, aiming to boost time and energy efficiency further. </td>
                <td><a href="https://arxiv.org/pdf/2101.09868" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Approximation by Superpositions of a Sigmoidal Function</td>
                <td>G. Cybenko</td>
                <td>universal approximator, completeness</td>
                <td>TODO</td>
                <td>Mathematics of Control, Signals, and Systems</td>
                <td>This paper demonstrates that finite linear combinations of compositions of a fixed univariate function and a set of affine functionals can uniformly approximate any continuous function of nn real variables within the unit hypercube, under mild conditions on the univariate function. These findings resolve an open question about the representability of functions by single hidden layer neural networks, specifically showing that arbitrary decision regions can be well approximated by continuous feedforward neural networks with a single hidden layer and any continuous sigmoidal nonlinearity.</td>
                <td><a href="https://cognitivemedium.com/magic_paper/assets/Cybenko.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning</td>
                <td>Ruqi Zhang et al</td>
                <td>2020</td>
                <td>mcmc, bayesian</td>
                <td>ICLR</td>
                <td>The posteriors over neural network weights are high dimensional and multimodal, with each mode representing a different meaningful interpretation of the data. The authors introduce Cyclical Stochastic Gradient MCMC (SG MCMC) with a cyclical stepsize schedule, where larger steps discover new modes and smaller steps characterize each mode, and they prove the non asymptotic convergence of this algorithm.</td>
                <td><a href="https://arxiv.org/pdf/1902.03932" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>DaDianNao: A Machine Learning Supercomputer</td>
                <td>Yunji Chen et al</td>
                <td>2014</td>
                <td>accelerator, gpu</td>
                <td>IEEE/ACM</td>
                <td>This paper introduces a custom multi chip architecture optimized for Convolutional and Deep Neural Networks (CNNs and DNNs), addressing their computational and memory intensive nature by leveraging on chip storage to enhance internal bandwidth and reduce external communication bottlenecks. The authors demonstrate significant performance gains with their 64 chip system achieving up to a 450.65x speedup over GPUs and reducing energy consumption by up to 150.31x on large neural network layers, implemented with custom storage, computational units, and robust interconnects at 28nm scale.</td>
                <td><a href="https://www.cs.virginia.edu/~smk9u/CS6501F16/p609-chen.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>DARTS: Differentiable Architecture Search</td>
                <td>Hanxiao Liu et al</td>
                <td>2019</td>
                <td>nas</td>
                <td>ICLR</td>
                <td>This paper introduces a differentiable approach to architecture search, tackling scalability challenges by reformulating the task to allow gradient based optimization over a continuous relaxation of architecture representations. Unlike traditional methods relying on evolutionary or reinforcement learning in discrete, non differentiable spaces, the proposed method efficiently discovers high performance convolutional architectures for image classification and recurrent architectures for language modeling.</td>
                <td><a href="https://arxiv.org/pdf/1806.09055" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Decoupled Contrastive Learning</td>
                <td>Chun Hsiao Yeh et al</td>
                <td>2022</td>
                <td>contrastive learning, self-supervised learning</td>
                <td>ACM</td>
                <td>This paper introduces decoupled contrastive learning (DCL), which removes the negative positive coupling (NPC) effect from the InfoNCE loss, significantly improving the efficiency of self supervised learning (SSL) tasks with smaller batch sizes. DCL achieves efficient and reliable performance enhancements across various benchmarks, outperforming the SimCLR baseline without requiring momentum encoding, large batch sizes, or extensive epochs.</td>
                <td><a href="https://arxiv.org/pdf/2110.06848" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Deep Image Prior</td>
                <td>Dmitry Ulyanov et al</td>
                <td>2020</td>
                <td>inpatining, super-resolution, denoising</td>
                <td>IEEE</td>
                <td>This paper challenges the conventional wisdom by demonstrating that the structure of a generator network, even when randomly initialized, can effectively capture low level image statistics without any specific training on example images. The authors show that this randomly initialized neural network can serve as a powerful handcrafted prior, yielding excellent results in standard image processing tasks such as denoising, super resolution, and inpainting. Furthermore, the same network structure can invert deep neural representations for diagnostic purposes and restore images based on input pairs like flash and no flash conditions, showcasing its versatility and effectiveness across various image restoration applications.</td>
                <td><a href="https://arxiv.org/pdf/1711.10925" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Deep Double Descent: Where Bigger Models and More Data Hurt</td>
                <td>Preetum Nakkiran et al</td>
                <td>2019</td>
                <td>capacity, double descent</td>
                <td>Arxiv</td>
                <td>This paper explores the &quot;double descent&quot; phenomenon in modern deep learning tasks, showing that as model size or training epochs increase, performance initially worsens before improving. The authors unify these observations by introducing a new complexity measure termed effective model complexity, conjecturing a generalized double descent across this measure.</td>
                <td><a href="https://arxiv.org/pdf/1912.02292" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>DeepShift: Towards Multiplication Less Neural Networks</td>
                <td>Mostafa Elhoushi et al</td>
                <td>2021</td>
                <td>multiplication-less, efficiency</td>
                <td>Arxiv</td>
                <td>This paper addresses the computational challenges of deploying convolutional neural networks (CNNs) on edge computing platforms by introducing convolutional shifts and fully connected shifts, replacing multiplications with efficient bitwise operations during both training and inference. The proposed DeepShift models achieve competitive or higher accuracies compared to baseline models like ResNet18, ResNet50, VGG16, and GoogleNet, while significantly reducing the memory footprint by using only 5 bits or less for weight representation during inference.</td>
                <td><a href="https://arxiv.org/pdf/1905.13298" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>DepthShrinker: A New Compression Paradigm Towards Boosting Real Hardware Efficiency of Compact Neural Networks</td>
                <td>Yonggan Fu et al</td>
                <td>2022</td>
                <td>compression, efficiency, pruning</td>
                <td>ICML</td>
                <td>This paper introduces DepthShrinker, a framework designed to enhance hardware efficiency of deep neural networks (DNNs) by transforming irregular computation patterns of compact operators into dense ones, thereby improving hardware utilization without sacrificing model accuracy. By leveraging insights that certain activation functions can be removed post training without loss of accuracy, DepthShrinker pioneers a compression paradigm that optimizes DNNs for real hardware efficiency, presenting a significant advancement in efficient model deployment.|</td>
                <td><a href="https://arxiv.org/pdf/2206.00843" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Dimensionality Reduction by Learning an Invariant Mapping</td>
                <td>Raia Hadsell et al</td>
                <td>2006</td>
                <td>dimensionality reduction, mapping</td>
                <td>CVPR</td>
                <td>DrLIM, or Dimensionality Reduction by Learning an Invariant Mapping, addresses key limitations of existing dimensionality reduction techniques by learning a non linear function that maps high dimensional data to a low dimensional manifold based solely on neighborhood relationships, without requiring a predefined distance metric in input space. The method is distinguished by its ability to handle transformations and maintain invariant mappings, demonstrated through experiments that show its effectiveness in preserving neighborhood relationships and accurately mapping new, unseen samples to meaningful locations on the manifold. Unlike methods like LLE, which may struggle with variability and registration issues in input data, DrLIM&#x27;s contrastive loss function ensures robustness by balancing similarity and dissimilarity in output space, offering a promising approach for applications requiring invariant mappings, such as learning positional information from image sequences in robotics.</td>
                <td><a href="https://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Disentangling Trainability and Generalization in Deep Neural Networks</td>
                <td>Lechao Xiao et al</td>
                <td>2020</td>
                <td>neural tangent kernel, ntk</td>
                <td>ICML</td>
                <td>This study focuses on characterizing the trainability and generalization of deep neural networks, particularly under conditions of very wide and very deep architectures, leveraging insights from the Neural Tangent Kernel (NTK). By analyzing the NTK&#x27;s spectrum, the study formulates necessary conditions for both memorization and generalization across architectures like Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs). The research identifies key spectral quantities such as λmax, λbulk, κ, and P(Θ(l)) that critically influence the performance of deep networks, providing a precise theoretical framework validated by extensive experiments on CIFAR10. It highlights distinctions in generalization behavior between CNNs with and without global average pooling.</td>
                <td><a href="https://arxiv.org/pdf/1912.13053" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Finding Structure in Time</td>
                <td>Jeffrey Elman</td>
                <td>1990</td>
                <td>rnn</td>
                <td>Cognitive Science</td>
                <td>I think this was the original backpropagation through time paper. Good insights on time dependent system learning.</td>
                <td><a href="https://axon.cs.byu.edu/~martinez/classes/678/Papers/Elman_time.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>E2 Train: Training State of the art CNNs with Over 80% Energy Savings</td>
                <td>Yue Wang et al</td>
                <td>2019</td>
                <td>cnn, batch, energy</td>
                <td>NeurIPS</td>
                <td>This paper introduces E2 Train, a framework for energy efficient CNN training on resource constrained platforms. E2 Train optimizes training energy costs through stochastic mini batch dropping, selective layer updates, and low cost, low precision back propagation strategies. Experimental results on CIFAR 10 and CIFAR 100 demonstrate significant energy savings of over 90% and 84%, respectively, with minimal loss in accuracy. E2 Train addresses the challenge of on device CNN training by integrating three levels of energy saving techniques: data level stochastic mini batch dropping, model level selective layer updates, and algorithm level low precision back propagation. Real energy measurements on an FPGA validate its effectiveness, achieving notable energy reductions in training ResNet models on CIFAR datasets.</td>
                <td><a href="https://arxiv.org/pdf/1910.13349" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>cuDNN: Efficient Primitives for Deep Learning</td>
                <td>Sharan Chetlur et al</td>
                <td>2014</td>
                <td>cuda, gpu</td>
                <td>Arxiv</td>
                <td>This paper introduces cuDNN, a library designed to optimize deep learning primitives akin to BLAS for HPC. cuDNN offers efficient implementations of key deep learning kernels tailored for GPUs, improving performance and reducing memory usage in frameworks like Caffe by up to 36%.</td>
                <td><a href="https://arxiv.org/pdf/1410.0759" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>EIE: Efficient Inference Engine on Compressed Deep Neural Network</td>
                <td>Song Han et al</td>
                <td>2016</td>
                <td>compression, accelerator, co-design</td>
                <td>Arxiv</td>
                <td>This paper introduces EIE, an energy efficient inference engine designed for compressed deep neural networks, achieving significant energy savings by exploiting weight sharing, sparsity, and quantization. EIE performs sparse matrix vector multiplications directly on compressed models, enabling 189× and 13× faster inference speeds compared to CPU and GPU implementations of uncompressed DNNs.</td>
                <td><a href="https://arxiv.org/pdf/1602.01528" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>An Empirical Analysis of Deep Network Loss Surfaces</td>
                <td>Daniel Jiwoong Im et al</td>
                <td>2016</td>
                <td>optimization, loss surface, saddle points</td>
                <td>Arxiv</td>
                <td>This paper empirically investigates the geometry of loss functions in state of the art neural networks, employing various stochastic optimization methods. Through visualizations in low dimensional subspaces, it explores how different optimization procedures lead to distinct local minima, even when algorithms are changed late in the optimization process. The study reveals that modifications to optimization procedures consistently yield different local minima, each affecting the network&#x27;s performance on test examples differently. Interestingly, while different optimization algorithms find varied local minima from different initializations, the shape of the loss function around these minima remains characteristic to the algorithm used, with ADAM showing larger basins compared to vanilla SGD.</td>
                <td><a href="https://arxiv.org/pdf/1612.04010" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>EyeCoD: Eye Tracking System Accelerator via FlatCam based Algorithm &amp; Accelerator Co Design</td>
                <td>Haoran You et al</td>
                <td>2023</td>
                <td>accelerator, co-design, eye-tracking</td>
                <td>ACM</td>
                <td>This paper introduces EyeCoD, a lensless FlatCam based eye tracking system designed to overcome limitations of traditional systems, such as large form factor and high communication costs. By integrating a predict then focus algorithm pipeline and dedicated hardware accelerator, EyeCoD achieves significant reductions in computation and communication overhead while maintaining high tracking accuracy.</td>
                <td><a href="https://arxiv.org/pdf/2206.00877" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices</td>
                <td>Yu Hsin Chen et al</td>
                <td>2019</td>
                <td>efficiency, sparsity</td>
                <td>Arxiv</td>
                <td>This paper introduces Eyeriss v2, a specialized DNN accelerator architecture designed to efficiently handle compact and sparse neural networks. Unlike traditional DNN accelerators, Eyeriss v2 incorporates a hierarchical mesh network on chip to adapt to varying layer shapes and sizes, optimizing data reuse and bandwidth utilization. Eyeriss v2 excels in processing sparse data directly in the compressed domain, both for weights and activations, thereby enhancing processing speed and energy efficiency particularly suited for sparse models.</td>
                <td><a href="https://arxiv.org/pdf/1807.07928" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Eyeriss: A Spatial Architecture for Energy Efficient Dataflow for Convolutional Neural Networks</td>
                <td>Yu Hsin Chen et al</td>
                <td>2016</td>
                <td>cnn, row-stationary, efficiency</td>
                <td>ACM/IEEE</td>
                <td>The paper addresses the high energy consumption in deep convolutional neural networks (CNNs) due to extensive data movement, despite advancements in parallel computing paradigms like SIMD/SIMT. Introduces a novel row stationary (RS) dataflow designed for spatial architectures. RS maximizes local data reuse and minimizes data movement during convolutions, leveraging PE local storage, inter PE communication, and spatial parallelism.</td>
                <td><a href="https://eems.mit.edu/wp-content/uploads/2016/04/eyeriss_isca_2016.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Flat Minima</td>
                <td>Sepp Hochreiter et al</td>
                <td>1997</td>
                <td>flat minima, low complexity, gibbs</td>
                <td>Neural Computation</td>
                <td>The algorithm focuses on identifying &quot;flat&quot; minima of the error function in weight space. A flat minimum is characterized by a large connected region where the error remains approximately constant. This property suggests simplicity in the network structure and low expected overfitting, supported by an MDL based Bayesian argument. Unlike traditional approaches that rely on Gaussian assumptions or specific weight priors, this algorithm uses a Bayesian framework with a prior over input output functions. This approach considers both network architecture and the training set, facilitating the identification of simpler and more generalizable models.</td>
                <td><a href="https://www.researchgate.net/publication/14100213_Flat_Minima" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Fused Layer CNN Accelerators</td>
                <td>Manoj Alwani et al</td>
                <td>2016</td>
                <td>cnn, accelerator, fusion</td>
                <td>IEEE</td>
                <td>This work introduces a novel approach to CNN accelerator design by fusing the computation of multiple convolutional layers. By rearranging the dataflow across layers, intermediate data can be cached on chip between adjacent layers, reducing the need for off chip memory storage and minimizing data transfer. Specifically, the study demonstrates the effectiveness of this approach by implementing a fused layer CNN accelerator for the initial five convolutional layers of VGGNet E. Using 362KB of on chip storage, the accelerator significantly reduces off chip feature map data transfer by 95%, from 77MB to 3.6MB per image processed.  This innovation targets early convolutional layers where data transfer typically dominates. By optimizing data reuse and minimizing off chip memory usage, the proposed design strategy enhances the efficiency of CNN accelerators, paving the way for improved performance in various machine learning tasks.</td>
                <td><a href="https://compas.cs.stonybrook.edu/~mferdman/downloads.php/MICRO16_Fused_Layer_CNN_Accelerators.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>EnlightenGAN: Deep Light Enhancement Without Paired Supervision</td>
                <td>Yifan Jiang et al</td>
                <td>2021</td>
                <td>gan, enhancement, unsupervised</td>
                <td>IEEE</td>
                <td>Exploration of low light to well lit image generation using GANs. Also provides an interesting global local discriminator and self regularized perceptual loss fusion, with a simplified attention (the attention is just an inverse of the brightness of the image essentially).</td>
                <td><a href="https://arxiv.org/pdf/1906.06972" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Understanding the difficulty of training deep feedforward neural networks</td>
                <td>Xavier Glorot et al</td>
                <td>2010</td>
                <td>activation, saturation, initialization</td>
                <td>AISTATS</td>
                <td>The logistic sigmoid activation function is problematic for deep networks due to its mean value, which can lead to saturation of the top hidden layer. This saturation slows down learning and can cause training plateaus. The difficulty in training deep networks correlates with the singular values of the Jacobian matrix for each layer. When these values deviate significantly from 1, it indicates poor activation and gradient flow across layers, complicating training. New initialization schemes have been proposed to address issues with activation saturation and gradient flow. These schemes aim to achieve faster convergence by ensuring that activations and gradients flow well across layers, thereby improving overall training efficiency.</td>
                <td><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Group Normalization</td>
                <td>Yuxin Wu et al</td>
                <td>2018</td>
                <td>normalization</td>
                <td>Arxiv</td>
                <td>Batch Normalization performs normalization along the batch dimension, which causes errors to increase rapidly as batch sizes decrease. This limitation makes BN less effective for training larger models and tasks that require smaller batches due to memory constraints. GN divides channels into groups and computes normalization statistics (mean and variance) independently within each group. Unlike BN, GN&#x27;s computation is not dependent on batch sizes, leading to stable performance across a wide range of batch sizes.</td>
                <td><a href="https://arxiv.org/pdf/1803.08494" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Singularity of the Hessian in Deep Learning</td>
                <td>Levent Sagun et al</td>
                <td>2017</td>
                <td>eigenvalues, hessian</td>
                <td>ICLR</td>
                <td>The bulk of eigenvalues concentrated around zero indicates how overparametrized the model is. In deep learning, overparametrization often leads to better generalization despite the potential for higher computational costs. The edges of the eigenvalue distribution, scattered away from zero, reflect the complexity of the input data. This complexity influences how the loss landscape is structured and affects optimization difficulty. Second order optimization methods, which leverage information from the Hessian, can potentially accelerate training and find better solutions by providing insights into the loss landscape&#x27;s curvature. The top discrete eigenvalues of the Hessian are influenced by the data characteristics, indicating that different datasets may require different optimization strategies or model architectures for optimal performance.</td>
                <td><a href="https://arxiv.org/pdf/1611.07476" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Long Short Term Memory</td>
                <td>Sepp Hochreiter et al</td>
                <td>1997</td>
                <td>lstm, rnn</td>
                <td>Neural Computation</td>
                <td>The original paper on the LSTM. A classic, and demonstrated the power of gating.</td>
                <td><a href="https://deeplearning.cs.cmu.edu/F23/document/readings/LSTM.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>On the importance of initialization and momentum in deep learning</td>
                <td>Ilya Sutskever et al</td>
                <td>2013</td>
                <td>initialization, momentum</td>
                <td>ICML</td>
                <td>Traditionally, training DNNs and RNNs with stochastic gradient descent (SGD) with momentum was considered challenging due to issues with gradient propagation and vanishing/exploding gradients, especially in networks with many layers or long term dependencies. The paper demonstrates that using a well designed random initialization significantly improves the training success of deep and recurrent networks with SGD and momentum.</td>
                <td><a href="https://www.cs.toronto.edu/~fritz/absps/momentum.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Algorithms for manifold learning</td>
                <td>Lawrence Cayton</td>
                <td>2005</td>
                <td>manifold learning, dimensionality reduction</td>
                <td>Arxiv</td>
                <td>Many datasets exhibit complex relationships that cannot be effectively captured by linear methods like Principal Component Analysis (PCA). Manifold hypothesis: Despite high dimensional appearances, data points often lie on or near a much lower dimensional manifold embedded within the higher dimensional space. Manifold learning aims to uncover this underlying low dimensional structure to provide a more meaningful and compact representation of the data.</td>
                <td><a href="https://cseweb.ucsd.edu/~lcayton/resexam.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</td>
                <td>Ben Mildenhall et al</td>
                <td>2020</td>
                <td>nerf, view synthesis, 3d, scene representation, volume rendering</td>
                <td>Arxiv</td>
                <td>The method utilizes a fully connected deep network to represent scenes as continuous volumetric functions. This network takes a 5D input (spatial location and viewing direction) and outputs volume density and view dependent radiance. By querying these 5D coordinates along camera rays and employing differentiable volume rendering techniques, the method synthesizes novel views of scenes.</td>
                <td><a href="https://arxiv.org/pdf/2003.08934" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>On Large Batch Training for Deep Learning Generalization Gap and Sharp Minima</td>
                <td>Nitish Shirish Keskar et al</td>
                <td>2017</td>
                <td>sharp minima, large batch</td>
                <td>ICLR</td>
                <td>The study identifies a phenomenon where large batch SGD methods tend to converge towards sharp minimizers of training and testing functions. Sharp minima are associated with poorer generalization, meaning the model performs worse on unseen data. In contrast, small batch methods more consistently converge towards flat minimizers. This behavior is attributed to the inherent noise in gradient estimation during training with small batches.</td>
                <td><a href="https://arxiv.org/pdf/1609.04836" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Optimizing FPGA based Accelerator Design for Deep Convolutional Neural Networks</td>
                <td>Chen Zhang et al</td>
                <td>2015</td>
                <td>cnn, fpga, accelerator</td>
                <td>ACM</td>
                <td>The study employs quantitative analysis techniques, including loop tiling and transformation, to optimize the CNN accelerator design. These techniques aim to maximize computation throughput while minimizing the resource utilization on the FPGA, particularly balancing logic resource usage and memory bandwidth.|</td>
                <td><a href="https://dl.acm.org/doi/10.1145/2684746.2689060" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Learning Phrase Representation using RNN Encoder Decoder for Statistical Machine Translation</td>
                <td>Kyunghyun Cho et al</td>
                <td>2014</td>
                <td>encoder decoder, machine translation</td>
                <td>Arxiv</td>
                <td>Introduces a novel neural network architecture called RNN Encoder Decoder, comprising two recurrent neural networks. One RNN serves as an encoder, converting a sequence of symbols into a fixed length vector representation. The other RNN acts as a decoder, generating another sequence of symbols based on the encoded representation.</td>
                <td><a href="https://arxiv.org/pdf/1406.1078" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Qualitatively Characterizing Neural Network Optimization Problems</td>
                <td>Ian Goodfellow et al</td>
                <td>2015</td>
                <td>optimization, visualization</td>
                <td>ICLR</td>
                <td>Demonstrates that contemporary neural networks can achieve minimal training error through direct training with stochastic gradient descent alone, without needing complex schemes like unsupervised pretraining. This finding challenges earlier beliefs about the difficulty of navigating non convex optimization landscapes in neural network training. They also introduce a nice graphical tool to show the energy landscape.</td>
                <td><a href="https://arxiv.org/pdf/1412.6544" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Language Models are Unsupervised Multitask Learners</td>
                <td>Alec Radford et al</td>
                <td>2018</td>
                <td>unsupervised, GPT</td>
                <td>Arxiv</td>
                <td>Demonstrates that language models, specifically GPT 2, trained on the WebText dataset, start to learn various natural language processing tasks (question answering, machine translation, reading comprehension, summarization) without explicit task specific supervision. For instance, when conditioned on a document and questions, the model achieves an F1 score of 55 on the CoQA dataset, matching or exceeding several baseline systems that were trained with over 127,000 examples.</td>
                <td><a href="https://cdn.openai.com/better language models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>On the difficulty of training Recurrent Neural Networks</td>
                <td>Razvan Pascanu et al</td>
                <td>2013</td>
                <td>exploding gradient, vanishing gradient, gradient clipping, normalization</td>
                <td>Arxiv</td>
                <td>Explanation of issues in RNNs (vanishing / exploding gradient) and proposal of gradient clipping.</td>
                <td><a href="https://arxiv.org/pdf/1211.5063" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Learning representations by back propagating errors</td>
                <td>David Rumelhart et al</td>
                <td>1986</td>
                <td>backpropagation, learning procedure, convergence</td>
                <td>Nature</td>
                <td>The main paper for backprop.</td>
                <td><a href="https://www.nature.com/articles/323533a0" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>The Shattered Gradients Problem: If resnets are the answer, then what is the question?</td>
                <td>David Balduzzi et al</td>
                <td>2018</td>
                <td>shattering, initialization</td>
                <td>ICML</td>
                <td>The paper identifies the &quot;shattered gradients&quot; problem in standard feedforward neural networks. It shows that gradients in these networks exhibit an exponential decay in correlation with depth, leading to gradients that resemble white noise. In contrast, architectures like highway and ResNets with skip connections demonstrate gradients that decay sublinearly, indicating greater resilience against shattering. The paper introduces a new initialization technique termed &quot;Looks Linear&quot; (LL) that addresses the shattered gradients issue. Preliminary experiments demonstrate that LL initialization enables the training of very deep networks without the need for skip connections like those in ResNets or highway networks. This initialization method offers a promising alternative to achieving stable gradient propagation in deep networks, potentially simplifying network architecture and improving training efficiency.</td>
                <td><a href="https://arxiv.org/pdf/1702.08591" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>A Simple Baseline for Bayesian Uncertainty in Deep Learning</td>
                <td>Wesley Maddox et al</td>
                <td>2019</td>
                <td>bayesian, uncertainty, guassian</td>
                <td>NeurIPS</td>
                <td>SWAG combines Stochastic Weight Averaging (SWA) with Gaussian fitting to provide an approximate posterior distribution over neural network weights. SWA computes the first moment of SGD iterates using a modified learning rate schedule. SWAG extends this by fitting a Gaussian distribution using SWA&#x27;s solution as the first moment and incorporating a low rank plus diagonal covariance derived from SGD iterates.</td>
                <td><a href="https://arxiv.org/pdf/1902.02476" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>SmartExchange: Trading Higher cost Memory Storage/Access for Lower cost Computation</td>
                <td>Yang Zhao et al</td>
                <td>2020</td>
                <td>compression, accelerator, pruning, decomposition, quantization</td>
                <td>ACM/IEEE</td>
                <td>SmartExchange integrates sparsification or pruning, decomposition, and quantization techniques into a unified algorithm. It aims to enforce a structured DNN weight format where each layer&#x27;s weight matrix is represented as a product of a small basis matrix and a large sparse coefficient matrix with power of 2 elements.</td>
                <td><a href="https://arxiv.org/pdf/2005.03403" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>On the Spectral Bias of Neural Networks</td>
                <td>Nasim Rahaman et al</td>
                <td>2019</td>
                <td>spectra, fourier analysis, manifold learning</td>
                <td>ICML</td>
                <td>Neural networks, particularly deep ReLU networks, exhibit a learning bias towards low frequency functions. This bias means they tend to prioritize learning global variations over local fluctuations in data. This property aligns with their ability to generalize well across different samples and datasets. Contrary to intuition, as the complexity of the data manifold increases, deep networks find it easier to learn higher frequency functions. This suggests that while they naturally favor low frequency patterns, they can also adapt to more complex data structures to capture higher frequency variations.</td>
                <td><a href="https://arxiv.org/pdf/1806.08734" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Sequence to Sequence Learning with Neural Networks</td>
                <td>Ilya Sutskever et al</td>
                <td>2014</td>
                <td>seq2seq</td>
                <td>Arxiv</td>
                <td>The paper introduces an end to end approach for sequence learning using multilayered Long Short Term Memory (LSTM) networks. This method requires minimal assumptions about the structure of the sequences and effectively maps input sequences to a fixed dimensional vector using one LSTM layer, and decodes target sequences using another deep LSTM layer.</td>
                <td><a href="https://arxiv.org/abs/1409.3215" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Tiled convolutional neural networks</td>
                <td>Quoc Le et al</td>
                <td>2010</td>
                <td>tiling, cnn</td>
                <td>NeurIPS</td>
                <td>Tiled CNNs introduce a novel approach to learning invariances by using a regular &quot;tiled&quot; pattern of tied weights. Unlike traditional CNNs where adjacent hidden units share identical weights, Tiled CNNs require only that hidden units at a certain distance from each other share tied weights. This allows the network to learn complex invariances such as scale and rotational invariance, in addition to translational invariance.</td>
                <td><a href="https://dl.acm.org/doi/10.5555/2997189.2997332" target="_blank">Link</a></td>
            </tr>
        
            <tr>
                <td>Unsupervised Learning of Image Manifolds by Semidefinite Programming</td>
                <td>Kilian Weinberger et al</td>
                <td>2004</td>
                <td>manifold learning, dimensionality reduction</td>
                <td>IEEE</td>
                <td>The paper proposes a new approach to detect low dimensional structure in high dimensional datasets using semidefinite programming (SDP). SDP is leveraged to analyze data that resides on or near a low dimensional manifold, which is a common challenge in computer vision and pattern recognition. The algorithm introduced overcomes limitations observed in previous manifold learning techniques like Isomap and locally linear embedding (LLE). These traditional methods often struggle with certain types of data distributions or computational complexities, which the proposed SDP based approach aims to address more effectively.</td>
                <td><a href="https://ieeexplore.ieee.org/document/1315272" target="_blank">Link</a></td>
            </tr>
        
        </tbody>
    </table>

    <script>
        const table = document.getElementById('paperTable');
        const tbody = table.querySelector('tbody');
        const clearButton = document.getElementById('clearSearch');
        const searchInputs = {
            title: document.getElementById('titleSearch'),
            author: document.getElementById('authorSearch'),
            year: document.getElementById('yearSearch'),
            topic: document.getElementById('topicSearch'),
            venue: document.getElementById('venueSearch'),
            description: document.getElementById('descriptionSearch')
        };
        const paperCountElement = document.getElementById('paperCount');
        const searchCountElement = document.getElementById('searchCount');
        let sortOrder = {};

        function setupEventListeners() {
            for (let key in searchInputs) {
                searchInputs[key].addEventListener('keyup', searchTable);
            }

            clearButton.addEventListener('click', function() {
                for (let key in searchInputs) {
                    searchInputs[key].value = '';
                }
                searchTable();
            });

            table.querySelector('thead').addEventListener('click', function(e) {
                const th = e.target.closest('th');
                if (!th) return;
                const column = th.dataset.sort;
                const dataType = column === 'year' ? 'number' : 'string';
                sortOrder[column] = sortOrder[column] === 'asc' ? 'desc' : 'asc';
                sortTable(column, dataType, sortOrder[column]);
            });
        }

        function searchTable() {
            const rows = tbody.getElementsByTagName('tr');
            let numRowsMatch = 0;
            for (let i = 0; i < rows.length; i++) {
                const row = rows[i];
                const cells = row.getElementsByTagName('td');
                let foundMatch = true;

                for (let key in searchInputs) {
                    const cellText = cells[Object.keys(searchInputs).indexOf(key)].textContent.toLowerCase();
                    const searchTerm = searchInputs[key].value.toLowerCase();
                    if (searchTerm && !cellText.includes(searchTerm)) {
                        foundMatch = false;
                        break;
                    }
                }

                if(foundMatch) {
                    row.style.display = '';
                    numRowsMatch++;
                } else {
                    row.style.display = 'none';
                }
            }
            updateSearchCount(numRowsMatch);
        }

        function updateSearchCount(count) {
            searchCountElement.textContent = `Your search returned ${count} papers. Nice!`;
        }

        function sortTable(column, dataType, order) {
            const rows = Array.from(tbody.querySelectorAll('tr'));
            const columnIndex = Array.from(table.querySelector('thead tr').children).findIndex(th => th.dataset.sort === column);

            rows.sort((a, b) => {
                let aValue = a.children[columnIndex].textContent;
                let bValue = b.children[columnIndex].textContent;

                if (dataType === 'number') {
                    return order === 'asc' ? Number(aValue) - Number(bValue) : Number(bValue) - Number(aValue);
                } else {
                    return order === 'asc' ? aValue.localeCompare(bValue) : bValue.localeCompare(aValue);
                }
            });

            tbody.append(...rows);
        }

        setupEventListeners();
    </script>
</body>
</html>
    